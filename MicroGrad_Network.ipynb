{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c3ddcd",
   "metadata": {},
   "source": [
    "##### → Import Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2560d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cpu.\n",
      "NumPy Version: 2.1.3.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from graphviz import Digraph\n",
    "from graphviz import Source\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "print(f'PyTorch Version: {torch.__version__}.')\n",
    "print(f'NumPy Version: {np.__version__}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf04351",
   "metadata": {},
   "source": [
    "##### → Design Data Structure and Computational Graph: how does a node look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bc8288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=[], _operation='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0    # No effect when an instance (=object) has been just created.\n",
    "        self.label = label\n",
    "\n",
    "        # These are for backprop.\n",
    "        self._children = _children\n",
    "        self._operation = _operation\n",
    "        self._backward = lambda: None # blank function.\n",
    "        '''\n",
    "        • The left nodes cannot call not  _backward method.\n",
    "        • It is an empty function when an instance (=object) has just been created.\n",
    "        • \"def v(): None\" and \"v = lambda: None\" represent exactly same function.\n",
    "        '''\n",
    "    #end-def\n",
    "\n",
    "    def __repr__(self,):\n",
    "        return f'Value(data={self.data:0.4f}, grad={self.grad:0.4f}, label={self.label})'\n",
    "    #end-def\n",
    "\n",
    "    def __add__(self, other):\n",
    "        '''\n",
    "        • Operation: When we ask to calculate \"a+b,\" \n",
    "        Python internally calls \"a.__add__(b).\"\n",
    "        '''\n",
    "        other = other if isinstance(other, Value) else Value(data=other)\n",
    "        \n",
    "        output = Value(data=self.data + other.data,\n",
    "                     _children=[self, other],\n",
    "                     _operation='+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad  += 1.0 * output.grad\n",
    "            other.grad += 1.0 * output.grad\n",
    "        #end-def\n",
    "        output._backward = _backward\n",
    "        # This should not be _backward(); otherwise, it calls each time.\n",
    "        return output\n",
    "    #end-def\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    #end-def\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        '''\n",
    "        Operation: When we ask to calculate \"a*b,\" \n",
    "        Python internally calls \"a.__mul__(b).\"\n",
    "        '''\n",
    "        other = other if isinstance(other, Value) else Value(data=other)\n",
    "\n",
    "        output = Value(data=self.data * other.data,\n",
    "                     _children=[self, other],\n",
    "                     _operation='*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad  += other.data * output.grad\n",
    "            other.grad += self.data  * output.grad\n",
    "\n",
    "            # print(f'''\n",
    "            #     self.data: {self.data}, self.grad: {self.grad},\n",
    "            #     other.data: {other.data}, self.grad: {other.grad}, \n",
    "            #     output.data: {output.data}, output.grad {output.grad}\n",
    "            # ''')\n",
    "        #end-def\n",
    "        output._backward = _backward\n",
    "        # This should not be _backward(); otherwise, it calls each time.\n",
    "        return output\n",
    "    #end-def\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    #end-def\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * (-1.0)\n",
    "    #end-def\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    #end-def\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return (-self) + other\n",
    "    #end-def\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), '__pow__ supports solely int and float.'\n",
    "        \n",
    "        output = Value(self.data ** other, _children=[self,], _operation=f'{self.data}^{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * (self.data ** (other - 1.0))) * output.grad\n",
    "        #end-def\n",
    "        output._backward = _backward\n",
    "        \n",
    "        return output\n",
    "    #end-def\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        # z = a(=2) / 2.5 = 0.8\n",
    "        return self * (other ** (-1.0))\n",
    "    #end-def\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        # z = 2.5 / a(=2) = 1.25\n",
    "        return (self**(-1.0)) * other\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        \n",
    "        # exponent_portion = math.exp(2.0*x) # exponet_portion = (2.0*x).exp()\n",
    "\n",
    "        # data = (exponent_portion - 1) / (exponent_portion + 1)\n",
    "\n",
    "        data = math.tanh(x)\n",
    "\n",
    "        output = Value(data=data, _children=[self], _operation='tanh( )')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1.0-(output.data**2.0)) * output.grad\n",
    "        #end-def\n",
    "        output._backward = _backward\n",
    "\n",
    "        return output\n",
    "    #end-def\n",
    "\n",
    "    def ReLU(self):\n",
    "        x = self.data\n",
    "        output = Value(data=x if x>0 else 0.0, _children=[self,], _operation='ReLU( )')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1.0 if x>0 else 0.0) * output.grad\n",
    "        #end-def\n",
    "        output._backward = _backward\n",
    "        \n",
    "        return output\n",
    "    #end-def\n",
    "\n",
    "    def log(self):\n",
    "        output = Value(data=math.log(self.data), _children=[self,], _operation='logₑ( )')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1.0/self.data) * output.grad\n",
    "        #end-def\n",
    "        output._backward = _backward\n",
    "        \n",
    "        return output\n",
    "    #end-def\n",
    "\n",
    "    def sigmoid(self):\n",
    "        x = self.data\n",
    "        \n",
    "        sigmoid = 1.0 / (1.0 + math.exp(-x))\n",
    "\n",
    "        output = Value(data=sigmoid, _children=[self,], _operation='sigmoid( )')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (sigmoid * (1.0 - sigmoid)) * output.grad\n",
    "        #end-def\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "    #end-def\n",
    "\n",
    "    def exp(self,):\n",
    "        x = math.exp(self.data)\n",
    "        output = Value(data=x, _children=[self,], _operation='exp( )')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += x * output.grad\n",
    "        #end-def\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "    #end-def\n",
    "\n",
    "    def backward(self):\n",
    "        ordered_topology = []\n",
    "        visited_nodes = []\n",
    "\n",
    "        def build_topology(node):\n",
    "            if node not in visited_nodes:\n",
    "                visited_nodes.append(node)\n",
    "\n",
    "                # children = node._children\n",
    "                # child = 0\n",
    "                # while child<len(children):\n",
    "                #     build_topology(children[child])\n",
    "                #     child += 1\n",
    "                # #end-while\n",
    "\n",
    "                for child in node._children:\n",
    "                    build_topology(child)\n",
    "                #end-for\n",
    "                \n",
    "                ordered_topology.append(node)\n",
    "            #end-if/else\n",
    "        #end-def\n",
    "        build_topology(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(ordered_topology):\n",
    "            node._backward()\n",
    "        #end-def\n",
    "    #end-def\n",
    "#end-def"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75357646",
   "metadata": {},
   "source": [
    "##### → Lookup the Computational Graph: connect_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e3965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(root):\n",
    "    # builds a set of all nodes and edges in a graph\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._children:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "            #end-for\n",
    "        #end-if/else\n",
    "    #end-def\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "#end-def\n",
    "\n",
    "def connect_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        # for any value in the graph, create a rectangular ('record') node for it\n",
    "        dot.node(name = uid, label = \"%s | {data = %.4f} | {grad = %.4f}\" % (n.label, n.data, n.grad), shape='record')\n",
    "        if n._operation:\n",
    "            # if this value is a result of some operation, create an op node for it\n",
    "            dot.node(name = uid + n._operation, label = n._operation)\n",
    "            # and connect this node to it\n",
    "            dot.edge(uid + n._operation, uid)\n",
    "        #end-else/if\n",
    "    #end-for\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        # connect n1 to the op node of n2\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._operation)\n",
    "    #end-for\n",
    "\n",
    "    return dot\n",
    "#end-def"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba7fa8d",
   "metadata": {},
   "source": [
    "##### // Test Backprop on Computational Graph / Test Engine: MicroGrad vs PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b44aa476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_1():\n",
    "    x = Value(-4.0)\n",
    "    z = 2 * x + 2 + x\n",
    "    q = z.ReLU() + z * x\n",
    "    h = (z * z).ReLU()\n",
    "    y = h + q + q * x\n",
    "    y.backward()\n",
    "    x_mg, y_mg = x, y\n",
    "\n",
    "    # Value(data=-4.0000, grad=46.0000, label=) Value(data=-20.0000, grad=1.0000, label=)\n",
    "    # print(x_mg.data, y_mg.data)\n",
    "\n",
    "    x = torch.tensor([-4.0], requires_grad = True)\n",
    "    z = 2 * x + 2 + x\n",
    "    q = z.relu() + z * x\n",
    "    h = (z * z).relu()\n",
    "    y = h + q + q * x\n",
    "    y.backward()\n",
    "    x_pt, y_pt = x, y\n",
    "\n",
    "    # tensor([-4.], requires_grad=True) tensor([-20.], grad_fn=<AddBackward0>)\n",
    "    # print(x_pt.data.item(), y_pt.data.item())\n",
    "\n",
    "    # Forward pass went well, if it works.\n",
    "    assert y_mg.data == y_pt.data.item()\n",
    "    # Backward pass went well, if it works.\n",
    "    assert x_mg.grad == x_pt.grad.item()\n",
    "#end-def\n",
    "\n",
    "sanity_check_1()\n",
    "\n",
    "\n",
    "\n",
    "def sanity_check_2():\n",
    "    a = Value(-4.0)\n",
    "    b = Value(2.0)\n",
    "    c = a + b\n",
    "    d = a * b + b**3\n",
    "    c += c + 1\n",
    "    c += 1 + c + (-a)\n",
    "    d += d * 2 + (b + a).ReLU()\n",
    "    d += 3 * d + (b - a).ReLU()\n",
    "    e = c - d\n",
    "    f = e**2\n",
    "    g = f / 2.0\n",
    "    g += 10.0 / f\n",
    "    g.backward()\n",
    "    a_mg, b_mg, g_mg = a, b, g\n",
    "\n",
    "    a = torch.tensor([-4.0], requires_grad = True)\n",
    "    b = torch.tensor([2.0],  requires_grad = True)\n",
    "    c = a + b\n",
    "    d = a * b + b**3\n",
    "    c = c + c + 1\n",
    "    c = c + 1 + c + (-a)\n",
    "    d = d + d * 2 + (b + a).relu()\n",
    "    d = d + 3 * d + (b - a).relu()\n",
    "    e = c - d\n",
    "    f = e**2\n",
    "    g = f / 2.0\n",
    "    g = g + 10.0 / f\n",
    "    g.backward()\n",
    "    a_pt, b_pt, g_pt = a, b, g\n",
    "\n",
    "    # Forward pass went well, if it works.\n",
    "    # print(g_mg.data, g_pt.data.item())\n",
    "    assert abs(g_mg.data == g_pt.data.item()) < (1/1000)\n",
    "    \n",
    "\n",
    "    # backward pass went well:\n",
    "    assert abs(a_mg.grad == a_pt.grad.item()) < (1/1000)\n",
    "    assert abs(b_mg.grad == b_pt.grad.item()) < (1/1000)\n",
    "#end-def\n",
    "\n",
    "sanity_check_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d042e",
   "metadata": {},
   "source": [
    "##### // Check MicroGrad_Engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15631cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"409pt\" height=\"78pt\"\n",
       " viewBox=\"0.00 0.00 409.19 78.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 74)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-74 405.19,-74 405.19,4 -4,4\"/>\n",
       "<!-- 132855580913424 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>132855580913424</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"278.19,-0.5 278.19,-69.5 401.19,-69.5 401.19,-0.5 278.19,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"339.69\" y=\"-54.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"278.19,-46.5 401.19,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"339.69\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">data = &#45;0.9866</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"278.19,-23.5 401.19,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"339.69\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad = 1.0000</text>\n",
       "</g>\n",
       "<!-- 132855580913424tanh( ) -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>132855580913424tanh( )</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"200.6\" cy=\"-35\" rx=\"41.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"200.6\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">tanh( )</text>\n",
       "</g>\n",
       "<!-- 132855580913424tanh( )&#45;&gt;132855580913424 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>132855580913424tanh( )&#45;&gt;132855580913424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M242.36,-35C250.41,-35 259.09,-35 267.81,-35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"268.04,-38.5 278.04,-35 268.04,-31.5 268.04,-38.5\"/>\n",
       "</g>\n",
       "<!-- 132855580903568 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>132855580903568</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-0.5 0,-69.5 123,-69.5 123,-0.5 0,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"61.5\" y=\"-54.3\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-46.5 123,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"61.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">data = &#45;2.5000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"0,-23.5 123,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"61.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">grad = 0.0266</text>\n",
       "</g>\n",
       "<!-- 132855580903568&#45;&gt;132855580913424tanh( ) -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>132855580903568&#45;&gt;132855580913424tanh( )</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M123.03,-35C131.5,-35 140.14,-35 148.42,-35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"148.67,-38.5 158.67,-35 148.67,-31.5 148.67,-38.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x78d4d9b4ef50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(-2.5, label='a')\n",
    "\n",
    "z = a.tanh()\n",
    "\n",
    "# print(z)\n",
    "z.backward()\n",
    "connect_dot(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd97c9f",
   "metadata": {},
   "source": [
    "##### → Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef3e977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def zero_grad(self):\n",
    "        for parameter in self.parameters():\n",
    "            parameter.grad = 0.0\n",
    "        #end-for\n",
    "    #end-def\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    #end-def\n",
    "#end-class\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, nin: int, non_linearity=True):\n",
    "        # nin   [number_of_connections_of_each_neuron, number_of_dimenstions]: 3\n",
    "        # 'nin' indicates how many input(s) are connected to a [single] neuron.\n",
    "\n",
    "        self.weights       = [Value(data=random.uniform(-1, 1.0) * (nin**(-0.5)), label=f'w{_+1}') for _ in range(nin)]  # Value(data=[-1, 1))\n",
    "        self.b             = Value(data=0.0, label='b')\n",
    "        self.non_linearity = non_linearity\n",
    "    #end-def\n",
    "    \n",
    "    def __call__(self, xs):\n",
    "        # z = Σ(w[i] * x[i]) + b\n",
    "        # a = f(z)\n",
    "        \n",
    "        z = sum([(w*x) for w, x in zip(self.weights, xs)]) + self.b # pre-activation\n",
    "        # a = z.tanh() # passed through activaton function (=non linearity).\n",
    "        # a.label = 'a (=output for each sample)'\n",
    "        return z.ReLU() if self.non_linearity == True else z\n",
    "    #end-def\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.weights + [self.b]\n",
    "    #end-def\n",
    "#end-class\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    '''In a layer, every neuron is connected to inputs (nin), but \n",
    "    no neuron is connected to each other.'''\n",
    "    def __init__(self, nin: int, nout: int, non_linearity):\n",
    "        # nin   [number_of_connections_of_each_neuron, number_of_dimenstions]: 3\n",
    "        # nout  [number_of_neurons_of_each_layer]: 5\n",
    "        # nouts [neurons_of_each_layer]: [4, 4, 1]\n",
    "\n",
    "        self.neurons = [Neuron(nin, non_linearity) for _ in range(nout)]\n",
    "    #end-def\n",
    "\n",
    "    def __call__(self, xs):\n",
    "        outputs = [neuron(xs) for neuron in self.neurons]\n",
    "        return outputs if len(outputs) != 1 else outputs[0]\n",
    "    #end-def\n",
    "\n",
    "    def parameters(self):\n",
    "        # params = [neuron.parameters() for p in neuron for neuron in self.neurons]\n",
    "        # return params\n",
    "        # def parameters(self):\n",
    "            # return self.weights + [self.b]\n",
    "        # params = []\n",
    "        # for neuron in self.neurons:\n",
    "        #     for parameter in neuron.parameters():\n",
    "        #         params.append(parameter)\n",
    "        #     #end-for\n",
    "        # #end\n",
    "        return [parameter for neuron in self.neurons for parameter in neuron.parameters()]\n",
    "    #end-def\n",
    "#end-class\n",
    "\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, nin, nouts):\n",
    "        # nin   [number_of_connections_of_each_neuron, number_of_dimenstions]: 3\n",
    "        # nout  [number_of_neurons_of_each_layer]: 5\n",
    "        # nouts [neurons_of_each_layer]: [4, 4, 1]\n",
    "\n",
    "        layer_nin_nout = [nin] + nouts\n",
    "\n",
    "        self.layers = [\n",
    "            Layer(layer_nin_nout[i], layer_nin_nout[i+1], non_linearity =(True if i!=len(nouts)-1 else False))\n",
    "            for i in range(len(nouts))\n",
    "        ]\n",
    "    #end-def \n",
    "\n",
    "    def __call__(self, xs):\n",
    "        # return [layer(xs) for layer in self.layers]\n",
    "        for layer in self.layers:\n",
    "            xs = layer(xs)\n",
    "        #end-for\n",
    "        return xs\n",
    "    #end-def\n",
    "\n",
    "    def parameters(self):\n",
    "        return [parameter for layer in self.layers for parameter in layer.parameters()]\n",
    "    #end-def\n",
    "#end-class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9bb6b",
   "metadata": {},
   "source": [
    "##### → Training 1: four samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "aaea4ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\n",
    "    [Value(2.0, label='x1 [sample #1]'),   Value(3.0,  label='x2 [sample #1]'),  Value(-1.0, label='x3 [sample #1]')],\n",
    "    [Value(3.0, label='x1 [sample #2]'),   Value(-1.0, label='x2 [sample #2]'),  Value(0.5,  label='x3 [sample #2]')],\n",
    "    [Value(0.5, label='x1 [sample #3]'),   Value(1.0,  label='x2 [sample #3]'),  Value(1.0,  label='x3 [sample #3]')],\n",
    "    [Value(1.0, label='x1 [sample #4]'),   Value(1.0,  label='x2 [sample #4]'),  Value(-1.0, label='x3 [sample #4]')],\n",
    "]\n",
    "\n",
    "ys = [1.0, -1.0, -1.0, 1.0] #desired outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "7404c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nin = 3   # dimention of X\n",
    "nouts = [4, 4, 1]\n",
    "\n",
    "# nin   [number_of_connections_of_each_neuron, number_of_dimenstions]: 3\n",
    "# nout  [number_of_neurons_of_each_layer]: 5\n",
    "# nouts [neurons_of_each_layer]: [4, 4, 1]\n",
    "\n",
    "model = MLP(nin, nouts)\n",
    "\n",
    "# parameters = model.parameters()\n",
    "# number_of_parameters = len(parameters)\n",
    "# print(number_of_parameters) # 41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe1fba",
   "metadata": {},
   "source": [
    "###### `Forward pass:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7dc675ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Neuron' object has no attribute 'non_linearity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[197], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ysp \u001b[38;5;241m=\u001b[39m [model(xs) \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m X]\n\u001b[1;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([(y \u001b[38;5;241m-\u001b[39m yp)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m y, yp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ys, ysp)])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[197], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ysp \u001b[38;5;241m=\u001b[39m [model(xs) \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m X]\n\u001b[1;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([(y \u001b[38;5;241m-\u001b[39m yp)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m y, yp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ys, ysp)])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[193], line 81\u001b[0m, in \u001b[0;36mMLP.__call__\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, xs):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# return [layer(xs) for layer in self.layers]\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 81\u001b[0m         xs \u001b[38;5;241m=\u001b[39m layer(xs)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m#end-for\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xs\n",
      "Cell \u001b[0;32mIn[193], line 51\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, xs):\n\u001b[0;32m---> 51\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [neuron(xs) \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons]\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[193], line 51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, xs):\n\u001b[0;32m---> 51\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [neuron(xs) \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons]\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[193], line 30\u001b[0m, in \u001b[0;36mNeuron.__call__\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     27\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([(w\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m w, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, xs)]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;66;03m# pre-activation\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# a = z.tanh() # passed through activaton function (=non linearity).\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# a.label = 'a (=output for each sample)'\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z\u001b[38;5;241m.\u001b[39mtanh() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_linearity \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m z\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Neuron' object has no attribute 'non_linearity'"
     ]
    }
   ],
   "source": [
    "ysp = [model(xs) for xs in X]\n",
    "\n",
    "loss = sum([(y - yp)**2.0 for y, yp in zip(ys, ysp)])\n",
    "print(f'loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a449fb92",
   "metadata": {},
   "source": [
    "###### `Backward pass:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63eb728",
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.parameters():\n",
    "    parameter.grad = 0.0\n",
    "#end-for\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872455a",
   "metadata": {},
   "source": [
    "###### `Update parameters:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1.0/1000.0\n",
    "\n",
    "for parameter in model.parameters():\n",
    "    parameter.data = parameter.data - (learning_rate * parameter.grad)\n",
    "#end-for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1970de6d",
   "metadata": {},
   "source": [
    "###### `Forward pass`, `Backward pass` and `Update parameters`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "b3bd5350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.7005 at epoch 0\n",
      "loss: 0.5570 at epoch 50\n",
      "loss: 0.4329 at epoch 100\n",
      "loss: 0.3312 at epoch 150\n",
      "loss: 0.2507 at epoch 200\n",
      "loss: 0.1880 at epoch 250\n",
      "loss: 0.1400 at epoch 300\n",
      "loss: 0.1036 at epoch 350\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 350+1\n",
    "learning_rate = 1.0/1000.0\n",
    "\n",
    "\n",
    "def MSE(ysp):\n",
    "    return sum([(y - yp)**2.0 for y, yp in zip(ys, ysp)]) / len(ys)\n",
    "#end-def\n",
    "\n",
    "losses = []\n",
    "for epoch in range(number_of_epochs):\n",
    "    # Forward pass:\n",
    "    ysp = [model(xs) for xs in X]\n",
    "    loss = MSE(ysp)\n",
    "    \n",
    "    # Backward pass:\n",
    "    for parameter in model.parameters():\n",
    "        parameter.grad = 0.0\n",
    "    #end-for\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters:\n",
    "    for parameter in model.parameters():\n",
    "        parameter.data = parameter.data - (learning_rate * parameter.grad)\n",
    "    #end-for\n",
    "\n",
    "    # Visualize:\n",
    "    if epoch%50==0:\n",
    "        print(f'loss: {loss.data:0.4f} at epoch {epoch}')\n",
    "    #end-if/else\n",
    "\n",
    "    losses.append(loss.data)\n",
    "#end-for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "ed43f659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def loss_plot(losses):\n",
    "    plt.title('How does loss function change over training steps?', fontsize=12)\n",
    "\n",
    "    plt.plot(losses, color='red', marker='.', linewidth=2.0)\n",
    "    plt.xlabel('Epoch (Training Step)', fontsize=10)\n",
    "    plt.ylabel('MSE Loss', fontsize=10)\n",
    "\n",
    "    plt.savefig(fname='output/Loss_Function_over_Training_Steps.png', dpi=600)\n",
    "    plt.show()\n",
    "#end-def\n",
    "\n",
    "loss_plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad7509",
   "metadata": {},
   "source": [
    "##### → Training 2: Grok's Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "4c68031c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Accuracy: 0.5333\n",
      "Epoch 2, Val Accuracy: 0.5667\n",
      "Epoch 3, Val Accuracy: 0.6333\n",
      "Epoch 4, Val Accuracy: 0.6667\n",
      "Epoch 5, Val Accuracy: 0.6667\n",
      "Epoch 6, Val Accuracy: 0.7000\n",
      "Epoch 7, Val Accuracy: 0.7000\n",
      "Epoch 8, Val Accuracy: 0.7333\n",
      "Epoch 9, Val Accuracy: 0.7333\n",
      "Epoch 10, Val Accuracy: 0.7333\n",
      "Epoch 11, Val Accuracy: 0.7000\n",
      "Epoch 12, Val Accuracy: 0.7000\n",
      "Epoch 13, Val Accuracy: 0.7000\n",
      "Epoch 14, Val Accuracy: 0.7000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[262], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Backpropagate\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m average_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n",
      "Cell \u001b[0;32mIn[184], line 200\u001b[0m, in \u001b[0;36mValue.backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m         ordered_topology\u001b[38;5;241m.\u001b[39mappend(node)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m#end-if/else\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m#end-def\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m build_topology(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(ordered_topology):\n",
      "Cell \u001b[0;32mIn[184], line 194\u001b[0m, in \u001b[0;36mValue.backward.<locals>.build_topology\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# children = node._children\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# child = 0\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# while child<len(children):\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m#     build_topology(children[child])\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m#     child += 1\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# #end-while\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39m_children:\n\u001b[0;32m--> 194\u001b[0m     build_topology(child)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m#end-for\u001b[39;00m\n\u001b[1;32m    197\u001b[0m ordered_topology\u001b[38;5;241m.\u001b[39mappend(node)\n",
      "Cell \u001b[0;32mIn[184], line 194\u001b[0m, in \u001b[0;36mValue.backward.<locals>.build_topology\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# children = node._children\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# child = 0\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# while child<len(children):\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m#     build_topology(children[child])\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m#     child += 1\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# #end-while\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39m_children:\n\u001b[0;32m--> 194\u001b[0m     build_topology(child)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m#end-for\u001b[39;00m\n\u001b[1;32m    197\u001b[0m ordered_topology\u001b[38;5;241m.\u001b[39mappend(node)\n",
      "    \u001b[0;31m[... skipping similar frames: Value.backward.<locals>.build_topology at line 194 (38 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[184], line 194\u001b[0m, in \u001b[0;36mValue.backward.<locals>.build_topology\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# children = node._children\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# child = 0\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# while child<len(children):\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m#     build_topology(children[child])\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m#     child += 1\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# #end-while\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39m_children:\n\u001b[0;32m--> 194\u001b[0m     build_topology(child)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m#end-for\u001b[39;00m\n\u001b[1;32m    197\u001b[0m ordered_topology\u001b[38;5;241m.\u001b[39mappend(node)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate a sample dataset with 1000 samples and 5 features\n",
    "n_samples = 300\n",
    "X = np.random.randn(n_samples, 5)\n",
    "y = np.zeros(n_samples, dtype=int)\n",
    "y[X[:, 0] < -1] = 0  # Class 0 if first feature < -1\n",
    "y[(X[:, 0] >= -1) & (X[:, 0] < 1)] = 1  # Class 1 if -1 <= first feature < 1\n",
    "y[X[:, 0] >= 1] = 2  # Class 2 if first feature >= 1\n",
    "\n",
    "# Split dataset into training (80%), validation (10%), and test (10%) sets\n",
    "indices = np.arange(n_samples)\n",
    "np.random.shuffle(indices)\n",
    "train_size = int(0.8 * n_samples)\n",
    "val_size = int(0.1 * n_samples)\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_val = X[val_indices]\n",
    "y_val = y[val_indices]\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "# Initialize the MLP with input size 5, four hidden layers of 8 neurons, and 3 output classes\n",
    "model = MLP(5, [8, 8, 8, 3])\n",
    "\n",
    "# Set training hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "\n",
    "# Function to compute accuracy on a dataset\n",
    "def compute_accuracy(model, X, y):\n",
    "    correct = 0\n",
    "    for x, true_y in zip(X, y):\n",
    "        x_val = [Value(feature) for feature in x]\n",
    "        logits = model(x_val)\n",
    "        pred = np.argmax([logit.data for logit in logits])\n",
    "        if pred == true_y:\n",
    "            correct += 1\n",
    "    return correct / len(y)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle training data\n",
    "    train_perm = np.random.permutation(len(X_train))\n",
    "    X_train_shuffled = X_train[train_perm]\n",
    "    y_train_shuffled = y_train[train_perm]\n",
    "    \n",
    "    # Process mini-batches\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        end_i = min(i + batch_size, len(X_train))\n",
    "        batch_X = X_train_shuffled[i:end_i]\n",
    "        batch_y = y_train_shuffled[i:end_i]\n",
    "        \n",
    "        # Compute cross-entropy loss for the batch\n",
    "        total_loss = Value(0.0)\n",
    "        for x, y in zip(batch_X, batch_y):\n",
    "            x_val = [Value(feature) for feature in x]\n",
    "            logits = model(x_val)\n",
    "            exp_logits = [logit.exp() for logit in logits]\n",
    "            sum_exp = Value(0.0)\n",
    "            for el in exp_logits:\n",
    "                sum_exp += el\n",
    "            probs = [el / sum_exp for el in exp_logits]\n",
    "            loss = -probs[y].log()\n",
    "            total_loss += loss\n",
    "        \n",
    "        average_loss = total_loss / len(batch_X)\n",
    "        \n",
    "        # Zero gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Backpropagate\n",
    "        average_loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        for p in model.parameters():\n",
    "            p.data -= learning_rate * p.grad\n",
    "    \n",
    "    # Compute and print validation accuracy\n",
    "    val_accuracy = compute_accuracy(model, X_val, y_val)\n",
    "    print(f\"Epoch {epoch + 1}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy = compute_accuracy(model, X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "cb012886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in range(1, 20):\n",
    "#     lr = 1.0 - ((0.9*k)/100.0)\n",
    "#     print(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18688d02",
   "metadata": {},
   "source": [
    "##### Karpaty's Assignment on MicroGrad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8d3bd1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Value(data=0.5915, grad=0.0000, label=), Value(data=1.0591, grad=0.0000, label=), Value(data=-0.6549, grad=0.0000, label=)], [Value(data=1.8970, grad=0.0000, label=), Value(data=-1.6331, grad=0.0000, label=), Value(data=0.2792, grad=0.0000, label=)], [Value(data=0.7984, grad=0.0000, label=), Value(data=-1.1234, grad=0.0000, label=), Value(data=0.2001, grad=0.0000, label=)], [Value(data=1.0400, grad=0.0000, label=), Value(data=1.7956, grad=0.0000, label=), Value(data=-1.0599, grad=0.0000, label=)]]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(4, [5, 3])\n",
    "# print(len(model.parameters()))\n",
    "\n",
    "xs = [1.0, 2.0, 3.0]\n",
    "\n",
    "\n",
    "X = [\n",
    "    [1.0, 2.0, 3.0, 1],\n",
    "    [-1.0, 2.0, -3.0, 1],\n",
    "    [1.0, 2.0, -3.0, 1],\n",
    "    [-1.0, 2.0, 3.0, 0],\n",
    "]\n",
    "\n",
    "\n",
    "Y = [\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "]\n",
    "\n",
    "Yp = [model(xs) for xs in X]\n",
    "\n",
    "print(Yp)\n",
    "# ysp = [Value(data=0.0), Value(data=3.0), Value(data=-2.0), Value(data=1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fb676d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "aa7e7e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "V = [[1, 3, 3], [1, 2, 3]]\n",
    "\n",
    "print(len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2f48c6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=2.3000, grad=0.0000, label=)\n"
     ]
    }
   ],
   "source": [
    "def softmax(logits):\n",
    "    # When we have multiples outputs...\n",
    "    elements = [logit.exp() for logit in logits]\n",
    "    SUM = sum(elements)\n",
    "    return [(element/SUM) for element in elements]\n",
    "#end-def\n",
    "\n",
    "def NLL(ys, ps):\n",
    "    return -sum([y*(p+0.00000000001).log() for y, p in zip(ys, ps)])\n",
    "#end-def\n",
    "\n",
    "def categorical_cross_entrorpy(Y, Yp):\n",
    "    losses = []\n",
    "    for ys, ysp in zip(Y, Yp):\n",
    "        ps = softmax(ysp)\n",
    "        losses.append(NLL(ys, ps))\n",
    "    #end-for\n",
    "    return sum(losses)/len(Y)\n",
    "#end-def\n",
    "\n",
    "loss = categorical_cross_entrorpy(Y, Yp)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d41d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7f97abc",
   "metadata": {},
   "source": [
    "##### → Classify Iris Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d9b49",
   "metadata": {},
   "source": [
    "###### Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f988fc20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setosa:     [1, 0, 0]\n",
    "# versicolor: [0, 1, 0]\n",
    "# virginica:  [0, 0, 1]\n",
    "\n",
    "Y = [\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [0.0, 0.0, 1.0]\n",
    "]\n",
    "\n",
    "# Y\n",
    "\n",
    "X = [\n",
    "    [5.1, 3.5, 1.4, 0.2],\n",
    "    [4.9, 3.0, 1.4, 0.2],\n",
    "    [4.7, 3.2, 1.3, 0.2],\n",
    "    [4.6, 3.1, 1.5, 0.2],\n",
    "    [5.0, 3.6, 1.4, 0.2],\n",
    "    [5.4, 3.9, 1.7, 0.4],\n",
    "    [4.6, 3.4, 1.4, 0.3],\n",
    "    [5.0, 3.4, 1.5, 0.2],\n",
    "    [4.4, 2.9, 1.4, 0.2],\n",
    "    [4.9, 3.1, 1.5, 0.1],\n",
    "    [5.4, 3.7, 1.5, 0.2],\n",
    "    [4.8, 3.4, 1.6, 0.2],\n",
    "    [4.8, 3.0, 1.4, 0.1],\n",
    "    [4.3, 3.0, 1.1, 0.1],\n",
    "    [5.8, 4.0, 1.2, 0.2],\n",
    "    [5.7, 4.4, 1.5, 0.4],\n",
    "    [5.4, 3.9, 1.3, 0.4],\n",
    "    [5.1, 3.5, 1.4, 0.3],\n",
    "    [5.7, 3.8, 1.7, 0.3],\n",
    "    [5.1, 3.8, 1.5, 0.3],\n",
    "    [5.4, 3.4, 1.7, 0.2],\n",
    "    [5.1, 3.7, 1.5, 0.4],\n",
    "    [4.6, 3.6, 1.0, 0.2],\n",
    "    [5.1, 3.3, 1.7, 0.5],\n",
    "    [4.8, 3.4, 1.9, 0.2],\n",
    "    [5.0, 3.0, 1.6, 0.2],\n",
    "    [5.0, 3.4, 1.6, 0.4],\n",
    "    [5.2, 3.5, 1.5, 0.2],\n",
    "    [5.2, 3.4, 1.4, 0.2],\n",
    "    [4.7, 3.2, 1.6, 0.2],\n",
    "    [4.8, 3.1, 1.6, 0.2],\n",
    "    [5.4, 3.4, 1.5, 0.4],\n",
    "    [5.2, 4.1, 1.5, 0.1],\n",
    "    [5.5, 4.2, 1.4, 0.2],\n",
    "    [4.9, 3.1, 1.5, 0.1],\n",
    "    [5.0, 3.2, 1.2, 0.2],\n",
    "    [5.5, 3.5, 1.3, 0.2],\n",
    "    [4.9, 3.1, 1.5, 0.1],\n",
    "    [4.4, 3.0, 1.3, 0.2],\n",
    "    [5.1, 3.4, 1.5, 0.2],\n",
    "    [5.0, 3.5, 1.3, 0.3],\n",
    "    [4.5, 2.3, 1.3, 0.3],\n",
    "    [4.4, 3.2, 1.3, 0.2],\n",
    "    [5.0, 3.5, 1.6, 0.6],\n",
    "    [5.1, 3.8, 1.9, 0.4],\n",
    "    [4.8, 3.0, 1.4, 0.3],\n",
    "    [5.1, 3.8, 1.6, 0.2],\n",
    "    [4.6, 3.2, 1.4, 0.2],\n",
    "    [5.3, 3.7, 1.5, 0.2],\n",
    "    [5.0, 3.3, 1.4, 0.2],\n",
    "    [7.0, 3.2, 4.7, 1.4],\n",
    "    [6.4, 3.2, 4.5, 1.5],\n",
    "    [6.9, 3.1, 4.9, 1.5],\n",
    "    [5.5, 2.3, 4.0, 1.3],\n",
    "    [6.5, 2.8, 4.6, 1.5],\n",
    "    [5.7, 2.8, 4.5, 1.3],\n",
    "    [6.3, 3.3, 4.7, 1.6],\n",
    "    [4.9, 2.4, 3.3, 1.0],\n",
    "    [6.6, 2.9, 4.6, 1.3],\n",
    "    [5.2, 2.7, 3.9, 1.4],\n",
    "    [5.0, 2.0, 3.5, 1.0],\n",
    "    [5.9, 3.0, 4.2, 1.5],\n",
    "    [6.0, 2.2, 4.0, 1.0],\n",
    "    [6.1, 2.9, 4.7, 1.4],\n",
    "    [5.6, 2.9, 3.6, 1.3],\n",
    "    [6.7, 3.1, 4.4, 1.4],\n",
    "    [5.6, 3.0, 4.5, 1.5],\n",
    "    [5.8, 2.7, 4.1, 1.0],\n",
    "    [6.2, 2.2, 4.5, 1.5],\n",
    "    [5.6, 2.5, 3.9, 1.1],\n",
    "    [5.9, 3.2, 4.8, 1.8],\n",
    "    [6.1, 2.8, 4.0, 1.3],\n",
    "    [6.3, 2.5, 4.9, 1.5],\n",
    "    [6.1, 2.8, 4.7, 1.2],\n",
    "    [6.4, 2.9, 4.3, 1.3],\n",
    "    [6.6, 3.0, 4.4, 1.4],\n",
    "    [6.8, 2.8, 4.8, 1.4],\n",
    "    [6.7, 3.0, 5.0, 1.7],\n",
    "    [6.0, 2.9, 4.5, 1.5],\n",
    "    [5.7, 2.6, 3.5, 1.0],\n",
    "    [5.5, 2.4, 3.8, 1.1],\n",
    "    [5.5, 2.4, 3.7, 1.0],\n",
    "    [5.8, 2.7, 3.9, 1.2],\n",
    "    [6.0, 2.7, 5.1, 1.6],\n",
    "    [5.4, 3.0, 4.5, 1.5],\n",
    "    [6.0, 3.4, 4.5, 1.6],\n",
    "    [6.7, 3.1, 4.7, 1.5],\n",
    "    [6.3, 2.3, 4.4, 1.3],\n",
    "    [5.6, 3.0, 4.1, 1.3],\n",
    "    [5.5, 2.5, 4.0, 1.3],\n",
    "    [5.5, 2.6, 4.4, 1.2],\n",
    "    [6.1, 3.0, 4.6, 1.4],\n",
    "    [5.8, 2.6, 4.0, 1.2],\n",
    "    [5.0, 2.3, 3.3, 1.0],\n",
    "    [5.6, 2.7, 4.2, 1.3],\n",
    "    [5.7, 3.0, 4.2, 1.2],\n",
    "    [5.7, 2.9, 4.2, 1.3],\n",
    "    [6.2, 2.9, 4.3, 1.3],\n",
    "    [5.1, 2.5, 3.0, 1.1],\n",
    "    [5.7, 2.8, 4.1, 1.3],\n",
    "    [6.3, 3.3, 6.0, 2.5],\n",
    "    [5.8, 2.7, 5.1, 1.9],\n",
    "    [7.1, 3.0, 5.9, 2.1],\n",
    "    [6.3, 2.9, 5.6, 1.8],\n",
    "    [6.5, 3.0, 5.8, 2.2],\n",
    "    [7.6, 3.0, 6.6, 2.1],\n",
    "    [4.9, 2.5, 4.5, 1.7],\n",
    "    [7.3, 2.9, 6.3, 1.8],\n",
    "    [6.7, 2.5, 5.8, 1.8],\n",
    "    [7.2, 3.6, 6.1, 2.5],\n",
    "    [6.5, 3.2, 5.1, 2.0],\n",
    "    [6.4, 2.7, 5.3, 1.9],\n",
    "    [6.8, 3.0, 5.5, 2.1],\n",
    "    [5.7, 2.5, 5.0, 2.0],\n",
    "    [5.8, 2.8, 5.1, 2.4],\n",
    "    [6.4, 3.2, 5.3, 2.3],\n",
    "    [6.5, 3.0, 5.5, 1.8],\n",
    "    [7.7, 3.8, 6.7, 2.2],\n",
    "    [7.7, 2.6, 6.9, 2.3],\n",
    "    [6.0, 2.2, 5.0, 1.5],\n",
    "    [6.9, 3.2, 5.7, 2.3],\n",
    "    [5.6, 2.8, 4.9, 2.0],\n",
    "    [7.7, 2.8, 6.7, 2.0],\n",
    "    [6.3, 2.7, 4.9, 1.8],\n",
    "    [6.7, 3.3, 5.7, 2.1],\n",
    "    [7.2, 3.2, 6.0, 1.8],\n",
    "    [6.2, 2.8, 4.8, 1.8],\n",
    "    [6.1, 3.0, 4.9, 1.8],\n",
    "    [6.4, 2.8, 5.6, 2.1],\n",
    "    [7.2, 3.0, 5.8, 1.6],\n",
    "    [7.4, 2.8, 6.1, 1.9],\n",
    "    [7.9, 3.8, 6.4, 2.0],\n",
    "    [6.4, 2.8, 5.6, 2.2],\n",
    "    [6.3, 2.8, 5.1, 1.5],\n",
    "    [6.1, 2.6, 5.6, 1.4],\n",
    "    [7.7, 3.0, 6.1, 2.3],\n",
    "    [6.3, 3.4, 5.6, 2.4],\n",
    "    [6.4, 3.1, 5.5, 1.8],\n",
    "    [6.0, 3.0, 4.8, 1.8],\n",
    "    [6.9, 3.1, 5.4, 2.1],\n",
    "    [6.7, 3.1, 5.6, 2.4],\n",
    "    [6.9, 3.1, 5.1, 2.3],\n",
    "    [5.8, 2.7, 5.1, 1.9],\n",
    "    [6.8, 3.2, 5.9, 2.3],\n",
    "    [6.7, 3.3, 5.7, 2.5],\n",
    "    [6.7, 3.0, 5.2, 2.3],\n",
    "    [6.3, 2.5, 5.0, 1.9],\n",
    "    [6.5, 3.0, 5.2, 2.0],\n",
    "    [6.2, 3.4, 5.4, 2.3],\n",
    "    [5.9, 3.0, 5.1, 1.8]\n",
    "]\n",
    "\n",
    "len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088be93f",
   "metadata": {},
   "source": [
    "###### Dataset shuffle and split into train, validation and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ef717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X, Y = shuffle(X, Y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39425890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e0755cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 150\n",
      "Training set: 104 samples (69.3%)\n",
      "Validation set: 23 samples (15.3%)\n",
      "Test set: 23 samples (15.3%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Step 1: Split into train+val and test\n",
    "X_temp, X_test, Y_temp, Y_test = train_test_split(X, Y, \n",
    "    test_size=test_ratio,\n",
    "    random_state=42,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "val_relative_ratio = val_ratio / (train_ratio + val_ratio)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_temp, Y_temp, test_size=val_relative_ratio,\n",
    "    random_state=42,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Verify the sizes\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Training set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e954be",
   "metadata": {},
   "source": [
    "###### NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2eede613",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(4, [10, 3])\n",
    "\n",
    "# Yp = [model(xs) for xs in X_train]\n",
    "\n",
    "# print(Yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d21e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(Yp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6920adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    # When we have multiples outputs...\n",
    "    elements = [logit.exp() for logit in logits]\n",
    "    SUM = sum(elements)\n",
    "    return [(element/SUM) for element in elements]\n",
    "#end-def\n",
    "\n",
    "def NLL(ys, ps):\n",
    "    return -sum([y*(p+0.00000000001).log() for y, p in zip(ys, ps)])\n",
    "#end-def\n",
    "\n",
    "def categorical_cross_entrorpy(Y, Yp):\n",
    "    losses = []\n",
    "    for ys, ysp in zip(Y, Yp):\n",
    "        ps = softmax(ysp)\n",
    "        losses.append(NLL(ys, ps))\n",
    "    #end-for\n",
    "    return sum(losses)/len(Y)\n",
    "#end-def\n",
    "\n",
    "# loss = categorical_cross_entrorpy(Y, Yp)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21d2a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ae18022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When epoch is 1, training loss is 1.92593 and validation loss is 2.31105.\n",
      "When epoch is 2, training loss is 1.71769 and validation loss is 2.02738.\n",
      "When epoch is 3, training loss is 1.55910 and validation loss is 1.80892.\n",
      "When epoch is 4, training loss is 1.43730 and validation loss is 1.64023.\n",
      "When epoch is 5, training loss is 1.34180 and validation loss is 1.50812.\n",
      "When epoch is 6, training loss is 1.26519 and validation loss is 1.40284.\n",
      "When epoch is 7, training loss is 1.20255 and validation loss is 1.31765.\n",
      "When epoch is 8, training loss is 1.15072 and validation loss is 1.24798.\n",
      "When epoch is 9, training loss is 1.10754 and validation loss is 1.19120.\n",
      "When epoch is 10, training loss is 1.07149 and validation loss is 1.14443.\n",
      "When epoch is 11, training loss is 1.04140 and validation loss is 1.10610.\n",
      "When epoch is 12, training loss is 1.01645 and validation loss is 1.07473.\n",
      "When epoch is 13, training loss is 0.99582 and validation loss is 1.04909.\n",
      "When epoch is 14, training loss is 0.97887 and validation loss is 1.02853.\n",
      "When epoch is 15, training loss is 0.96465 and validation loss is 1.01174.\n",
      "When epoch is 16, training loss is 0.95247 and validation loss is 0.99774.\n",
      "When epoch is 17, training loss is 0.94192 and validation loss is 0.98604.\n",
      "When epoch is 18, training loss is 0.93266 and validation loss is 0.97609.\n",
      "When epoch is 19, training loss is 0.92443 and validation loss is 0.96750.\n",
      "When epoch is 20, training loss is 0.91700 and validation loss is 0.96007.\n",
      "When epoch is 21, training loss is 0.91019 and validation loss is 0.95355.\n",
      "When epoch is 22, training loss is 0.90387 and validation loss is 0.94767.\n",
      "When epoch is 23, training loss is 0.89794 and validation loss is 0.94236.\n",
      "When epoch is 24, training loss is 0.89230 and validation loss is 0.93751.\n",
      "When epoch is 25, training loss is 0.88690 and validation loss is 0.93287.\n",
      "When epoch is 26, training loss is 0.88169 and validation loss is 0.92847.\n",
      "When epoch is 27, training loss is 0.87663 and validation loss is 0.92427.\n",
      "When epoch is 28, training loss is 0.87170 and validation loss is 0.92021.\n",
      "When epoch is 29, training loss is 0.86691 and validation loss is 0.91630.\n",
      "When epoch is 30, training loss is 0.86224 and validation loss is 0.91249.\n",
      "When epoch is 31, training loss is 0.85763 and validation loss is 0.90877.\n",
      "When epoch is 32, training loss is 0.85310 and validation loss is 0.90512.\n",
      "When epoch is 33, training loss is 0.84862 and validation loss is 0.90153.\n",
      "When epoch is 34, training loss is 0.84425 and validation loss is 0.89795.\n",
      "When epoch is 35, training loss is 0.83998 and validation loss is 0.89444.\n",
      "When epoch is 36, training loss is 0.83576 and validation loss is 0.89092.\n",
      "When epoch is 37, training loss is 0.83160 and validation loss is 0.88757.\n",
      "When epoch is 38, training loss is 0.82754 and validation loss is 0.88415.\n",
      "When epoch is 39, training loss is 0.82356 and validation loss is 0.88085.\n",
      "When epoch is 40, training loss is 0.81964 and validation loss is 0.87755.\n",
      "When epoch is 41, training loss is 0.81576 and validation loss is 0.87433.\n",
      "When epoch is 42, training loss is 0.81194 and validation loss is 0.87111.\n",
      "When epoch is 43, training loss is 0.80814 and validation loss is 0.86797.\n",
      "When epoch is 44, training loss is 0.80439 and validation loss is 0.86481.\n",
      "When epoch is 45, training loss is 0.80068 and validation loss is 0.86174.\n",
      "When epoch is 46, training loss is 0.79703 and validation loss is 0.85871.\n",
      "When epoch is 47, training loss is 0.79344 and validation loss is 0.85567.\n",
      "When epoch is 48, training loss is 0.78989 and validation loss is 0.85273.\n",
      "When epoch is 49, training loss is 0.78638 and validation loss is 0.84976.\n",
      "When epoch is 50, training loss is 0.78289 and validation loss is 0.84687.\n",
      "When epoch is 51, training loss is 0.77942 and validation loss is 0.84390.\n",
      "When epoch is 52, training loss is 0.77597 and validation loss is 0.84102.\n",
      "When epoch is 53, training loss is 0.77255 and validation loss is 0.83816.\n",
      "When epoch is 54, training loss is 0.76915 and validation loss is 0.83532.\n",
      "When epoch is 55, training loss is 0.76576 and validation loss is 0.83250.\n",
      "When epoch is 56, training loss is 0.76241 and validation loss is 0.82970.\n",
      "When epoch is 57, training loss is 0.75908 and validation loss is 0.82697.\n",
      "When epoch is 58, training loss is 0.75578 and validation loss is 0.82425.\n",
      "When epoch is 59, training loss is 0.75250 and validation loss is 0.82150.\n",
      "When epoch is 60, training loss is 0.74924 and validation loss is 0.81881.\n",
      "When epoch is 61, training loss is 0.74601 and validation loss is 0.81614.\n",
      "When epoch is 62, training loss is 0.74280 and validation loss is 0.81350.\n",
      "When epoch is 63, training loss is 0.73961 and validation loss is 0.81087.\n",
      "When epoch is 64, training loss is 0.73645 and validation loss is 0.80825.\n",
      "When epoch is 65, training loss is 0.73330 and validation loss is 0.80565.\n",
      "When epoch is 66, training loss is 0.73018 and validation loss is 0.80305.\n",
      "When epoch is 67, training loss is 0.72708 and validation loss is 0.80047.\n",
      "When epoch is 68, training loss is 0.72400 and validation loss is 0.79790.\n",
      "When epoch is 69, training loss is 0.72095 and validation loss is 0.79534.\n",
      "When epoch is 70, training loss is 0.71792 and validation loss is 0.79280.\n",
      "When epoch is 71, training loss is 0.71492 and validation loss is 0.79027.\n",
      "When epoch is 72, training loss is 0.71193 and validation loss is 0.78775.\n",
      "When epoch is 73, training loss is 0.70897 and validation loss is 0.78524.\n",
      "When epoch is 74, training loss is 0.70603 and validation loss is 0.78275.\n",
      "When epoch is 75, training loss is 0.70312 and validation loss is 0.78028.\n",
      "When epoch is 76, training loss is 0.70023 and validation loss is 0.77782.\n",
      "When epoch is 77, training loss is 0.69736 and validation loss is 0.77537.\n",
      "When epoch is 78, training loss is 0.69452 and validation loss is 0.77294.\n",
      "When epoch is 79, training loss is 0.69169 and validation loss is 0.77053.\n",
      "When epoch is 80, training loss is 0.68890 and validation loss is 0.76813.\n",
      "When epoch is 81, training loss is 0.68612 and validation loss is 0.76575.\n",
      "When epoch is 82, training loss is 0.68337 and validation loss is 0.76338.\n",
      "When epoch is 83, training loss is 0.68064 and validation loss is 0.76103.\n",
      "When epoch is 84, training loss is 0.67793 and validation loss is 0.75869.\n",
      "When epoch is 85, training loss is 0.67525 and validation loss is 0.75637.\n",
      "When epoch is 86, training loss is 0.67258 and validation loss is 0.75406.\n",
      "When epoch is 87, training loss is 0.66994 and validation loss is 0.75177.\n",
      "When epoch is 88, training loss is 0.66732 and validation loss is 0.74949.\n",
      "When epoch is 89, training loss is 0.66472 and validation loss is 0.74722.\n",
      "When epoch is 90, training loss is 0.66215 and validation loss is 0.74497.\n",
      "When epoch is 91, training loss is 0.65960 and validation loss is 0.74274.\n",
      "When epoch is 92, training loss is 0.65706 and validation loss is 0.74052.\n",
      "When epoch is 93, training loss is 0.65455 and validation loss is 0.73832.\n",
      "When epoch is 94, training loss is 0.65206 and validation loss is 0.73613.\n",
      "When epoch is 95, training loss is 0.64960 and validation loss is 0.73396.\n",
      "When epoch is 96, training loss is 0.64715 and validation loss is 0.73180.\n",
      "When epoch is 97, training loss is 0.64473 and validation loss is 0.72966.\n",
      "When epoch is 98, training loss is 0.64232 and validation loss is 0.72753.\n",
      "When epoch is 99, training loss is 0.63994 and validation loss is 0.72541.\n",
      "When epoch is 100, training loss is 0.63758 and validation loss is 0.72332.\n",
      "When epoch is 101, training loss is 0.63523 and validation loss is 0.72123.\n",
      "When epoch is 102, training loss is 0.63291 and validation loss is 0.71916.\n",
      "When epoch is 103, training loss is 0.63061 and validation loss is 0.71711.\n",
      "When epoch is 104, training loss is 0.62833 and validation loss is 0.71507.\n",
      "When epoch is 105, training loss is 0.62608 and validation loss is 0.71305.\n",
      "When epoch is 106, training loss is 0.62384 and validation loss is 0.71104.\n",
      "When epoch is 107, training loss is 0.62162 and validation loss is 0.70905.\n",
      "When epoch is 108, training loss is 0.61942 and validation loss is 0.70708.\n",
      "When epoch is 109, training loss is 0.61724 and validation loss is 0.70512.\n",
      "When epoch is 110, training loss is 0.61508 and validation loss is 0.70317.\n",
      "When epoch is 111, training loss is 0.61294 and validation loss is 0.70124.\n",
      "When epoch is 112, training loss is 0.61082 and validation loss is 0.69933.\n",
      "When epoch is 113, training loss is 0.60872 and validation loss is 0.69743.\n",
      "When epoch is 114, training loss is 0.60664 and validation loss is 0.69554.\n",
      "When epoch is 115, training loss is 0.60457 and validation loss is 0.69367.\n",
      "When epoch is 116, training loss is 0.60253 and validation loss is 0.69181.\n",
      "When epoch is 117, training loss is 0.60050 and validation loss is 0.68997.\n",
      "When epoch is 118, training loss is 0.59850 and validation loss is 0.68814.\n",
      "When epoch is 119, training loss is 0.59651 and validation loss is 0.68632.\n",
      "When epoch is 120, training loss is 0.59454 and validation loss is 0.68452.\n",
      "When epoch is 121, training loss is 0.59258 and validation loss is 0.68273.\n",
      "When epoch is 122, training loss is 0.59065 and validation loss is 0.68096.\n",
      "When epoch is 123, training loss is 0.58873 and validation loss is 0.67920.\n",
      "When epoch is 124, training loss is 0.58684 and validation loss is 0.67748.\n",
      "When epoch is 125, training loss is 0.58496 and validation loss is 0.67577.\n",
      "When epoch is 126, training loss is 0.58309 and validation loss is 0.67408.\n",
      "When epoch is 127, training loss is 0.58125 and validation loss is 0.67239.\n",
      "When epoch is 128, training loss is 0.57942 and validation loss is 0.67071.\n",
      "When epoch is 129, training loss is 0.57761 and validation loss is 0.66905.\n",
      "When epoch is 130, training loss is 0.57581 and validation loss is 0.66740.\n",
      "When epoch is 131, training loss is 0.57403 and validation loss is 0.66575.\n",
      "When epoch is 132, training loss is 0.57227 and validation loss is 0.66412.\n",
      "When epoch is 133, training loss is 0.57053 and validation loss is 0.66251.\n",
      "When epoch is 134, training loss is 0.56880 and validation loss is 0.66090.\n",
      "When epoch is 135, training loss is 0.56708 and validation loss is 0.65930.\n",
      "When epoch is 136, training loss is 0.56538 and validation loss is 0.65772.\n",
      "When epoch is 137, training loss is 0.56370 and validation loss is 0.65615.\n",
      "When epoch is 138, training loss is 0.56203 and validation loss is 0.65459.\n",
      "When epoch is 139, training loss is 0.56038 and validation loss is 0.65304.\n",
      "When epoch is 140, training loss is 0.55875 and validation loss is 0.65151.\n",
      "When epoch is 141, training loss is 0.55712 and validation loss is 0.64999.\n",
      "When epoch is 142, training loss is 0.55552 and validation loss is 0.64847.\n",
      "When epoch is 143, training loss is 0.55393 and validation loss is 0.64697.\n",
      "When epoch is 144, training loss is 0.55235 and validation loss is 0.64548.\n",
      "When epoch is 145, training loss is 0.55078 and validation loss is 0.64401.\n",
      "When epoch is 146, training loss is 0.54923 and validation loss is 0.64254.\n",
      "When epoch is 147, training loss is 0.54770 and validation loss is 0.64108.\n",
      "When epoch is 148, training loss is 0.54618 and validation loss is 0.63964.\n",
      "When epoch is 149, training loss is 0.54467 and validation loss is 0.63821.\n",
      "When epoch is 150, training loss is 0.54318 and validation loss is 0.63679.\n",
      "When epoch is 151, training loss is 0.54170 and validation loss is 0.63538.\n",
      "When epoch is 152, training loss is 0.54023 and validation loss is 0.63398.\n",
      "When epoch is 153, training loss is 0.53877 and validation loss is 0.63259.\n",
      "When epoch is 154, training loss is 0.53733 and validation loss is 0.63121.\n",
      "When epoch is 155, training loss is 0.53590 and validation loss is 0.62984.\n",
      "When epoch is 156, training loss is 0.53449 and validation loss is 0.62849.\n",
      "When epoch is 157, training loss is 0.53309 and validation loss is 0.62714.\n",
      "When epoch is 158, training loss is 0.53169 and validation loss is 0.62580.\n",
      "When epoch is 159, training loss is 0.53032 and validation loss is 0.62448.\n",
      "When epoch is 160, training loss is 0.52895 and validation loss is 0.62316.\n",
      "When epoch is 161, training loss is 0.52760 and validation loss is 0.62186.\n",
      "When epoch is 162, training loss is 0.52625 and validation loss is 0.62056.\n",
      "When epoch is 163, training loss is 0.52492 and validation loss is 0.61927.\n",
      "When epoch is 164, training loss is 0.52360 and validation loss is 0.61800.\n",
      "When epoch is 165, training loss is 0.52229 and validation loss is 0.61673.\n",
      "When epoch is 166, training loss is 0.52100 and validation loss is 0.61547.\n",
      "When epoch is 167, training loss is 0.51971 and validation loss is 0.61422.\n",
      "When epoch is 168, training loss is 0.51844 and validation loss is 0.61298.\n",
      "When epoch is 169, training loss is 0.51717 and validation loss is 0.61175.\n",
      "When epoch is 170, training loss is 0.51592 and validation loss is 0.61053.\n",
      "When epoch is 171, training loss is 0.51468 and validation loss is 0.60931.\n",
      "When epoch is 172, training loss is 0.51345 and validation loss is 0.60811.\n",
      "When epoch is 173, training loss is 0.51222 and validation loss is 0.60691.\n",
      "When epoch is 174, training loss is 0.51101 and validation loss is 0.60573.\n",
      "When epoch is 175, training loss is 0.50981 and validation loss is 0.60456.\n",
      "When epoch is 176, training loss is 0.50862 and validation loss is 0.60341.\n",
      "When epoch is 177, training loss is 0.50744 and validation loss is 0.60226.\n",
      "When epoch is 178, training loss is 0.50627 and validation loss is 0.60111.\n",
      "When epoch is 179, training loss is 0.50511 and validation loss is 0.59998.\n",
      "When epoch is 180, training loss is 0.50395 and validation loss is 0.59885.\n",
      "When epoch is 181, training loss is 0.50281 and validation loss is 0.59772.\n",
      "When epoch is 182, training loss is 0.50168 and validation loss is 0.59661.\n",
      "When epoch is 183, training loss is 0.50055 and validation loss is 0.59550.\n",
      "When epoch is 184, training loss is 0.49944 and validation loss is 0.59440.\n",
      "When epoch is 185, training loss is 0.49833 and validation loss is 0.59330.\n",
      "When epoch is 186, training loss is 0.49723 and validation loss is 0.59222.\n",
      "When epoch is 187, training loss is 0.49614 and validation loss is 0.59113.\n",
      "When epoch is 188, training loss is 0.49506 and validation loss is 0.59006.\n",
      "When epoch is 189, training loss is 0.49399 and validation loss is 0.58899.\n",
      "When epoch is 190, training loss is 0.49293 and validation loss is 0.58793.\n",
      "When epoch is 191, training loss is 0.49187 and validation loss is 0.58688.\n",
      "When epoch is 192, training loss is 0.49082 and validation loss is 0.58582.\n",
      "When epoch is 193, training loss is 0.48978 and validation loss is 0.58478.\n",
      "When epoch is 194, training loss is 0.48875 and validation loss is 0.58374.\n",
      "When epoch is 195, training loss is 0.48773 and validation loss is 0.58270.\n",
      "When epoch is 196, training loss is 0.48671 and validation loss is 0.58168.\n",
      "When epoch is 197, training loss is 0.48570 and validation loss is 0.58066.\n",
      "When epoch is 198, training loss is 0.48470 and validation loss is 0.57964.\n",
      "When epoch is 199, training loss is 0.48371 and validation loss is 0.57864.\n",
      "When epoch is 200, training loss is 0.48272 and validation loss is 0.57764.\n",
      "When epoch is 201, training loss is 0.48174 and validation loss is 0.57664.\n",
      "When epoch is 202, training loss is 0.48077 and validation loss is 0.57565.\n",
      "When epoch is 203, training loss is 0.47980 and validation loss is 0.57466.\n",
      "When epoch is 204, training loss is 0.47885 and validation loss is 0.57370.\n",
      "When epoch is 205, training loss is 0.47789 and validation loss is 0.57274.\n",
      "When epoch is 206, training loss is 0.47695 and validation loss is 0.57177.\n",
      "When epoch is 207, training loss is 0.47601 and validation loss is 0.57082.\n",
      "When epoch is 208, training loss is 0.47508 and validation loss is 0.56987.\n",
      "When epoch is 209, training loss is 0.47416 and validation loss is 0.56893.\n",
      "When epoch is 210, training loss is 0.47324 and validation loss is 0.56798.\n",
      "When epoch is 211, training loss is 0.47233 and validation loss is 0.56706.\n",
      "When epoch is 212, training loss is 0.47142 and validation loss is 0.56612.\n",
      "When epoch is 213, training loss is 0.47052 and validation loss is 0.56519.\n",
      "When epoch is 214, training loss is 0.46963 and validation loss is 0.56429.\n",
      "When epoch is 215, training loss is 0.46874 and validation loss is 0.56337.\n",
      "When epoch is 216, training loss is 0.46786 and validation loss is 0.56245.\n",
      "When epoch is 217, training loss is 0.46699 and validation loss is 0.56155.\n",
      "When epoch is 218, training loss is 0.46612 and validation loss is 0.56065.\n",
      "When epoch is 219, training loss is 0.46525 and validation loss is 0.55975.\n",
      "When epoch is 220, training loss is 0.46439 and validation loss is 0.55886.\n",
      "When epoch is 221, training loss is 0.46354 and validation loss is 0.55798.\n",
      "When epoch is 222, training loss is 0.46270 and validation loss is 0.55710.\n",
      "When epoch is 223, training loss is 0.46185 and validation loss is 0.55622.\n",
      "When epoch is 224, training loss is 0.46102 and validation loss is 0.55535.\n",
      "When epoch is 225, training loss is 0.46019 and validation loss is 0.55449.\n",
      "When epoch is 226, training loss is 0.45936 and validation loss is 0.55363.\n",
      "When epoch is 227, training loss is 0.45854 and validation loss is 0.55277.\n",
      "When epoch is 228, training loss is 0.45772 and validation loss is 0.55192.\n",
      "When epoch is 229, training loss is 0.45691 and validation loss is 0.55107.\n",
      "When epoch is 230, training loss is 0.45611 and validation loss is 0.55023.\n",
      "When epoch is 231, training loss is 0.45531 and validation loss is 0.54939.\n",
      "When epoch is 232, training loss is 0.45451 and validation loss is 0.54856.\n",
      "When epoch is 233, training loss is 0.45372 and validation loss is 0.54773.\n",
      "When epoch is 234, training loss is 0.45293 and validation loss is 0.54690.\n",
      "When epoch is 235, training loss is 0.45215 and validation loss is 0.54608.\n",
      "When epoch is 236, training loss is 0.45137 and validation loss is 0.54527.\n",
      "When epoch is 237, training loss is 0.45060 and validation loss is 0.54445.\n",
      "When epoch is 238, training loss is 0.44983 and validation loss is 0.54364.\n",
      "When epoch is 239, training loss is 0.44907 and validation loss is 0.54284.\n",
      "When epoch is 240, training loss is 0.44831 and validation loss is 0.54203.\n",
      "When epoch is 241, training loss is 0.44755 and validation loss is 0.54124.\n",
      "When epoch is 242, training loss is 0.44680 and validation loss is 0.54044.\n",
      "When epoch is 243, training loss is 0.44606 and validation loss is 0.53964.\n",
      "When epoch is 244, training loss is 0.44531 and validation loss is 0.53884.\n",
      "When epoch is 245, training loss is 0.44458 and validation loss is 0.53805.\n",
      "When epoch is 246, training loss is 0.44384 and validation loss is 0.53726.\n",
      "When epoch is 247, training loss is 0.44311 and validation loss is 0.53647.\n",
      "When epoch is 248, training loss is 0.44239 and validation loss is 0.53569.\n",
      "When epoch is 249, training loss is 0.44166 and validation loss is 0.53491.\n",
      "When epoch is 250, training loss is 0.44095 and validation loss is 0.53414.\n",
      "When epoch is 251, training loss is 0.44023 and validation loss is 0.53337.\n",
      "When epoch is 252, training loss is 0.43952 and validation loss is 0.53261.\n",
      "When epoch is 253, training loss is 0.43881 and validation loss is 0.53185.\n",
      "When epoch is 254, training loss is 0.43811 and validation loss is 0.53109.\n",
      "When epoch is 255, training loss is 0.43741 and validation loss is 0.53033.\n",
      "When epoch is 256, training loss is 0.43671 and validation loss is 0.52958.\n",
      "When epoch is 257, training loss is 0.43602 and validation loss is 0.52883.\n",
      "When epoch is 258, training loss is 0.43533 and validation loss is 0.52809.\n",
      "When epoch is 259, training loss is 0.43464 and validation loss is 0.52735.\n",
      "When epoch is 260, training loss is 0.43396 and validation loss is 0.52661.\n",
      "When epoch is 261, training loss is 0.43328 and validation loss is 0.52587.\n",
      "When epoch is 262, training loss is 0.43260 and validation loss is 0.52514.\n",
      "When epoch is 263, training loss is 0.43193 and validation loss is 0.52441.\n",
      "When epoch is 264, training loss is 0.43126 and validation loss is 0.52368.\n",
      "When epoch is 265, training loss is 0.43059 and validation loss is 0.52296.\n",
      "When epoch is 266, training loss is 0.42993 and validation loss is 0.52224.\n",
      "When epoch is 267, training loss is 0.42927 and validation loss is 0.52152.\n",
      "When epoch is 268, training loss is 0.42861 and validation loss is 0.52080.\n",
      "When epoch is 269, training loss is 0.42795 and validation loss is 0.52009.\n",
      "When epoch is 270, training loss is 0.42730 and validation loss is 0.51938.\n",
      "When epoch is 271, training loss is 0.42665 and validation loss is 0.51867.\n",
      "When epoch is 272, training loss is 0.42600 and validation loss is 0.51797.\n",
      "When epoch is 273, training loss is 0.42536 and validation loss is 0.51726.\n",
      "When epoch is 274, training loss is 0.42472 and validation loss is 0.51656.\n",
      "When epoch is 275, training loss is 0.42408 and validation loss is 0.51586.\n",
      "When epoch is 276, training loss is 0.42344 and validation loss is 0.51517.\n",
      "When epoch is 277, training loss is 0.42281 and validation loss is 0.51447.\n",
      "When epoch is 278, training loss is 0.42218 and validation loss is 0.51378.\n",
      "When epoch is 279, training loss is 0.42155 and validation loss is 0.51309.\n",
      "When epoch is 280, training loss is 0.42093 and validation loss is 0.51240.\n",
      "When epoch is 281, training loss is 0.42030 and validation loss is 0.51172.\n",
      "When epoch is 282, training loss is 0.41968 and validation loss is 0.51103.\n",
      "When epoch is 283, training loss is 0.41907 and validation loss is 0.51034.\n",
      "When epoch is 284, training loss is 0.41845 and validation loss is 0.50965.\n",
      "When epoch is 285, training loss is 0.41784 and validation loss is 0.50896.\n",
      "When epoch is 286, training loss is 0.41723 and validation loss is 0.50827.\n",
      "When epoch is 287, training loss is 0.41663 and validation loss is 0.50758.\n",
      "When epoch is 288, training loss is 0.41603 and validation loss is 0.50689.\n",
      "When epoch is 289, training loss is 0.41542 and validation loss is 0.50621.\n",
      "When epoch is 290, training loss is 0.41483 and validation loss is 0.50553.\n",
      "When epoch is 291, training loss is 0.41423 and validation loss is 0.50486.\n",
      "When epoch is 292, training loss is 0.41364 and validation loss is 0.50418.\n",
      "When epoch is 293, training loss is 0.41304 and validation loss is 0.50351.\n",
      "When epoch is 294, training loss is 0.41246 and validation loss is 0.50285.\n",
      "When epoch is 295, training loss is 0.41187 and validation loss is 0.50218.\n",
      "When epoch is 296, training loss is 0.41128 and validation loss is 0.50152.\n",
      "When epoch is 297, training loss is 0.41070 and validation loss is 0.50086.\n",
      "When epoch is 298, training loss is 0.41012 and validation loss is 0.50020.\n",
      "When epoch is 299, training loss is 0.40954 and validation loss is 0.49955.\n",
      "When epoch is 300, training loss is 0.40896 and validation loss is 0.49889.\n",
      "When epoch is 301, training loss is 0.40839 and validation loss is 0.49824.\n",
      "When epoch is 302, training loss is 0.40781 and validation loss is 0.49759.\n",
      "When epoch is 303, training loss is 0.40724 and validation loss is 0.49695.\n",
      "When epoch is 304, training loss is 0.40667 and validation loss is 0.49630.\n",
      "When epoch is 305, training loss is 0.40611 and validation loss is 0.49566.\n",
      "When epoch is 306, training loss is 0.40554 and validation loss is 0.49502.\n",
      "When epoch is 307, training loss is 0.40498 and validation loss is 0.49438.\n",
      "When epoch is 308, training loss is 0.40442 and validation loss is 0.49374.\n",
      "When epoch is 309, training loss is 0.40386 and validation loss is 0.49310.\n",
      "When epoch is 310, training loss is 0.40330 and validation loss is 0.49247.\n",
      "When epoch is 311, training loss is 0.40274 and validation loss is 0.49184.\n",
      "When epoch is 312, training loss is 0.40219 and validation loss is 0.49120.\n",
      "When epoch is 313, training loss is 0.40163 and validation loss is 0.49057.\n",
      "When epoch is 314, training loss is 0.40108 and validation loss is 0.48995.\n",
      "When epoch is 315, training loss is 0.40053 and validation loss is 0.48932.\n",
      "When epoch is 316, training loss is 0.39998 and validation loss is 0.48869.\n",
      "When epoch is 317, training loss is 0.39944 and validation loss is 0.48807.\n",
      "When epoch is 318, training loss is 0.39889 and validation loss is 0.48745.\n",
      "When epoch is 319, training loss is 0.39835 and validation loss is 0.48683.\n",
      "When epoch is 320, training loss is 0.39781 and validation loss is 0.48621.\n",
      "When epoch is 321, training loss is 0.39727 and validation loss is 0.48559.\n",
      "When epoch is 322, training loss is 0.39673 and validation loss is 0.48498.\n",
      "When epoch is 323, training loss is 0.39619 and validation loss is 0.48437.\n",
      "When epoch is 324, training loss is 0.39566 and validation loss is 0.48376.\n",
      "When epoch is 325, training loss is 0.39513 and validation loss is 0.48314.\n",
      "When epoch is 326, training loss is 0.39460 and validation loss is 0.48253.\n",
      "When epoch is 327, training loss is 0.39407 and validation loss is 0.48192.\n",
      "When epoch is 328, training loss is 0.39355 and validation loss is 0.48131.\n",
      "When epoch is 329, training loss is 0.39302 and validation loss is 0.48070.\n",
      "When epoch is 330, training loss is 0.39250 and validation loss is 0.48010.\n",
      "When epoch is 331, training loss is 0.39198 and validation loss is 0.47950.\n",
      "When epoch is 332, training loss is 0.39146 and validation loss is 0.47889.\n",
      "When epoch is 333, training loss is 0.39094 and validation loss is 0.47829.\n",
      "When epoch is 334, training loss is 0.39042 and validation loss is 0.47769.\n",
      "When epoch is 335, training loss is 0.38991 and validation loss is 0.47710.\n",
      "When epoch is 336, training loss is 0.38939 and validation loss is 0.47650.\n",
      "When epoch is 337, training loss is 0.38888 and validation loss is 0.47591.\n",
      "When epoch is 338, training loss is 0.38836 and validation loss is 0.47531.\n",
      "When epoch is 339, training loss is 0.38785 and validation loss is 0.47472.\n",
      "When epoch is 340, training loss is 0.38734 and validation loss is 0.47413.\n",
      "When epoch is 341, training loss is 0.38683 and validation loss is 0.47354.\n",
      "When epoch is 342, training loss is 0.38633 and validation loss is 0.47296.\n",
      "When epoch is 343, training loss is 0.38582 and validation loss is 0.47237.\n",
      "When epoch is 344, training loss is 0.38532 and validation loss is 0.47178.\n",
      "When epoch is 345, training loss is 0.38481 and validation loss is 0.47120.\n",
      "When epoch is 346, training loss is 0.38431 and validation loss is 0.47062.\n",
      "When epoch is 347, training loss is 0.38381 and validation loss is 0.47003.\n",
      "When epoch is 348, training loss is 0.38331 and validation loss is 0.46945.\n",
      "When epoch is 349, training loss is 0.38281 and validation loss is 0.46887.\n",
      "When epoch is 350, training loss is 0.38231 and validation loss is 0.46829.\n",
      "When epoch is 351, training loss is 0.38181 and validation loss is 0.46772.\n",
      "When epoch is 352, training loss is 0.38131 and validation loss is 0.46714.\n",
      "When epoch is 353, training loss is 0.38082 and validation loss is 0.46656.\n",
      "When epoch is 354, training loss is 0.38032 and validation loss is 0.46599.\n",
      "When epoch is 355, training loss is 0.37983 and validation loss is 0.46542.\n",
      "When epoch is 356, training loss is 0.37934 and validation loss is 0.46484.\n",
      "When epoch is 357, training loss is 0.37885 and validation loss is 0.46426.\n",
      "When epoch is 358, training loss is 0.37836 and validation loss is 0.46368.\n",
      "When epoch is 359, training loss is 0.37788 and validation loss is 0.46310.\n",
      "When epoch is 360, training loss is 0.37739 and validation loss is 0.46253.\n",
      "When epoch is 361, training loss is 0.37691 and validation loss is 0.46195.\n",
      "When epoch is 362, training loss is 0.37642 and validation loss is 0.46138.\n",
      "When epoch is 363, training loss is 0.37594 and validation loss is 0.46081.\n",
      "When epoch is 364, training loss is 0.37546 and validation loss is 0.46024.\n",
      "When epoch is 365, training loss is 0.37498 and validation loss is 0.45967.\n",
      "When epoch is 366, training loss is 0.37450 and validation loss is 0.45910.\n",
      "When epoch is 367, training loss is 0.37402 and validation loss is 0.45853.\n",
      "When epoch is 368, training loss is 0.37354 and validation loss is 0.45797.\n",
      "When epoch is 369, training loss is 0.37307 and validation loss is 0.45740.\n",
      "When epoch is 370, training loss is 0.37259 and validation loss is 0.45684.\n",
      "When epoch is 371, training loss is 0.37212 and validation loss is 0.45628.\n",
      "When epoch is 372, training loss is 0.37164 and validation loss is 0.45572.\n",
      "When epoch is 373, training loss is 0.37117 and validation loss is 0.45516.\n",
      "When epoch is 374, training loss is 0.37070 and validation loss is 0.45457.\n",
      "When epoch is 375, training loss is 0.37024 and validation loss is 0.45399.\n",
      "When epoch is 376, training loss is 0.36978 and validation loss is 0.45341.\n",
      "When epoch is 377, training loss is 0.36932 and validation loss is 0.45284.\n",
      "When epoch is 378, training loss is 0.36886 and validation loss is 0.45227.\n",
      "When epoch is 379, training loss is 0.36840 and validation loss is 0.45170.\n",
      "When epoch is 380, training loss is 0.36794 and validation loss is 0.45114.\n",
      "When epoch is 381, training loss is 0.36748 and validation loss is 0.45057.\n",
      "When epoch is 382, training loss is 0.36703 and validation loss is 0.45001.\n",
      "When epoch is 383, training loss is 0.36657 and validation loss is 0.44945.\n",
      "When epoch is 384, training loss is 0.36612 and validation loss is 0.44889.\n",
      "When epoch is 385, training loss is 0.36567 and validation loss is 0.44833.\n",
      "When epoch is 386, training loss is 0.36522 and validation loss is 0.44777.\n",
      "When epoch is 387, training loss is 0.36477 and validation loss is 0.44721.\n",
      "When epoch is 388, training loss is 0.36432 and validation loss is 0.44666.\n",
      "When epoch is 389, training loss is 0.36388 and validation loss is 0.44611.\n",
      "When epoch is 390, training loss is 0.36343 and validation loss is 0.44556.\n",
      "When epoch is 391, training loss is 0.36299 and validation loss is 0.44501.\n",
      "When epoch is 392, training loss is 0.36254 and validation loss is 0.44447.\n",
      "When epoch is 393, training loss is 0.36210 and validation loss is 0.44392.\n",
      "When epoch is 394, training loss is 0.36166 and validation loss is 0.44338.\n",
      "When epoch is 395, training loss is 0.36122 and validation loss is 0.44284.\n",
      "When epoch is 396, training loss is 0.36078 and validation loss is 0.44230.\n",
      "When epoch is 397, training loss is 0.36034 and validation loss is 0.44176.\n",
      "When epoch is 398, training loss is 0.35990 and validation loss is 0.44122.\n",
      "When epoch is 399, training loss is 0.35946 and validation loss is 0.44069.\n",
      "When epoch is 400, training loss is 0.35903 and validation loss is 0.44016.\n",
      "When epoch is 401, training loss is 0.35859 and validation loss is 0.43963.\n",
      "When epoch is 402, training loss is 0.35816 and validation loss is 0.43911.\n",
      "When epoch is 403, training loss is 0.35773 and validation loss is 0.43858.\n",
      "When epoch is 404, training loss is 0.35730 and validation loss is 0.43806.\n",
      "When epoch is 405, training loss is 0.35687 and validation loss is 0.43754.\n",
      "When epoch is 406, training loss is 0.35644 and validation loss is 0.43702.\n",
      "When epoch is 407, training loss is 0.35601 and validation loss is 0.43650.\n",
      "When epoch is 408, training loss is 0.35558 and validation loss is 0.43598.\n",
      "When epoch is 409, training loss is 0.35515 and validation loss is 0.43546.\n",
      "When epoch is 410, training loss is 0.35473 and validation loss is 0.43494.\n",
      "When epoch is 411, training loss is 0.35430 and validation loss is 0.43444.\n",
      "When epoch is 412, training loss is 0.35388 and validation loss is 0.43393.\n",
      "When epoch is 413, training loss is 0.35345 and validation loss is 0.43342.\n",
      "When epoch is 414, training loss is 0.35303 and validation loss is 0.43291.\n",
      "When epoch is 415, training loss is 0.35261 and validation loss is 0.43241.\n",
      "When epoch is 416, training loss is 0.35219 and validation loss is 0.43190.\n",
      "When epoch is 417, training loss is 0.35177 and validation loss is 0.43139.\n",
      "When epoch is 418, training loss is 0.35135 and validation loss is 0.43089.\n",
      "When epoch is 419, training loss is 0.35093 and validation loss is 0.43039.\n",
      "When epoch is 420, training loss is 0.35051 and validation loss is 0.42988.\n",
      "When epoch is 421, training loss is 0.35010 and validation loss is 0.42938.\n",
      "When epoch is 422, training loss is 0.34968 and validation loss is 0.42888.\n",
      "When epoch is 423, training loss is 0.34927 and validation loss is 0.42838.\n",
      "When epoch is 424, training loss is 0.34885 and validation loss is 0.42788.\n",
      "When epoch is 425, training loss is 0.34844 and validation loss is 0.42738.\n",
      "When epoch is 426, training loss is 0.34802 and validation loss is 0.42688.\n",
      "When epoch is 427, training loss is 0.34761 and validation loss is 0.42638.\n",
      "When epoch is 428, training loss is 0.34720 and validation loss is 0.42588.\n",
      "When epoch is 429, training loss is 0.34679 and validation loss is 0.42538.\n",
      "When epoch is 430, training loss is 0.34638 and validation loss is 0.42489.\n",
      "When epoch is 431, training loss is 0.34597 and validation loss is 0.42439.\n",
      "When epoch is 432, training loss is 0.34556 and validation loss is 0.42390.\n",
      "When epoch is 433, training loss is 0.34515 and validation loss is 0.42340.\n",
      "When epoch is 434, training loss is 0.34475 and validation loss is 0.42291.\n",
      "When epoch is 435, training loss is 0.34434 and validation loss is 0.42240.\n",
      "When epoch is 436, training loss is 0.34394 and validation loss is 0.42190.\n",
      "When epoch is 437, training loss is 0.34353 and validation loss is 0.42139.\n",
      "When epoch is 438, training loss is 0.34313 and validation loss is 0.42089.\n",
      "When epoch is 439, training loss is 0.34273 and validation loss is 0.42039.\n",
      "When epoch is 440, training loss is 0.34233 and validation loss is 0.41989.\n",
      "When epoch is 441, training loss is 0.34192 and validation loss is 0.41939.\n",
      "When epoch is 442, training loss is 0.34152 and validation loss is 0.41889.\n",
      "When epoch is 443, training loss is 0.34113 and validation loss is 0.41839.\n",
      "When epoch is 444, training loss is 0.34073 and validation loss is 0.41790.\n",
      "When epoch is 445, training loss is 0.34033 and validation loss is 0.41740.\n",
      "When epoch is 446, training loss is 0.33993 and validation loss is 0.41690.\n",
      "When epoch is 447, training loss is 0.33954 and validation loss is 0.41641.\n",
      "When epoch is 448, training loss is 0.33914 and validation loss is 0.41591.\n",
      "When epoch is 449, training loss is 0.33875 and validation loss is 0.41542.\n",
      "When epoch is 450, training loss is 0.33835 and validation loss is 0.41492.\n",
      "When epoch is 451, training loss is 0.33796 and validation loss is 0.41443.\n",
      "When epoch is 452, training loss is 0.33757 and validation loss is 0.41394.\n",
      "When epoch is 453, training loss is 0.33717 and validation loss is 0.41345.\n",
      "When epoch is 454, training loss is 0.33678 and validation loss is 0.41296.\n",
      "When epoch is 455, training loss is 0.33639 and validation loss is 0.41248.\n",
      "When epoch is 456, training loss is 0.33600 and validation loss is 0.41199.\n",
      "When epoch is 457, training loss is 0.33561 and validation loss is 0.41150.\n",
      "When epoch is 458, training loss is 0.33523 and validation loss is 0.41102.\n",
      "When epoch is 459, training loss is 0.33484 and validation loss is 0.41054.\n",
      "When epoch is 460, training loss is 0.33445 and validation loss is 0.41006.\n",
      "When epoch is 461, training loss is 0.33406 and validation loss is 0.40958.\n",
      "When epoch is 462, training loss is 0.33368 and validation loss is 0.40910.\n",
      "When epoch is 463, training loss is 0.33329 and validation loss is 0.40862.\n",
      "When epoch is 464, training loss is 0.33291 and validation loss is 0.40814.\n",
      "When epoch is 465, training loss is 0.33253 and validation loss is 0.40766.\n",
      "When epoch is 466, training loss is 0.33214 and validation loss is 0.40719.\n",
      "When epoch is 467, training loss is 0.33176 and validation loss is 0.40671.\n",
      "When epoch is 468, training loss is 0.33138 and validation loss is 0.40624.\n",
      "When epoch is 469, training loss is 0.33100 and validation loss is 0.40576.\n",
      "When epoch is 470, training loss is 0.33062 and validation loss is 0.40529.\n",
      "When epoch is 471, training loss is 0.33024 and validation loss is 0.40482.\n",
      "When epoch is 472, training loss is 0.32986 and validation loss is 0.40435.\n",
      "When epoch is 473, training loss is 0.32948 and validation loss is 0.40387.\n",
      "When epoch is 474, training loss is 0.32910 and validation loss is 0.40340.\n",
      "When epoch is 475, training loss is 0.32873 and validation loss is 0.40293.\n",
      "When epoch is 476, training loss is 0.32835 and validation loss is 0.40247.\n",
      "When epoch is 477, training loss is 0.32798 and validation loss is 0.40200.\n",
      "When epoch is 478, training loss is 0.32760 and validation loss is 0.40153.\n",
      "When epoch is 479, training loss is 0.32723 and validation loss is 0.40106.\n",
      "When epoch is 480, training loss is 0.32686 and validation loss is 0.40059.\n",
      "When epoch is 481, training loss is 0.32649 and validation loss is 0.40012.\n",
      "When epoch is 482, training loss is 0.32611 and validation loss is 0.39966.\n",
      "When epoch is 483, training loss is 0.32574 and validation loss is 0.39920.\n",
      "When epoch is 484, training loss is 0.32537 and validation loss is 0.39874.\n",
      "When epoch is 485, training loss is 0.32501 and validation loss is 0.39827.\n",
      "When epoch is 486, training loss is 0.32464 and validation loss is 0.39782.\n",
      "When epoch is 487, training loss is 0.32427 and validation loss is 0.39736.\n",
      "When epoch is 488, training loss is 0.32390 and validation loss is 0.39690.\n",
      "When epoch is 489, training loss is 0.32354 and validation loss is 0.39644.\n",
      "When epoch is 490, training loss is 0.32317 and validation loss is 0.39598.\n",
      "When epoch is 491, training loss is 0.32280 and validation loss is 0.39553.\n",
      "When epoch is 492, training loss is 0.32244 and validation loss is 0.39507.\n",
      "When epoch is 493, training loss is 0.32207 and validation loss is 0.39462.\n",
      "When epoch is 494, training loss is 0.32171 and validation loss is 0.39416.\n",
      "When epoch is 495, training loss is 0.32135 and validation loss is 0.39371.\n",
      "When epoch is 496, training loss is 0.32099 and validation loss is 0.39326.\n",
      "When epoch is 497, training loss is 0.32062 and validation loss is 0.39281.\n",
      "When epoch is 498, training loss is 0.32026 and validation loss is 0.39235.\n",
      "When epoch is 499, training loss is 0.31990 and validation loss is 0.39190.\n",
      "When epoch is 500, training loss is 0.31954 and validation loss is 0.39145.\n",
      "When epoch is 501, training loss is 0.31918 and validation loss is 0.39101.\n",
      "When epoch is 502, training loss is 0.31882 and validation loss is 0.39055.\n",
      "When epoch is 503, training loss is 0.31847 and validation loss is 0.39011.\n",
      "When epoch is 504, training loss is 0.31811 and validation loss is 0.38966.\n",
      "When epoch is 505, training loss is 0.31775 and validation loss is 0.38921.\n",
      "When epoch is 506, training loss is 0.31739 and validation loss is 0.38877.\n",
      "When epoch is 507, training loss is 0.31704 and validation loss is 0.38832.\n",
      "When epoch is 508, training loss is 0.31668 and validation loss is 0.38787.\n",
      "When epoch is 509, training loss is 0.31633 and validation loss is 0.38743.\n",
      "When epoch is 510, training loss is 0.31597 and validation loss is 0.38699.\n",
      "When epoch is 511, training loss is 0.31562 and validation loss is 0.38654.\n",
      "When epoch is 512, training loss is 0.31527 and validation loss is 0.38609.\n",
      "When epoch is 513, training loss is 0.31492 and validation loss is 0.38564.\n",
      "When epoch is 514, training loss is 0.31456 and validation loss is 0.38519.\n",
      "When epoch is 515, training loss is 0.31421 and validation loss is 0.38475.\n",
      "When epoch is 516, training loss is 0.31386 and validation loss is 0.38431.\n",
      "When epoch is 517, training loss is 0.31351 and validation loss is 0.38386.\n",
      "When epoch is 518, training loss is 0.31316 and validation loss is 0.38341.\n",
      "When epoch is 519, training loss is 0.31282 and validation loss is 0.38297.\n",
      "When epoch is 520, training loss is 0.31247 and validation loss is 0.38252.\n",
      "When epoch is 521, training loss is 0.31212 and validation loss is 0.38209.\n",
      "When epoch is 522, training loss is 0.31177 and validation loss is 0.38164.\n",
      "When epoch is 523, training loss is 0.31143 and validation loss is 0.38120.\n",
      "When epoch is 524, training loss is 0.31108 and validation loss is 0.38076.\n",
      "When epoch is 525, training loss is 0.31074 and validation loss is 0.38032.\n",
      "When epoch is 526, training loss is 0.31039 and validation loss is 0.37988.\n",
      "When epoch is 527, training loss is 0.31005 and validation loss is 0.37944.\n",
      "When epoch is 528, training loss is 0.30971 and validation loss is 0.37901.\n",
      "When epoch is 529, training loss is 0.30936 and validation loss is 0.37857.\n",
      "When epoch is 530, training loss is 0.30902 and validation loss is 0.37813.\n",
      "When epoch is 531, training loss is 0.30868 and validation loss is 0.37770.\n",
      "When epoch is 532, training loss is 0.30834 and validation loss is 0.37726.\n",
      "When epoch is 533, training loss is 0.30800 and validation loss is 0.37683.\n",
      "When epoch is 534, training loss is 0.30766 and validation loss is 0.37640.\n",
      "When epoch is 535, training loss is 0.30732 and validation loss is 0.37597.\n",
      "When epoch is 536, training loss is 0.30698 and validation loss is 0.37553.\n",
      "When epoch is 537, training loss is 0.30664 and validation loss is 0.37510.\n",
      "When epoch is 538, training loss is 0.30631 and validation loss is 0.37467.\n",
      "When epoch is 539, training loss is 0.30597 and validation loss is 0.37425.\n",
      "When epoch is 540, training loss is 0.30563 and validation loss is 0.37382.\n",
      "When epoch is 541, training loss is 0.30530 and validation loss is 0.37339.\n",
      "When epoch is 542, training loss is 0.30496 and validation loss is 0.37296.\n",
      "When epoch is 543, training loss is 0.30463 and validation loss is 0.37254.\n",
      "When epoch is 544, training loss is 0.30429 and validation loss is 0.37211.\n",
      "When epoch is 545, training loss is 0.30396 and validation loss is 0.37169.\n",
      "When epoch is 546, training loss is 0.30363 and validation loss is 0.37126.\n",
      "When epoch is 547, training loss is 0.30329 and validation loss is 0.37084.\n",
      "When epoch is 548, training loss is 0.30296 and validation loss is 0.37042.\n",
      "When epoch is 549, training loss is 0.30263 and validation loss is 0.37000.\n",
      "When epoch is 550, training loss is 0.30230 and validation loss is 0.36958.\n",
      "When epoch is 551, training loss is 0.30197 and validation loss is 0.36916.\n",
      "When epoch is 552, training loss is 0.30164 and validation loss is 0.36874.\n",
      "When epoch is 553, training loss is 0.30131 and validation loss is 0.36832.\n",
      "When epoch is 554, training loss is 0.30098 and validation loss is 0.36790.\n",
      "When epoch is 555, training loss is 0.30065 and validation loss is 0.36748.\n",
      "When epoch is 556, training loss is 0.30032 and validation loss is 0.36707.\n",
      "When epoch is 557, training loss is 0.30000 and validation loss is 0.36665.\n",
      "When epoch is 558, training loss is 0.29967 and validation loss is 0.36623.\n",
      "When epoch is 559, training loss is 0.29934 and validation loss is 0.36582.\n",
      "When epoch is 560, training loss is 0.29902 and validation loss is 0.36540.\n",
      "When epoch is 561, training loss is 0.29869 and validation loss is 0.36499.\n",
      "When epoch is 562, training loss is 0.29837 and validation loss is 0.36458.\n",
      "When epoch is 563, training loss is 0.29805 and validation loss is 0.36417.\n",
      "When epoch is 564, training loss is 0.29772 and validation loss is 0.36375.\n",
      "When epoch is 565, training loss is 0.29740 and validation loss is 0.36334.\n",
      "When epoch is 566, training loss is 0.29708 and validation loss is 0.36293.\n",
      "When epoch is 567, training loss is 0.29676 and validation loss is 0.36252.\n",
      "When epoch is 568, training loss is 0.29643 and validation loss is 0.36211.\n",
      "When epoch is 569, training loss is 0.29611 and validation loss is 0.36170.\n",
      "When epoch is 570, training loss is 0.29579 and validation loss is 0.36129.\n",
      "When epoch is 571, training loss is 0.29547 and validation loss is 0.36089.\n",
      "When epoch is 572, training loss is 0.29516 and validation loss is 0.36048.\n",
      "When epoch is 573, training loss is 0.29484 and validation loss is 0.36007.\n",
      "When epoch is 574, training loss is 0.29452 and validation loss is 0.35967.\n",
      "When epoch is 575, training loss is 0.29420 and validation loss is 0.35926.\n",
      "When epoch is 576, training loss is 0.29388 and validation loss is 0.35886.\n",
      "When epoch is 577, training loss is 0.29357 and validation loss is 0.35845.\n",
      "When epoch is 578, training loss is 0.29325 and validation loss is 0.35805.\n",
      "When epoch is 579, training loss is 0.29294 and validation loss is 0.35765.\n",
      "When epoch is 580, training loss is 0.29262 and validation loss is 0.35725.\n",
      "When epoch is 581, training loss is 0.29231 and validation loss is 0.35684.\n",
      "When epoch is 582, training loss is 0.29199 and validation loss is 0.35644.\n",
      "When epoch is 583, training loss is 0.29168 and validation loss is 0.35604.\n",
      "When epoch is 584, training loss is 0.29137 and validation loss is 0.35564.\n",
      "When epoch is 585, training loss is 0.29105 and validation loss is 0.35524.\n",
      "When epoch is 586, training loss is 0.29074 and validation loss is 0.35484.\n",
      "When epoch is 587, training loss is 0.29043 and validation loss is 0.35443.\n",
      "When epoch is 588, training loss is 0.29012 and validation loss is 0.35404.\n",
      "When epoch is 589, training loss is 0.28981 and validation loss is 0.35363.\n",
      "When epoch is 590, training loss is 0.28950 and validation loss is 0.35324.\n",
      "When epoch is 591, training loss is 0.28919 and validation loss is 0.35284.\n",
      "When epoch is 592, training loss is 0.28888 and validation loss is 0.35244.\n",
      "When epoch is 593, training loss is 0.28858 and validation loss is 0.35204.\n",
      "When epoch is 594, training loss is 0.28827 and validation loss is 0.35164.\n",
      "When epoch is 595, training loss is 0.28796 and validation loss is 0.35125.\n",
      "When epoch is 596, training loss is 0.28766 and validation loss is 0.35085.\n",
      "When epoch is 597, training loss is 0.28735 and validation loss is 0.35045.\n",
      "When epoch is 598, training loss is 0.28704 and validation loss is 0.35006.\n",
      "When epoch is 599, training loss is 0.28674 and validation loss is 0.34967.\n",
      "When epoch is 600, training loss is 0.28643 and validation loss is 0.34927.\n",
      "When epoch is 601, training loss is 0.28613 and validation loss is 0.34888.\n",
      "When epoch is 602, training loss is 0.28583 and validation loss is 0.34849.\n",
      "When epoch is 603, training loss is 0.28552 and validation loss is 0.34810.\n",
      "When epoch is 604, training loss is 0.28522 and validation loss is 0.34771.\n",
      "When epoch is 605, training loss is 0.28492 and validation loss is 0.34732.\n",
      "When epoch is 606, training loss is 0.28462 and validation loss is 0.34693.\n",
      "When epoch is 607, training loss is 0.28432 and validation loss is 0.34654.\n",
      "When epoch is 608, training loss is 0.28402 and validation loss is 0.34615.\n",
      "When epoch is 609, training loss is 0.28372 and validation loss is 0.34576.\n",
      "When epoch is 610, training loss is 0.28342 and validation loss is 0.34538.\n",
      "When epoch is 611, training loss is 0.28312 and validation loss is 0.34499.\n",
      "When epoch is 612, training loss is 0.28282 and validation loss is 0.34461.\n",
      "When epoch is 613, training loss is 0.28252 and validation loss is 0.34422.\n",
      "When epoch is 614, training loss is 0.28223 and validation loss is 0.34384.\n",
      "When epoch is 615, training loss is 0.28193 and validation loss is 0.34345.\n",
      "When epoch is 616, training loss is 0.28163 and validation loss is 0.34307.\n",
      "When epoch is 617, training loss is 0.28134 and validation loss is 0.34269.\n",
      "When epoch is 618, training loss is 0.28104 and validation loss is 0.34231.\n",
      "When epoch is 619, training loss is 0.28075 and validation loss is 0.34193.\n",
      "When epoch is 620, training loss is 0.28045 and validation loss is 0.34154.\n",
      "When epoch is 621, training loss is 0.28016 and validation loss is 0.34116.\n",
      "When epoch is 622, training loss is 0.27987 and validation loss is 0.34079.\n",
      "When epoch is 623, training loss is 0.27958 and validation loss is 0.34041.\n",
      "When epoch is 624, training loss is 0.27928 and validation loss is 0.34003.\n",
      "When epoch is 625, training loss is 0.27899 and validation loss is 0.33965.\n",
      "When epoch is 626, training loss is 0.27870 and validation loss is 0.33927.\n",
      "When epoch is 627, training loss is 0.27841 and validation loss is 0.33890.\n",
      "When epoch is 628, training loss is 0.27812 and validation loss is 0.33852.\n",
      "When epoch is 629, training loss is 0.27783 and validation loss is 0.33814.\n",
      "When epoch is 630, training loss is 0.27754 and validation loss is 0.33777.\n",
      "When epoch is 631, training loss is 0.27725 and validation loss is 0.33739.\n",
      "When epoch is 632, training loss is 0.27696 and validation loss is 0.33703.\n",
      "When epoch is 633, training loss is 0.27668 and validation loss is 0.33665.\n",
      "When epoch is 634, training loss is 0.27639 and validation loss is 0.33628.\n",
      "When epoch is 635, training loss is 0.27610 and validation loss is 0.33591.\n",
      "When epoch is 636, training loss is 0.27582 and validation loss is 0.33554.\n",
      "When epoch is 637, training loss is 0.27553 and validation loss is 0.33517.\n",
      "When epoch is 638, training loss is 0.27524 and validation loss is 0.33480.\n",
      "When epoch is 639, training loss is 0.27496 and validation loss is 0.33443.\n",
      "When epoch is 640, training loss is 0.27468 and validation loss is 0.33406.\n",
      "When epoch is 641, training loss is 0.27439 and validation loss is 0.33369.\n",
      "When epoch is 642, training loss is 0.27411 and validation loss is 0.33332.\n",
      "When epoch is 643, training loss is 0.27383 and validation loss is 0.33296.\n",
      "When epoch is 644, training loss is 0.27354 and validation loss is 0.33259.\n",
      "When epoch is 645, training loss is 0.27326 and validation loss is 0.33222.\n",
      "When epoch is 646, training loss is 0.27298 and validation loss is 0.33186.\n",
      "When epoch is 647, training loss is 0.27270 and validation loss is 0.33149.\n",
      "When epoch is 648, training loss is 0.27242 and validation loss is 0.33113.\n",
      "When epoch is 649, training loss is 0.27214 and validation loss is 0.33077.\n",
      "When epoch is 650, training loss is 0.27186 and validation loss is 0.33040.\n",
      "When epoch is 651, training loss is 0.27158 and validation loss is 0.33004.\n",
      "When epoch is 652, training loss is 0.27130 and validation loss is 0.32968.\n",
      "When epoch is 653, training loss is 0.27102 and validation loss is 0.32932.\n",
      "When epoch is 654, training loss is 0.27075 and validation loss is 0.32896.\n",
      "When epoch is 655, training loss is 0.27047 and validation loss is 0.32860.\n",
      "When epoch is 656, training loss is 0.27019 and validation loss is 0.32824.\n",
      "When epoch is 657, training loss is 0.26992 and validation loss is 0.32788.\n",
      "When epoch is 658, training loss is 0.26964 and validation loss is 0.32752.\n",
      "When epoch is 659, training loss is 0.26937 and validation loss is 0.32717.\n",
      "When epoch is 660, training loss is 0.26909 and validation loss is 0.32681.\n",
      "When epoch is 661, training loss is 0.26882 and validation loss is 0.32645.\n",
      "When epoch is 662, training loss is 0.26854 and validation loss is 0.32610.\n",
      "When epoch is 663, training loss is 0.26827 and validation loss is 0.32574.\n",
      "When epoch is 664, training loss is 0.26800 and validation loss is 0.32539.\n",
      "When epoch is 665, training loss is 0.26772 and validation loss is 0.32503.\n",
      "When epoch is 666, training loss is 0.26745 and validation loss is 0.32468.\n",
      "When epoch is 667, training loss is 0.26718 and validation loss is 0.32433.\n",
      "When epoch is 668, training loss is 0.26691 and validation loss is 0.32397.\n",
      "When epoch is 669, training loss is 0.26664 and validation loss is 0.32362.\n",
      "When epoch is 670, training loss is 0.26637 and validation loss is 0.32327.\n",
      "When epoch is 671, training loss is 0.26610 and validation loss is 0.32292.\n",
      "When epoch is 672, training loss is 0.26583 and validation loss is 0.32257.\n",
      "When epoch is 673, training loss is 0.26556 and validation loss is 0.32222.\n",
      "When epoch is 674, training loss is 0.26530 and validation loss is 0.32187.\n",
      "When epoch is 675, training loss is 0.26503 and validation loss is 0.32152.\n",
      "When epoch is 676, training loss is 0.26476 and validation loss is 0.32118.\n",
      "When epoch is 677, training loss is 0.26449 and validation loss is 0.32083.\n",
      "When epoch is 678, training loss is 0.26423 and validation loss is 0.32048.\n",
      "When epoch is 679, training loss is 0.26396 and validation loss is 0.32014.\n",
      "When epoch is 680, training loss is 0.26370 and validation loss is 0.31979.\n",
      "When epoch is 681, training loss is 0.26343 and validation loss is 0.31945.\n",
      "When epoch is 682, training loss is 0.26317 and validation loss is 0.31910.\n",
      "When epoch is 683, training loss is 0.26290 and validation loss is 0.31876.\n",
      "When epoch is 684, training loss is 0.26264 and validation loss is 0.31841.\n",
      "When epoch is 685, training loss is 0.26238 and validation loss is 0.31807.\n",
      "When epoch is 686, training loss is 0.26211 and validation loss is 0.31773.\n",
      "When epoch is 687, training loss is 0.26185 and validation loss is 0.31739.\n",
      "When epoch is 688, training loss is 0.26159 and validation loss is 0.31705.\n",
      "When epoch is 689, training loss is 0.26133 and validation loss is 0.31671.\n",
      "When epoch is 690, training loss is 0.26107 and validation loss is 0.31636.\n",
      "When epoch is 691, training loss is 0.26081 and validation loss is 0.31602.\n",
      "When epoch is 692, training loss is 0.26055 and validation loss is 0.31569.\n",
      "When epoch is 693, training loss is 0.26029 and validation loss is 0.31535.\n",
      "When epoch is 694, training loss is 0.26003 and validation loss is 0.31501.\n",
      "When epoch is 695, training loss is 0.25977 and validation loss is 0.31467.\n",
      "When epoch is 696, training loss is 0.25951 and validation loss is 0.31434.\n",
      "When epoch is 697, training loss is 0.25926 and validation loss is 0.31400.\n",
      "When epoch is 698, training loss is 0.25900 and validation loss is 0.31366.\n",
      "When epoch is 699, training loss is 0.25874 and validation loss is 0.31333.\n",
      "When epoch is 700, training loss is 0.25849 and validation loss is 0.31299.\n",
      "When epoch is 701, training loss is 0.25823 and validation loss is 0.31266.\n",
      "When epoch is 702, training loss is 0.25798 and validation loss is 0.31232.\n",
      "When epoch is 703, training loss is 0.25772 and validation loss is 0.31199.\n",
      "When epoch is 704, training loss is 0.25747 and validation loss is 0.31166.\n",
      "When epoch is 705, training loss is 0.25721 and validation loss is 0.31133.\n",
      "When epoch is 706, training loss is 0.25696 and validation loss is 0.31099.\n",
      "When epoch is 707, training loss is 0.25671 and validation loss is 0.31066.\n",
      "When epoch is 708, training loss is 0.25645 and validation loss is 0.31033.\n",
      "When epoch is 709, training loss is 0.25620 and validation loss is 0.31000.\n",
      "When epoch is 710, training loss is 0.25595 and validation loss is 0.30967.\n",
      "When epoch is 711, training loss is 0.25570 and validation loss is 0.30935.\n",
      "When epoch is 712, training loss is 0.25545 and validation loss is 0.30902.\n",
      "When epoch is 713, training loss is 0.25520 and validation loss is 0.30869.\n",
      "When epoch is 714, training loss is 0.25495 and validation loss is 0.30836.\n",
      "When epoch is 715, training loss is 0.25470 and validation loss is 0.30803.\n",
      "When epoch is 716, training loss is 0.25445 and validation loss is 0.30771.\n",
      "When epoch is 717, training loss is 0.25420 and validation loss is 0.30738.\n",
      "When epoch is 718, training loss is 0.25395 and validation loss is 0.30706.\n",
      "When epoch is 719, training loss is 0.25371 and validation loss is 0.30673.\n",
      "When epoch is 720, training loss is 0.25346 and validation loss is 0.30641.\n",
      "When epoch is 721, training loss is 0.25321 and validation loss is 0.30608.\n",
      "When epoch is 722, training loss is 0.25297 and validation loss is 0.30576.\n",
      "When epoch is 723, training loss is 0.25272 and validation loss is 0.30544.\n",
      "When epoch is 724, training loss is 0.25247 and validation loss is 0.30511.\n",
      "When epoch is 725, training loss is 0.25223 and validation loss is 0.30479.\n",
      "When epoch is 726, training loss is 0.25198 and validation loss is 0.30447.\n",
      "When epoch is 727, training loss is 0.25174 and validation loss is 0.30415.\n",
      "When epoch is 728, training loss is 0.25150 and validation loss is 0.30383.\n",
      "When epoch is 729, training loss is 0.25125 and validation loss is 0.30351.\n",
      "When epoch is 730, training loss is 0.25101 and validation loss is 0.30319.\n",
      "When epoch is 731, training loss is 0.25077 and validation loss is 0.30287.\n",
      "When epoch is 732, training loss is 0.25053 and validation loss is 0.30256.\n",
      "When epoch is 733, training loss is 0.25028 and validation loss is 0.30224.\n",
      "When epoch is 734, training loss is 0.25004 and validation loss is 0.30192.\n",
      "When epoch is 735, training loss is 0.24980 and validation loss is 0.30160.\n",
      "When epoch is 736, training loss is 0.24956 and validation loss is 0.30129.\n",
      "When epoch is 737, training loss is 0.24932 and validation loss is 0.30097.\n",
      "When epoch is 738, training loss is 0.24908 and validation loss is 0.30066.\n",
      "When epoch is 739, training loss is 0.24884 and validation loss is 0.30034.\n",
      "When epoch is 740, training loss is 0.24861 and validation loss is 0.30003.\n",
      "When epoch is 741, training loss is 0.24837 and validation loss is 0.29972.\n",
      "When epoch is 742, training loss is 0.24813 and validation loss is 0.29940.\n",
      "When epoch is 743, training loss is 0.24789 and validation loss is 0.29908.\n",
      "When epoch is 744, training loss is 0.24766 and validation loss is 0.29877.\n",
      "When epoch is 745, training loss is 0.24742 and validation loss is 0.29846.\n",
      "When epoch is 746, training loss is 0.24718 and validation loss is 0.29814.\n",
      "When epoch is 747, training loss is 0.24695 and validation loss is 0.29783.\n",
      "When epoch is 748, training loss is 0.24671 and validation loss is 0.29752.\n",
      "When epoch is 749, training loss is 0.24648 and validation loss is 0.29721.\n",
      "When epoch is 750, training loss is 0.24624 and validation loss is 0.29690.\n",
      "When epoch is 751, training loss is 0.24601 and validation loss is 0.29659.\n",
      "When epoch is 752, training loss is 0.24578 and validation loss is 0.29629.\n",
      "When epoch is 753, training loss is 0.24554 and validation loss is 0.29597.\n",
      "When epoch is 754, training loss is 0.24531 and validation loss is 0.29567.\n",
      "When epoch is 755, training loss is 0.24508 and validation loss is 0.29536.\n",
      "When epoch is 756, training loss is 0.24485 and validation loss is 0.29505.\n",
      "When epoch is 757, training loss is 0.24462 and validation loss is 0.29475.\n",
      "When epoch is 758, training loss is 0.24438 and validation loss is 0.29444.\n",
      "When epoch is 759, training loss is 0.24415 and validation loss is 0.29413.\n",
      "When epoch is 760, training loss is 0.24392 and validation loss is 0.29383.\n",
      "When epoch is 761, training loss is 0.24369 and validation loss is 0.29352.\n",
      "When epoch is 762, training loss is 0.24347 and validation loss is 0.29322.\n",
      "When epoch is 763, training loss is 0.24324 and validation loss is 0.29292.\n",
      "When epoch is 764, training loss is 0.24301 and validation loss is 0.29261.\n",
      "When epoch is 765, training loss is 0.24278 and validation loss is 0.29231.\n",
      "When epoch is 766, training loss is 0.24255 and validation loss is 0.29201.\n",
      "When epoch is 767, training loss is 0.24233 and validation loss is 0.29171.\n",
      "When epoch is 768, training loss is 0.24210 and validation loss is 0.29141.\n",
      "When epoch is 769, training loss is 0.24187 and validation loss is 0.29111.\n",
      "When epoch is 770, training loss is 0.24165 and validation loss is 0.29081.\n",
      "When epoch is 771, training loss is 0.24142 and validation loss is 0.29051.\n",
      "When epoch is 772, training loss is 0.24120 and validation loss is 0.29021.\n",
      "When epoch is 773, training loss is 0.24097 and validation loss is 0.28992.\n",
      "When epoch is 774, training loss is 0.24075 and validation loss is 0.28961.\n",
      "When epoch is 775, training loss is 0.24052 and validation loss is 0.28932.\n",
      "When epoch is 776, training loss is 0.24030 and validation loss is 0.28902.\n",
      "When epoch is 777, training loss is 0.24008 and validation loss is 0.28873.\n",
      "When epoch is 778, training loss is 0.23986 and validation loss is 0.28843.\n",
      "When epoch is 779, training loss is 0.23963 and validation loss is 0.28813.\n",
      "When epoch is 780, training loss is 0.23941 and validation loss is 0.28784.\n",
      "When epoch is 781, training loss is 0.23919 and validation loss is 0.28754.\n",
      "When epoch is 782, training loss is 0.23897 and validation loss is 0.28725.\n",
      "When epoch is 783, training loss is 0.23875 and validation loss is 0.28696.\n",
      "When epoch is 784, training loss is 0.23853 and validation loss is 0.28667.\n",
      "When epoch is 785, training loss is 0.23831 and validation loss is 0.28638.\n",
      "When epoch is 786, training loss is 0.23809 and validation loss is 0.28608.\n",
      "When epoch is 787, training loss is 0.23787 and validation loss is 0.28579.\n",
      "When epoch is 788, training loss is 0.23765 and validation loss is 0.28550.\n",
      "When epoch is 789, training loss is 0.23744 and validation loss is 0.28521.\n",
      "When epoch is 790, training loss is 0.23722 and validation loss is 0.28492.\n",
      "When epoch is 791, training loss is 0.23700 and validation loss is 0.28463.\n",
      "When epoch is 792, training loss is 0.23678 and validation loss is 0.28434.\n",
      "When epoch is 793, training loss is 0.23657 and validation loss is 0.28406.\n",
      "When epoch is 794, training loss is 0.23635 and validation loss is 0.28377.\n",
      "When epoch is 795, training loss is 0.23614 and validation loss is 0.28348.\n",
      "When epoch is 796, training loss is 0.23592 and validation loss is 0.28319.\n",
      "When epoch is 797, training loss is 0.23571 and validation loss is 0.28291.\n",
      "When epoch is 798, training loss is 0.23549 and validation loss is 0.28262.\n",
      "When epoch is 799, training loss is 0.23528 and validation loss is 0.28234.\n",
      "When epoch is 800, training loss is 0.23506 and validation loss is 0.28206.\n",
      "When epoch is 801, training loss is 0.23485 and validation loss is 0.28177.\n",
      "When epoch is 802, training loss is 0.23464 and validation loss is 0.28149.\n",
      "When epoch is 803, training loss is 0.23442 and validation loss is 0.28120.\n",
      "When epoch is 804, training loss is 0.23421 and validation loss is 0.28092.\n",
      "When epoch is 805, training loss is 0.23400 and validation loss is 0.28063.\n",
      "When epoch is 806, training loss is 0.23379 and validation loss is 0.28036.\n",
      "When epoch is 807, training loss is 0.23358 and validation loss is 0.28007.\n",
      "When epoch is 808, training loss is 0.23337 and validation loss is 0.27979.\n",
      "When epoch is 809, training loss is 0.23316 and validation loss is 0.27951.\n",
      "When epoch is 810, training loss is 0.23295 and validation loss is 0.27923.\n",
      "When epoch is 811, training loss is 0.23274 and validation loss is 0.27895.\n",
      "When epoch is 812, training loss is 0.23253 and validation loss is 0.27867.\n",
      "When epoch is 813, training loss is 0.23232 and validation loss is 0.27839.\n",
      "When epoch is 814, training loss is 0.23211 and validation loss is 0.27812.\n",
      "When epoch is 815, training loss is 0.23191 and validation loss is 0.27784.\n",
      "When epoch is 816, training loss is 0.23170 and validation loss is 0.27756.\n",
      "When epoch is 817, training loss is 0.23149 and validation loss is 0.27728.\n",
      "When epoch is 818, training loss is 0.23129 and validation loss is 0.27700.\n",
      "When epoch is 819, training loss is 0.23108 and validation loss is 0.27673.\n",
      "When epoch is 820, training loss is 0.23087 and validation loss is 0.27645.\n",
      "When epoch is 821, training loss is 0.23067 and validation loss is 0.27618.\n",
      "When epoch is 822, training loss is 0.23046 and validation loss is 0.27591.\n",
      "When epoch is 823, training loss is 0.23026 and validation loss is 0.27564.\n",
      "When epoch is 824, training loss is 0.23005 and validation loss is 0.27536.\n",
      "When epoch is 825, training loss is 0.22985 and validation loss is 0.27509.\n",
      "When epoch is 826, training loss is 0.22965 and validation loss is 0.27481.\n",
      "When epoch is 827, training loss is 0.22944 and validation loss is 0.27455.\n",
      "When epoch is 828, training loss is 0.22924 and validation loss is 0.27427.\n",
      "When epoch is 829, training loss is 0.22904 and validation loss is 0.27400.\n",
      "When epoch is 830, training loss is 0.22884 and validation loss is 0.27373.\n",
      "When epoch is 831, training loss is 0.22863 and validation loss is 0.27346.\n",
      "When epoch is 832, training loss is 0.22843 and validation loss is 0.27319.\n",
      "When epoch is 833, training loss is 0.22823 and validation loss is 0.27292.\n",
      "When epoch is 834, training loss is 0.22803 and validation loss is 0.27265.\n",
      "When epoch is 835, training loss is 0.22783 and validation loss is 0.27238.\n",
      "When epoch is 836, training loss is 0.22763 and validation loss is 0.27212.\n",
      "When epoch is 837, training loss is 0.22743 and validation loss is 0.27185.\n",
      "When epoch is 838, training loss is 0.22723 and validation loss is 0.27158.\n",
      "When epoch is 839, training loss is 0.22704 and validation loss is 0.27131.\n",
      "When epoch is 840, training loss is 0.22684 and validation loss is 0.27104.\n",
      "When epoch is 841, training loss is 0.22664 and validation loss is 0.27078.\n",
      "When epoch is 842, training loss is 0.22644 and validation loss is 0.27051.\n",
      "When epoch is 843, training loss is 0.22624 and validation loss is 0.27025.\n",
      "When epoch is 844, training loss is 0.22605 and validation loss is 0.26999.\n",
      "When epoch is 845, training loss is 0.22585 and validation loss is 0.26973.\n",
      "When epoch is 846, training loss is 0.22566 and validation loss is 0.26946.\n",
      "When epoch is 847, training loss is 0.22546 and validation loss is 0.26919.\n",
      "When epoch is 848, training loss is 0.22526 and validation loss is 0.26894.\n",
      "When epoch is 849, training loss is 0.22507 and validation loss is 0.26867.\n",
      "When epoch is 850, training loss is 0.22487 and validation loss is 0.26841.\n",
      "When epoch is 851, training loss is 0.22468 and validation loss is 0.26815.\n",
      "When epoch is 852, training loss is 0.22449 and validation loss is 0.26789.\n",
      "When epoch is 853, training loss is 0.22429 and validation loss is 0.26763.\n",
      "When epoch is 854, training loss is 0.22410 and validation loss is 0.26736.\n",
      "When epoch is 855, training loss is 0.22391 and validation loss is 0.26711.\n",
      "When epoch is 856, training loss is 0.22371 and validation loss is 0.26685.\n",
      "When epoch is 857, training loss is 0.22352 and validation loss is 0.26659.\n",
      "When epoch is 858, training loss is 0.22333 and validation loss is 0.26633.\n",
      "When epoch is 859, training loss is 0.22314 and validation loss is 0.26607.\n",
      "When epoch is 860, training loss is 0.22295 and validation loss is 0.26582.\n",
      "When epoch is 861, training loss is 0.22276 and validation loss is 0.26555.\n",
      "When epoch is 862, training loss is 0.22257 and validation loss is 0.26530.\n",
      "When epoch is 863, training loss is 0.22238 and validation loss is 0.26504.\n",
      "When epoch is 864, training loss is 0.22219 and validation loss is 0.26478.\n",
      "When epoch is 865, training loss is 0.22200 and validation loss is 0.26453.\n",
      "When epoch is 866, training loss is 0.22181 and validation loss is 0.26428.\n",
      "When epoch is 867, training loss is 0.22162 and validation loss is 0.26402.\n",
      "When epoch is 868, training loss is 0.22143 and validation loss is 0.26377.\n",
      "When epoch is 869, training loss is 0.22124 and validation loss is 0.26351.\n",
      "When epoch is 870, training loss is 0.22106 and validation loss is 0.26326.\n",
      "When epoch is 871, training loss is 0.22087 and validation loss is 0.26301.\n",
      "When epoch is 872, training loss is 0.22068 and validation loss is 0.26275.\n",
      "When epoch is 873, training loss is 0.22050 and validation loss is 0.26250.\n",
      "When epoch is 874, training loss is 0.22031 and validation loss is 0.26225.\n",
      "When epoch is 875, training loss is 0.22012 and validation loss is 0.26200.\n",
      "When epoch is 876, training loss is 0.21994 and validation loss is 0.26175.\n",
      "When epoch is 877, training loss is 0.21975 and validation loss is 0.26150.\n",
      "When epoch is 878, training loss is 0.21957 and validation loss is 0.26125.\n",
      "When epoch is 879, training loss is 0.21938 and validation loss is 0.26100.\n",
      "When epoch is 880, training loss is 0.21920 and validation loss is 0.26075.\n",
      "When epoch is 881, training loss is 0.21902 and validation loss is 0.26050.\n",
      "When epoch is 882, training loss is 0.21883 and validation loss is 0.26025.\n",
      "When epoch is 883, training loss is 0.21865 and validation loss is 0.26000.\n",
      "When epoch is 884, training loss is 0.21847 and validation loss is 0.25976.\n",
      "When epoch is 885, training loss is 0.21828 and validation loss is 0.25951.\n",
      "When epoch is 886, training loss is 0.21810 and validation loss is 0.25926.\n",
      "When epoch is 887, training loss is 0.21792 and validation loss is 0.25902.\n",
      "When epoch is 888, training loss is 0.21774 and validation loss is 0.25877.\n",
      "When epoch is 889, training loss is 0.21756 and validation loss is 0.25853.\n",
      "When epoch is 890, training loss is 0.21738 and validation loss is 0.25828.\n",
      "When epoch is 891, training loss is 0.21720 and validation loss is 0.25803.\n",
      "When epoch is 892, training loss is 0.21702 and validation loss is 0.25779.\n",
      "When epoch is 893, training loss is 0.21684 and validation loss is 0.25755.\n",
      "When epoch is 894, training loss is 0.21666 and validation loss is 0.25730.\n",
      "When epoch is 895, training loss is 0.21648 and validation loss is 0.25706.\n",
      "When epoch is 896, training loss is 0.21630 and validation loss is 0.25682.\n",
      "When epoch is 897, training loss is 0.21612 and validation loss is 0.25657.\n",
      "When epoch is 898, training loss is 0.21594 and validation loss is 0.25634.\n",
      "When epoch is 899, training loss is 0.21577 and validation loss is 0.25609.\n",
      "When epoch is 900, training loss is 0.21559 and validation loss is 0.25585.\n",
      "When epoch is 901, training loss is 0.21541 and validation loss is 0.25562.\n",
      "When epoch is 902, training loss is 0.21523 and validation loss is 0.25537.\n",
      "When epoch is 903, training loss is 0.21506 and validation loss is 0.25513.\n",
      "When epoch is 904, training loss is 0.21488 and validation loss is 0.25489.\n",
      "When epoch is 905, training loss is 0.21471 and validation loss is 0.25465.\n",
      "When epoch is 906, training loss is 0.21453 and validation loss is 0.25441.\n",
      "When epoch is 907, training loss is 0.21436 and validation loss is 0.25418.\n",
      "When epoch is 908, training loss is 0.21418 and validation loss is 0.25394.\n",
      "When epoch is 909, training loss is 0.21401 and validation loss is 0.25370.\n",
      "When epoch is 910, training loss is 0.21383 and validation loss is 0.25347.\n",
      "When epoch is 911, training loss is 0.21366 and validation loss is 0.25323.\n",
      "When epoch is 912, training loss is 0.21348 and validation loss is 0.25299.\n",
      "When epoch is 913, training loss is 0.21331 and validation loss is 0.25276.\n",
      "When epoch is 914, training loss is 0.21314 and validation loss is 0.25252.\n",
      "When epoch is 915, training loss is 0.21297 and validation loss is 0.25228.\n",
      "When epoch is 916, training loss is 0.21279 and validation loss is 0.25206.\n",
      "When epoch is 917, training loss is 0.21262 and validation loss is 0.25182.\n",
      "When epoch is 918, training loss is 0.21245 and validation loss is 0.25158.\n",
      "When epoch is 919, training loss is 0.21228 and validation loss is 0.25135.\n",
      "When epoch is 920, training loss is 0.21211 and validation loss is 0.25112.\n",
      "When epoch is 921, training loss is 0.21194 and validation loss is 0.25088.\n",
      "When epoch is 922, training loss is 0.21177 and validation loss is 0.25066.\n",
      "When epoch is 923, training loss is 0.21160 and validation loss is 0.25042.\n",
      "When epoch is 924, training loss is 0.21143 and validation loss is 0.25019.\n",
      "When epoch is 925, training loss is 0.21126 and validation loss is 0.24996.\n",
      "When epoch is 926, training loss is 0.21109 and validation loss is 0.24973.\n",
      "When epoch is 927, training loss is 0.21092 and validation loss is 0.24950.\n",
      "When epoch is 928, training loss is 0.21075 and validation loss is 0.24927.\n",
      "When epoch is 929, training loss is 0.21058 and validation loss is 0.24904.\n",
      "When epoch is 930, training loss is 0.21042 and validation loss is 0.24881.\n",
      "When epoch is 931, training loss is 0.21025 and validation loss is 0.24858.\n",
      "When epoch is 932, training loss is 0.21008 and validation loss is 0.24835.\n",
      "When epoch is 933, training loss is 0.20991 and validation loss is 0.24812.\n",
      "When epoch is 934, training loss is 0.20975 and validation loss is 0.24789.\n",
      "When epoch is 935, training loss is 0.20958 and validation loss is 0.24767.\n",
      "When epoch is 936, training loss is 0.20942 and validation loss is 0.24744.\n",
      "When epoch is 937, training loss is 0.20925 and validation loss is 0.24721.\n",
      "When epoch is 938, training loss is 0.20909 and validation loss is 0.24699.\n",
      "When epoch is 939, training loss is 0.20892 and validation loss is 0.24676.\n",
      "When epoch is 940, training loss is 0.20876 and validation loss is 0.24654.\n",
      "When epoch is 941, training loss is 0.20859 and validation loss is 0.24632.\n",
      "When epoch is 942, training loss is 0.20843 and validation loss is 0.24609.\n",
      "When epoch is 943, training loss is 0.20826 and validation loss is 0.24586.\n",
      "When epoch is 944, training loss is 0.20810 and validation loss is 0.24564.\n",
      "When epoch is 945, training loss is 0.20794 and validation loss is 0.24542.\n",
      "When epoch is 946, training loss is 0.20777 and validation loss is 0.24519.\n",
      "When epoch is 947, training loss is 0.20761 and validation loss is 0.24497.\n",
      "When epoch is 948, training loss is 0.20745 and validation loss is 0.24475.\n",
      "When epoch is 949, training loss is 0.20729 and validation loss is 0.24453.\n",
      "When epoch is 950, training loss is 0.20713 and validation loss is 0.24430.\n",
      "When epoch is 951, training loss is 0.20696 and validation loss is 0.24409.\n",
      "When epoch is 952, training loss is 0.20680 and validation loss is 0.24387.\n",
      "When epoch is 953, training loss is 0.20664 and validation loss is 0.24364.\n",
      "When epoch is 954, training loss is 0.20648 and validation loss is 0.24342.\n",
      "When epoch is 955, training loss is 0.20632 and validation loss is 0.24321.\n",
      "When epoch is 956, training loss is 0.20616 and validation loss is 0.24298.\n",
      "When epoch is 957, training loss is 0.20600 and validation loss is 0.24276.\n",
      "When epoch is 958, training loss is 0.20584 and validation loss is 0.24255.\n",
      "When epoch is 959, training loss is 0.20568 and validation loss is 0.24233.\n",
      "When epoch is 960, training loss is 0.20553 and validation loss is 0.24211.\n",
      "When epoch is 961, training loss is 0.20537 and validation loss is 0.24189.\n",
      "When epoch is 962, training loss is 0.20521 and validation loss is 0.24168.\n",
      "When epoch is 963, training loss is 0.20505 and validation loss is 0.24146.\n",
      "When epoch is 964, training loss is 0.20489 and validation loss is 0.24124.\n",
      "When epoch is 965, training loss is 0.20474 and validation loss is 0.24103.\n",
      "When epoch is 966, training loss is 0.20458 and validation loss is 0.24081.\n",
      "When epoch is 967, training loss is 0.20442 and validation loss is 0.24059.\n",
      "When epoch is 968, training loss is 0.20427 and validation loss is 0.24038.\n",
      "When epoch is 969, training loss is 0.20411 and validation loss is 0.24017.\n",
      "When epoch is 970, training loss is 0.20396 and validation loss is 0.23995.\n",
      "When epoch is 971, training loss is 0.20380 and validation loss is 0.23973.\n",
      "When epoch is 972, training loss is 0.20365 and validation loss is 0.23952.\n",
      "When epoch is 973, training loss is 0.20349 and validation loss is 0.23931.\n",
      "When epoch is 974, training loss is 0.20334 and validation loss is 0.23910.\n",
      "When epoch is 975, training loss is 0.20318 and validation loss is 0.23888.\n",
      "When epoch is 976, training loss is 0.20303 and validation loss is 0.23868.\n",
      "When epoch is 977, training loss is 0.20287 and validation loss is 0.23846.\n",
      "When epoch is 978, training loss is 0.20272 and validation loss is 0.23825.\n",
      "When epoch is 979, training loss is 0.20257 and validation loss is 0.23803.\n",
      "When epoch is 980, training loss is 0.20242 and validation loss is 0.23783.\n",
      "When epoch is 981, training loss is 0.20226 and validation loss is 0.23762.\n",
      "When epoch is 982, training loss is 0.20211 and validation loss is 0.23741.\n",
      "When epoch is 983, training loss is 0.20196 and validation loss is 0.23719.\n",
      "When epoch is 984, training loss is 0.20181 and validation loss is 0.23699.\n",
      "When epoch is 985, training loss is 0.20166 and validation loss is 0.23678.\n",
      "When epoch is 986, training loss is 0.20150 and validation loss is 0.23657.\n",
      "When epoch is 987, training loss is 0.20135 and validation loss is 0.23636.\n",
      "When epoch is 988, training loss is 0.20120 and validation loss is 0.23616.\n",
      "When epoch is 989, training loss is 0.20105 and validation loss is 0.23594.\n",
      "When epoch is 990, training loss is 0.20090 and validation loss is 0.23574.\n",
      "When epoch is 991, training loss is 0.20075 and validation loss is 0.23553.\n",
      "When epoch is 992, training loss is 0.20060 and validation loss is 0.23532.\n",
      "When epoch is 993, training loss is 0.20045 and validation loss is 0.23512.\n",
      "When epoch is 994, training loss is 0.20031 and validation loss is 0.23491.\n",
      "When epoch is 995, training loss is 0.20016 and validation loss is 0.23471.\n",
      "When epoch is 996, training loss is 0.20001 and validation loss is 0.23450.\n",
      "When epoch is 997, training loss is 0.19986 and validation loss is 0.23430.\n",
      "When epoch is 998, training loss is 0.19971 and validation loss is 0.23409.\n",
      "When epoch is 999, training loss is 0.19957 and validation loss is 0.23389.\n",
      "When epoch is 1000, training loss is 0.19942 and validation loss is 0.23369.\n",
      "When epoch is 1001, training loss is 0.19927 and validation loss is 0.23348.\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000+1\n",
    "lr = 1e-2\n",
    "\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass: training\n",
    "    Y_train_prediction = [model(xs) for xs in X_train]\n",
    "    loss_train = categorical_cross_entrorpy(Y_train, Y_train_prediction)\n",
    "\n",
    "    Y_val_prediction = [model(xs) for xs in X_val]\n",
    "    loss_val = categorical_cross_entrorpy(Y_val, Y_val_prediction)\n",
    "\n",
    "    # Backward pass:\n",
    "    model.zero_grad()\n",
    "    loss_train.backward()\n",
    "\n",
    "    # Update:\n",
    "    for parameter in model.parameters():\n",
    "        parameter.data -= lr * parameter.grad\n",
    "    #end-for\n",
    "\n",
    "    print(f'When epoch is {epoch+1}, training loss is {loss_train.data:0.5f} and validation loss is {loss_val.data:0.5f}.')\n",
    "    \n",
    "    losses_train.append(loss_train.data)\n",
    "    losses_val.append(loss_val.data)\n",
    "#end-for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86a95e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAfQFJREFUeJzt3XV4k1cbBvA7dVcoTWmpYIUihRbfkOGuQ4ZuDApDxhgybMjwsWEbOlyLDxgfbsOlLYPhUKSQ4tSpnu+PLqGppEmbNJX7d125Rk7O+75P3qbNs6MSIYQAERERUSFhoO8AiIiIiLSJyQ0REREVKkxuiIiIqFBhckNERESFCpMbIiIiKlSY3BAREVGhwuSGiIiIChUmN0RERFSoMLkhIiKiQoXJjR6tXbsWEokEV65cyfT1Nm3awMPDI2+D0oBEIsGUKVP0HUam8mtsgYGB8PHxgbm5OSQSCUJCQvQWy7lz5zBlyhS8f/8+w2sNGzZEw4YN8zwmdTx69AgSiQTz5s3TdygE1Z8jbejXr1+O/w7K/8Y+evRIqzFpy+bNm7FgwQJ9h6Fw7tw59OjRAx4eHrCwsEDlypWxfv16fYeVI0xuiPLIq1ev0Lt3b5QuXRoHDx7E+fPnUa5cOb3Fc+7cOUydOjXTL6UlS5ZgyZIleR8UFTiqPkfaMGnSJOzevTtHx7Zu3Rrnz5+HVCrVclTakd+Sm4kTJ0IIgQULFmDv3r2oUKEC+vbtm+P7r09G+g6AqKi4e/cuEhMT0atXLzRo0EDf4ahUsWJFfYdA+UhsbCwsLCy0cq64uDiYm5urXb906dI5vlbx4sVRvHjxHB9f1GzZsgUlSpRQPG/UqBEOHTqEXbt2oWPHjnqMTHNsuSlgPnz4gHHjxsHT0xMmJiYoWbIkhgwZovR/TaNHj4atrS2Sk5MVZcOGDYNEIsHPP/+sKHvz5g0MDAywePFildeMjIzEgAED4OjoCCsrK7Ro0QJ3797NtO6ZM2fQuHFjWFtbw8LCAnXr1sVff/2VoV54eDgCAgLg6uoKExMTeHp6YurUqUhKSlKqt3TpUlStWhVWVlawtraGt7c3xo8fr86tyuDGjRto37497O3tYWZmBl9fX6xbt06pTkpKCqZPn47y5cvD3NwcdnZ2qFKlChYuXKio8+rVKwwcOBBubm4wNTVF8eLFUa9ePRw9ejTLa/fr1w+ffPIJAKBbt26QSCSKbp+suoDSN8en7Y759ddf4enpCSsrK9SpUwcXLlzIcPzFixfRtm1bODo6wszMDKVLl8aIESMAAFOmTMHo0aMBAJ6enpBIJJBIJDh58mSWMb19+xbffPMNSpYsCRMTE3h5eWHChAmIj49XqieRSDB06FBs2LABFSpUgIWFBapWrYr9+/dneX/Sev/+Pb7//nt4eXnB1NQUTk5OaNWqFW7fvp2hbnb34cqVK+jevTs8PDxgbm4ODw8P9OjRA48fP1aqJ+++OHHiBAYPHoxixYrB0dERnTp1wvPnz5XqxsfH4/vvv4ezszMsLCxQv359XL16FR4eHujXr59SXXU/55lJSUnB3Llz4e3trbgPffr0QVhYmKLOiBEjYGlpicjIyAzHd+vWDSVKlEBiYqKiLDAwEHXq1IGlpSWsrKzQvHlzBAcHKx3Xr18/WFlZ4fr162jWrBmsra3RuHHjTGPM7nPk4eGBNm3aYNeuXahWrRrMzMwwdepUAMDvv/+O+vXrw8nJCZaWlqhcuTLmzp2rFK88nvTdUup+xjLrlmrYsCEqVaqEy5cv49NPP4WFhQW8vLwwe/ZspKSkKB3/77//olmzZrCwsEDx4sUxZMgQ/PXXX0rvMSvZ/Z1o2LAh/vrrLzx+/Fhx3yQSieL4hIQETJ8+XfHzL168OL788ku8evVK6Trye7x7925UqVIFZmZm8PLywqJFi5TqqfO3LW1iAwBhYWGIjo5GsWLFVL7XfEmQ3qxZs0YAEBcuXBCJiYkZHq1atRLu7u6K+ikpKaJ58+bCyMhITJo0SRw+fFjMmzdPWFpaimrVqokPHz4IIYQ4ePCgACDOnTunONbb21uYm5uLpk2bKsoCAwMFAHHz5s0sY0xJSRGNGjUSpqamYsaMGeLw4cNi8uTJwsvLSwAQkydPVtQ9efKkMDY2Fn5+fiIwMFDs2bNHNGvWTEgkErF161ZFPZlMJtzc3IS7u7tYvny5OHr0qPjpp5+Eqamp6Nevn6Leli1bBAAxbNgwcfjwYXH06FGxbNkyMXz48GzvbfrYbt++LaytrUXp0qXF+vXrxV9//SV69OghAIg5c+Yo6s2aNUsYGhqKyZMni2PHjomDBw+KBQsWiClTpijqNG/eXBQvXlysWLFCnDx5UuzZs0f8+OOPSu8xvfv374vff/9dABAzZ84U58+fF//++68QQogGDRqIBg0aZDimb9++Sj//0NBQAUB4eHiIFi1aiD179og9e/aIypUrC3t7e/H+/XtF3YMHDwpjY2NRpUoVsXbtWnH8+HGxevVq0b17dyGEEE+fPhXDhg0TAMSuXbvE+fPnxfnz50VERESmMcXFxYkqVaoIS0tLMW/ePHH48GExadIkYWRkJFq1apXh3nt4eIiaNWuKbdu2iQMHDoiGDRsKIyMj8eDBg6x/aEKIyMhI4ePjIywtLcW0adPEoUOHxM6dO8W3334rjh8/rvF92L59u/jxxx/F7t27xalTp8TWrVtFgwYNRPHixcWrV68U9eS/i15eXmLYsGHi0KFD4o8//hD29vaiUaNGSjH26NFDGBgYiB9++EEcPnxYLFiwQLi5uQlbW1vRt29fRT11P+dZGThwoAAghg4dKg4ePCiWLVsmihcvLtzc3BSxX7t2TQAQK1euVDr23bt3wtTUVIwcOVJRNmPGDCGRSMRXX30l9u/fL3bt2iXq1KkjLC0tFZ9FIVI/d8bGxsLDw0PMmjVLHDt2TBw6dCjTGLP7HLm7uwupVCq8vLzE6tWrxYkTJ8SlS5eEEEJ89913YunSpeLgwYPi+PHjYv78+aJYsWLiyy+/VLpG+t8DIdT/jMl/rqGhoYqyBg0aCEdHR1G2bFmxbNkyceTIEfHNN98IAGLdunWKes+fPxeOjo6iVKlSYu3ateLAgQOid+/ewsPDQwAQJ06cUPnzy+7vxL///ivq1asnnJ2dFfft/PnzQgghkpOTRYsWLYSlpaWYOnWqOHLkiPjjjz9EyZIlRcWKFUVsbKziOu7u7qJkyZKiVKlSYvXq1eLAgQOiZ8+eAoD4+eefFfXU+duW1suXL0WVKlVEyZIlhUwmU/le8yMmN3ok/8VT9Uj7Sy1PWubOnat0HnmSsmLFCiGEEDExMcLExERMmzZNCCFEWFiYACDGjh0rzM3NFUnQgAEDhIuLi8oY//e//wkAYuHChUrlM2bMyJBA1K5dWzg5OYmoqChFWVJSkqhUqZJwdXUVKSkpQgghAgIChJWVlXj8+LHSOefNmycAKP7QDh06VNjZ2WV3GzOVPrbu3bsLU1NT8eTJE6V6LVu2FBYWFoovxDZt2ghfX1+V57ayshIjRozQOKYTJ04IAGL79u1K5ZomN5UrVxZJSUmK8kuXLgkAYsuWLYqy0qVLi9KlS4u4uLgs4/n5558z/OHPKqZly5YJAGLbtm1K9ebMmSMAiMOHDyvKAIgSJUqIyMhIRVl4eLgwMDAQs2bNyjIeIYSYNm2aACCOHDmSZR1N7kN6SUlJIjo6WlhaWip9puW/i998841S/blz5woAij/u//77r+J3KS15Ip42uVH3c56ZW7duZRrPxYsXBQAxfvx4RVn16tVF3bp1leotWbJEABDXr18XQgjx5MkTYWRkJIYNG6ZULyoqSjg7O4uuXbsqyvr27SsAiNWrV2cZX1qqPkfu7u7C0NBQ3LlzR+U5kpOTRWJioli/fr0wNDQUb9++VYons+RGnc9YVskNAHHx4kWlc1asWFE0b95c8Xz06NFCIpFk+Dk1b95creRGnb8TrVu3zvDehPj4edq5c6dS+eXLlwUAsWTJEkWZu7u7kEgkIiQkRKlu06ZNhY2NjYiJiRFCqPe3Te7du3eiUqVKwtnZWdy6dUutY/IbdkvlA+vXr8fly5czPOTdGHLHjx8HgAxN359//jksLS1x7NgxAICFhQXq1KmjaP48cuQI7OzsMHr0aCQkJODMmTMAgKNHj6JJkyYqYztx4gQAoGfPnkrlX3zxhdLzmJgYXLx4EV26dIGVlZWi3NDQEL1790ZYWBju3LkDANi/fz8aNWoEFxcXJCUlKR4tW7YEAJw6dQoAULNmTbx//x49evTAn3/+idevX6uMVZXjx4+jcePGcHNzUyrv168fYmNjcf78ecU1r127hm+++QaHDh3KtLm/Zs2aWLt2LaZPn44LFy5kaEbXtdatW8PQ0FDxvEqVKgCg6Gq5e/cuHjx4gP79+8PMzEwr1zx+/DgsLS3RpUsXpXL5Z1H+2ZNr1KgRrK2tFc9LlCgBJyenDN1B6f3vf/9DuXLlsv1cAtnfBwCIjo7G2LFjUaZMGRgZGcHIyAhWVlaIiYnBrVu3MpyzXbt2Ss/Tn1P+2ezatatSvS5dusDISHkIo7qf88zIf+/S/67XrFkTFSpUULrfX375Jc6dO6f4/QKANWvWoEaNGqhUqRIA4NChQ0hKSkKfPn2UYjEzM0ODBg0y7WLp3LlzlvFpokqVKpkOnA8ODka7du3g6OgIQ0NDGBsbo0+fPkhOTs6y2zutnH7GAMDZ2Rk1a9bMEGfaY0+dOoVKlSplGH/Wo0ePbM8P5O7vxP79+2FnZ4e2bdsq/bx8fX3h7Oyc4efl4+ODqlWrKpV98cUXiIyMRFBQkCKe7P62yc2dOxc3b97EgQMH4O3trXbc+QmTm3ygQoUK8Pf3z/CwtbVVqvfmzRsYGRllGCAnkUjg7OyMN2/eKMqaNGmCCxcuICYmBkePHsVnn30GR0dH+Pn54ejRowgNDUVoaGi2XyLyazo6OiqVOzs7Kz1/9+4dhBCZzkpwcXFRnAsAXrx4gX379sHY2Fjp4ePjAwCKJKZ3795YvXo1Hj9+jM6dO8PJyQm1atXCkSNHVMac1ftQJ7Zx48Zh3rx5uHDhAlq2bAlHR0c0btxYabp+YGAg+vbtiz/++AN16tSBg4MD+vTpg/DwcI3jyon0PwtTU1MAqQM1ASj65F1dXbV2zTdv3sDZ2VlpTAAAODk5wcjISOmzl1mM8jjlMWbl1atXased3X0AUv/A//bbb/j6669x6NAhXLp0CZcvX0bx4sUzjSW7c8rfZ/qxCZn9jqj7Oc+M/DpZfWbT3u+ePXvC1NQUa9euBQDcvHkTly9fxpdffqkUCwDUqFEjQzyBgYEZYrGwsICNjU2W8Wkis/fw5MkTfPrpp3j27BkWLlyIv//+G5cvX8bvv/8OANl+ToCcf8bUPfbNmzcZfs5Axp99VnLzd+LFixd4//49TExMMvy8wsPDM/y80v89Tlumyd82uZs3b8LFxQXVqlVT673mR5wtVYA4OjoiKSkJr169UkpwhBAIDw9HjRo1FGWNGzfGpEmTcPr0aRw7dgyTJ09WlB8+fBienp6K5+pc882bN0p/ENL/gtrb28PAwAAymSzDOeQDMuWD0ooVK4YqVapgxowZmV5TnnAAqf9X+uWXXyImJganT5/G5MmT0aZNG9y9exfu7u4qY0//PtSJzcjICCNHjsTIkSPx/v17HD16FOPHj0fz5s3x9OlTWFhYoFixYliwYAEWLFiAJ0+eYO/evfjhhx/w8uVLHDx4UO2Y5MzMzBAREZGhPKctVfLPRtqBp7nl6OiIixcvQgihlOC8fPkSSUlJWhtwWLx4ca3FHRERgf3792Py5Mn44YcfFOXx8fF4+/Ztjs4p/x148eIFSpYsqSiX/46kpcnnPKvryGSyDMne8+fPle63vb092rdvj/Xr12P69OlYs2YNzMzMlFoY5PV37Nih1u9N+iQ2NzI71549exATE4Ndu3YpxaPPdZ/Sc3R0VCSFaan7PzG5+TshH9CeVb20LVZZxSQvk3+W1PnbJieVSvW6TIU2sOWmAJEnIhs3blQq37lzJ2JiYpQSlZo1a8LGxgYLFixAeHg4mjZtCiC1RSc4OBjbtm1DxYoVVf6BBVKbfgFg06ZNSuWbN29Wem5paYlatWph165dSv/3k5KSgo0bN8LV1VXxy9KmTRvcuHEDpUuXzrTFKrOYLC0t0bJlS0yYMAEJCQn4999/VcadXuPGjXH8+PEMM1/Wr18PCwsL1K5dO8MxdnZ26NKlC4YMGYK3b99muhBYqVKlMHToUDRt2lTR/KspDw8P3L17V2nW0Zs3b3Du3Lkcna9cuXIoXbo0Vq9enWEmU1qZtXRkpXHjxoiOjsaePXuUyuULfGWXJKurZcuWuHv3rqILNjckEgmEEIr3KffHH38ozSTURP369QGk/l95Wjt27MgwAyonn3O5zz77DEDG3/XLly/j1q1bGe73l19+iefPn+PAgQPYuHEjOnbsCDs7O8XrzZs3h5GRER48eJBpLP7+/hrfCzlNPkdy8oQn7c9GCIGVK1fmOA5ta9CgAW7cuIGbN28qlW/dulXjc2X1dyKrlqY2bdrgzZs3SE5OzvRnVb58eaX6//77L65du6ZUtnnzZlhbW6N69eoZzp/d37alS5dm6GouaNhyU4A0bdoUzZs3x9ixYxEZGYl69erhn3/+weTJk1GtWjX07t1bUdfQ0BANGjTAvn374OnpqVgrol69ejA1NcWxY8cwfPjwbK/ZrFkz1K9fH2PGjEFMTAz8/f1x9uxZbNiwIUPdWbNmoWnTpmjUqBFGjRoFExMTLFmyBDdu3MCWLVsUf9CmTZuGI0eOoG7duhg+fDjKly+PDx8+4NGjRzhw4ACWLVsGV1dXDBgwAObm5qhXrx6kUinCw8Mxa9Ys2NraKrVSqWPy5MmKMRA//vgjHBwcsGnTJvz111+YO3euoguwbdu2qFSpEvz9/VG8eHE8fvwYCxYsgLu7O8qWLYuIiAg0atQIX3zxBby9vWFtbY3Lly/j4MGD6NSpk0YxyfXu3RvLly9Hr169MGDAALx58wZz587NVbfA77//jrZt26J27dr47rvvUKpUKTx58gSHDh1SJKqVK1cGACxcuBB9+/aFsbExypcvn+H/CgGgT58++P3339G3b188evQIlStXxpkzZzBz5ky0atVKrTEy6hgxYgQCAwPRvn17/PDDD6hZsybi4uJw6tQptGnTRpFsq8PGxgb169fHzz//jGLFisHDwwOnTp3CqlWrlL74NeHj44MePXrgl19+gaGhIT777DP8+++/+OWXX2BrawsDg4//v6ju5zwz5cuXx8CBA7F48WIYGBigZcuWePToESZNmgQ3Nzd89913SvWbNWsGV1dXfPPNNwgPD1fqkgJSE+hp06ZhwoQJePjwIVq0aAF7e3u8ePECly5dgqWlpWKKtqY0+RzJNW3aFCYmJujRowfGjBmDDx8+YOnSpXj37l2OYtCFESNGYPXq1WjZsiWmTZuGEiVKYPPmzYolCdL+rNNT9+9E5cqVsWvXLixduhR+fn4wMDCAv78/unfvjk2bNqFVq1b49ttvUbNmTRgbGyMsLAwnTpxA+/btldadcXFxQbt27TBlyhRIpVJs3LgRR44cwZw5cxQtMtn9bUurcePGePz4Me7fv6/NW5q39DqcuYiTj+S/fPlypq9nNpI+Li5OjB07Vri7uwtjY2MhlUrF4MGDxbt37zIcv3DhQgFADBgwQKm8adOmAoDYu3evWnG+f/9efPXVV8LOzk5YWFiIpk2bitu3b2eYkSSEEH///bf47LPPhKWlpTA3Nxe1a9cW+/bty3DOV69eieHDhwtPT09hbGwsHBwchJ+fn5gwYYKIjo4WQgixbt060ahRI1GiRAlhYmIiXFxcRNeuXcU///yTbcyZxXb9+nXRtm1bYWtrK0xMTETVqlXFmjVrlOr88ssvom7duqJYsWLCxMRElCpVSvTv3188evRICCHEhw8fxKBBg0SVKlWEjY2NMDc3F+XLlxeTJ09WzErISlazpeTvtUKFCsLMzExUrFhRBAYGZjlbKu30TlXv9/z586Jly5bC1tZWmJqaitKlS4vvvvtOqc64ceOEi4uLMDAwUJoBktkMrjdv3ohBgwYJqVQqjIyMhLu7uxg3bpxi9l3aWIYMGZIhRnd3d6XZRFl59+6d+Pbbb0WpUqWEsbGxcHJyEq1btxa3b9/W+D6EhYWJzp07C3t7e2FtbS1atGghbty4kSGWrH4X5T+ztDNjPnz4IEaOHCmcnJyEmZmZqF27tjh//rywtbXNcH/V+ZxnJTk5WcyZM0eUK1dOGBsbi2LFiolevXqJp0+fZlp//PjxAoBwc3MTycnJmdbZs2ePaNSokbCxsRGmpqbC3d1ddOnSRRw9elRRp2/fvsLS0lJlbOll9Tlyd3cXrVu3zvSYffv2iapVqwozMzNRsmRJMXr0aMXszLT3O6vZUup8xrKaLeXj45Ph2Myuc+PGDdGkSRNhZmYmHBwcRP/+/cW6desEAHHt2rUs74e6fyfevn0runTpIuzs7IREIhFpv5ITExPFvHnzFPfIyspKeHt7i4CAAHHv3j2l99y6dWuxY8cO4ePjI0xMTISHh4f49ddflWLK7m9bWg0aNMh0FldBIhFCiDzNpoiICplz586hXr162LRpU4aZhFS4DBw4EFu2bMGbN29gYmKi73Dg4eGBSpUqqb1IZlHBbikiIg0cOXIE58+fh5+fH8zNzXHt2jXMnj0bZcuWzXHXJOVP06ZNg4uLC7y8vBAdHY39+/fjjz/+wMSJE/NFYkNZY3JDRKQBGxsbHD58GAsWLEBUVBSKFSuGli1bYtasWVpbV4jyB2NjY/z8888ICwtDUlISypYti19//RXffvutvkOjbLBbioiIiAoVTgUnIiKiQoXJDRERERUqTG6IiIioUClyA4pTUlLw/PlzWFtba3WJcSIiItIdIQSioqLg4uKichFFoAgmN8+fP8+wMzQREREVDE+fPs12g90il9zIlwR/+vSp1na9JSIiIt2KjIyEm5ubyq095IpcciPvirKxsWFyQ0REVMCoM6SEA4qJiIioUGFyQ0RERIUKkxsiIiIqVIrcmBsiIsq95ORkJCYm6jsMKmRMTEyyneatDiY3RESkNiEEwsPD8f79e32HQoWQgYEBPD09c73rOpMbIiJSmzyxcXJygoWFBRdDJa2RL7Irk8lQqlSpXH22mNwQEZFakpOTFYmNo6OjvsOhQqh48eJ4/vw5kpKSYGxsnOPzcEAxERGpRT7GxsLCQs+RUGEl745KTk7O1XmY3BARkUbYFUW6oq3PFpMbIiIiKlSY3GiRLEqGKSenQBYl03coRESkQw0bNsSIESPUrv/o0SNIJBKEhIToLCb6iMmNFsmiZZh6aipk0UxuiIjyA4lEovLRr1+/HJ13165d+Omnn9Su7+bmBplMhkqVKuXoeupiEpWKs6W05PKzy9hwbQMAIPR9KKpLq+s5IiIiksk+/s9mYGAgfvzxR9y5c0dRZm5urlQ/MTFRrVk6Dg4OGsVhaGgIZ2dnjY6hnGPLTS7JomQIkgVh8/XNWHx5MQDgyIMjCJIFIUgWxC4qIqLMhIUBJ06k/leHnJ2dFQ9bW1tIJBLF8w8fPsDOzg7btm1Dw4YNYWZmho0bN+LNmzfo0aMHXF1dYWFhgcqVK2PLli1K503fLeXh4YGZM2fiq6++grW1NUqVKoUVK1YoXk/fonLy5ElIJBIcO3YM/v7+sLCwQN26dZUSLwCYPn06nJycYG1tja+//ho//PADfH19c3w/4uPjMXz4cDg5OcHMzAyffPIJLl++rHj93bt36NmzJ4oXLw5zc3OULVsWa9asAQAkJCRg6NChkEqlMDMzg4eHB2bNmpXjWHSJyU0uLb+6HH4r/LDg4oIMZX4r/LD86nL9BUdElB+tWgW4uwOffZb631Wr9BrO2LFjMXz4cNy6dQvNmzfHhw8f4Ofnh/379+PGjRsYOHAgevfujYsXL6o8zy+//AJ/f38EBwfjm2++weDBg3H79m2Vx0yYMAG//PILrly5AiMjI3z11VeK1zZt2oQZM2Zgzpw5uHr1KkqVKoWlS5fm6r2OGTMGO3fuxLp16xAUFIQyZcqgefPmePv2LQBg0qRJuHnzJv73v//h1q1bWLp0KYoVKwYAWLRoEfbu3Ytt27bhzp072LhxIzw8PHIVj66wWyqXAvwC0K58O2y9vhU/n/8ZANC/Wn98U+MbAIDUSqrP8IiIdM/fHwgPV69ucrJy3ZQU4OuvgYkTAUND9a/p7AxcuaJZnFkYMWIEOnXqpFQ2atQoxb+HDRuGgwcPYvv27ahVq1aW52nVqhW++Sb1b//YsWMxf/58nDx5Et7e3lkeM2PGDDRo0AAA8MMPP6B169b48OEDzMzMsHjxYvTv3x9ffvklAODHH3/E4cOHER0dnaP3GRMTg6VLl2Lt2rVo2bIlAGDlypU4cuQIVq1ahdGjR+PJkyeoVq0a/P39AUApeXny5AnKli2LTz75BBKJBO7u7jmKIy8wucklqbUUUmspzj89ryhzt3XnmBsiKjrCw4Fnz3J/Dj2Rf5HLJScnY/bs2QgMDMSzZ88QHx+P+Ph4WFpaqjxPlSpVFP+Wd3+9fPlS7WOk0tT/GX758iVKlSqFO3fuKJIluZo1a+L48eNqva/0Hjx4gMTERNSrV09RZmxsjJo1a+LWrVsAgMGDB6Nz584ICgpCs2bN0KFDB9StWxcA0K9fPzRt2hTly5dHixYt0KZNGzRr1ixHsegakxstMTT4+H8cySJ3KysSERUomgyUTd9yk/YcmrbcaEn6pOWXX37B/PnzsWDBAlSuXBmWlpYYMWIEEhISVJ4n/UBkiUSClJQUtY+RL2CX9pj0i9oJIVSeTxX5sZmdU17WsmVLPH78GH/99ReOHj2Kxo0bY8iQIZg3bx6qV6+O0NBQ/O9//8PRo0fRtWtXNGnSBDt27MhxTLrC5EZLDCUffymtTaz1GAkRUR7TtHto1SogICA10TE0BJYvB/r3101sOfD333+jffv26NWrF4DUZOPevXuoUKFCnsZRvnx5XLp0Cb1791aUXclFV1yZMmVgYmKCM2fO4IsvvgCQOjvsypUrSoOjixcvjn79+qFfv3749NNPMXr0aMybNw8AYGNjg27duqFbt27o0qULWrRogbdv32o8e0zXmNxoSdqWG2tTJjdERFnq3x9o3hy4fx8oUwZwddV3RErKlCmDnTt34ty5c7C3t8evv/6K8PDwPE9uhg0bhgEDBsDf3x9169ZFYGAg/vnnH3h5eWV7bPpZVwBQsWJFDB48GKNHj4aDgwNKlSqFuXPnIjY2Fv3/Sy5//PFH+Pn5wcfHB/Hx8di/f7/ifc+fPx9SqRS+vr4wMDDA9u3b4ezsDDs7O62+b21gcqMlaVtuklPYLUVEpJKra75LauQmTZqE0NBQNG/eHBYWFhg4cCA6dOiAiIiIPI2jZ8+eePjwIUaNGoUPHz6ga9eu6NevHy5dupTtsd27d89QFhoaitmzZyMlJQW9e/dGVFQU/P39cejQIdjb2wNI3bhy3LhxePToEczNzfHpp59i69atAAArKyvMmTMH9+7dg6GhIWrUqIEDBw7AwCD/TbyWiNx04BVAkZGRsLW1RUREBGxsbLR23o3/bETv3alNh4tbLsbQmkO1dm4iovzgw4cPCA0NhaenJ8zMzPQdTpHUtGlTODs7Y8OGDfoORSdUfcY0+f5my42WsOWGiIi0KTY2FsuWLUPz5s1haGiILVu24OjRozhy5Ii+Q8v3mNxoCWdLERGRNkkkEhw4cADTp09HfHw8ypcvj507d6JJkyb6Di3fY3KjJWy5ISIibTI3N8fRo0f1HUaBlP9GARVQBpKPtzJFqF7XgIiIiHSHyY2WsFuKiIgof2ByoyXsliIiIsofmNxoCVtuiIiI8gcmN1rClhsiIqL8gcmNlrDlhoiIKH9gcqMlbLkhIiq8GjZsqLS5pIeHBxYsWKDyGIlEgj179uT62to6T1HC5EZL2HJDRJT/tG3bNstF786fPw+JRIKgoCCNz3v58mUMHDgwt+EpmTJlCnx9fTOUy2QytGzZUqvXSm/t2rX5cgPMnGJyoyVsuSEiUp8sSoYpJ6dAFiXT6XX69++P48eP4/HjxxleW716NXx9fVG9enWNz1u8eHFYWFhoI8RsOTs7w9TUNE+uVVgwudESttwQEalPFi3D1FNTIYvWbXLTpk0bODk5Ye3atUrlsbGxCAwMRP/+/fHmzRv06NEDrq6usLCwQOXKlbFlyxaV503fLXXv3j3Ur18fZmZmqFixYqb7P40dOxblypWDhYUFvLy8MGnSJCQmJgJIbTmZOnUqrl27BolEAolEoog5fbfU9evX8dlnn8Hc3ByOjo4YOHAgoqOjFa/369cPHTp0wLx58yCVSuHo6IghQ4YorpUTT548Qfv27WFlZQUbGxt07doVL168ULx+7do1NGrUCNbW1rCxsYGfnx+uXLkCAHj8+DHatm0Le3t7WFpawsfHBwcOHMhxLOrg9gtawhWKiYjyHyMjI/Tp0wdr167Fjz/+CIlEAgDYvn07EhIS0LNnT8TGxsLPzw9jx46FjY0N/vrrL/Tu3RteXl6oVatWttdISUlBp06dUKxYMVy4cAGRkZFK43PkrK2tsXbtWri4uOD69esYMGAArK2tMWbMGHTr1g03btzAwYMHFVsu2NraZjhHbGwsWrRogdq1a+Py5ct4+fIlvv76awwdOlQpgTtx4gSkUilOnDiB+/fvo1u3bvD19cWAAQM0vodCCHTo0AGWlpY4deoUkpKS8M0336Bbt244efIkAKBnz56oVq0ali5dCkNDQ4SEhMDY2BgAMGTIECQkJOD06dOwtLTEzZs3YWVlpXEcmmByoyXsliKiosp/hT/Co8OzrZeckqxo2U5MTm1FaLyuMYwNU78EDSWGSq3gqjhbOePKwCtq1f3qq6/w888/4+TJk2jUqBGA1C6pTp06wd7eHvb29hg1apSi/rBhw3Dw4EFs375dreTm6NGjuHXrFh49egRXV1cAwMyZMzOMk5k4caLi3x4eHvj+++8RGBiIMWPGwNzcHFZWVjAyMoKzs3OW19q0aRPi4uKwfv16WFpaAgB+++03tG3bFnPmzEGJEiUAAPb29vjtt99gaGgIb29vtG7dGseOHctRcnP06FH8888/CA0NhZubGwBgw4YN8PHxweXLl1GjRg08efIEo0ePhre3NwCgbNmyiuOfPHmCzp07o3LlygAALy8vjWPQFJMbLWG3FBEVVeHR4XgW9SxHx76Pf6/dYDLh7e2NunXrYvXq1WjUqBEePHiAv//+G4cPHwYAJCcnY/bs2QgMDMSzZ88QHx+P+Ph4RfKQnVu3bqFUqVKKxAYA6tSpk6Hejh07sGDBAty/fx/R0dFISkqCjY2NRu/l1q1bqFq1qlJs9erVQ0pKCu7cuaNIbnx8fGBo+PF7SSqV4vr16xpdK+013dzcFIkNAFSsWBF2dna4desWatSogZEjR+Lrr7/Ghg0b0KRJE3z++ecoXbo0AGD48OEYPHgwDh8+jCZNmqBz586oUqVKjmJRF8fcaAlbboioqHK2ckZJ65LZPpwtnVHcojiKWxSHnakdAMDO1E5R5myp3nlKWpeEs1XWrRuZ6d+/P3bu3InIyEisWbMG7u7uaNy4MQDgl19+wfz58zFmzBgcP34cISEhaN68ORISEtQ6txAiQ5m8+0vuwoUL6N69O1q2bIn9+/cjODgYEyZMUPsaaa+V/tyZXVPeJZT2tZSUnA2ZyOqaacunTJmCf//9F61bt8bx48dRsWJF7N69GwDw9ddf4+HDh+jduzeuX78Of39/LF68OEexqIstN1rClhsiKqrU7R5KK0gWBL8VfjjW9xiqSzWfraSprl274ttvv8XmzZuxbt06DBgwQPHF/Pfff6N9+/bo1asXgNQxNPfu3UOFChXUOnfFihXx5MkTPH/+HC4uLgBSp5mndfbsWbi7u2PChAmKsvQzuExMTJCcrPr7o2LFili3bh1iYmIUrTdnz56FgYEBypUrp1a8mpK/v6dPnypab27evImIiAile1SuXDmUK1cO3333HXr06IE1a9agY8eOAAA3NzcMGjQIgwYNwrhx47By5UoMGzZMJ/ECbLnRGqWWGyY3RET5ipWVFbp164bx48fj+fPn6Nevn+K1MmXK4MiRIzh37hxu3bqFgIAAhIdnP4ZIrkmTJihfvjz69OmDa9eu4e+//1ZKYuTXePLkCbZu3YoHDx5g0aJFipYNOQ8PD4SGhiIkJASvX79GfHx8hmv17NkTZmZm6Nu3L27cuIETJ05g2LBh6N27t6JLKqeSk5MREhKi9Lh58yaaNGmCKlWqoGfPnggKCsKlS5fQp08fNGjQAP7+/oiLi8PQoUNx8uRJPH78GGfPnsXly5cVic+IESNw6NAhhIaGIigoCMePH1c7ccwpJjdaotRyw24pIiKVpFZSTG4wGVIraZ5ds3///nj37h2aNGmCUqVKKconTZqE6tWro3nz5mjYsCGcnZ3RoUMHtc9rYGCA3bt3Iz4+HjVr1sTXX3+NGTNmKNVp3749vvvuOwwdOhS+vr44d+4cJk2apFSnc+fOaNGiBRo1aoTixYtnOh3dwsIChw4dwtu3b1GjRg106dIFjRs3xm+//abZzchEdHQ0qlWrpvRo1aqVYiq6vb096tevjyZNmsDLywuBgYEAAENDQ7x58wZ9+vRBuXLl0LVrV7Rs2RJTp04FkJo0DRkyBBUqVECLFi1Qvnx5LFmyJNfxqiIRmXUWFmKRkZGwtbVFRESExgO5VHn8/jE8FnoAALr6dEVgl0CtnZuIKD/48OEDQkND4enpCTMzM32HQ4WQqs+YJt/fbLnRErbcEBER5Q9MbrSEi/gRERHlD0xutIQDiomIiPIHJjdawm4pIiKi/IHJjZaw5YaIiooiNg+F8pC2PltMbrSELTdEVNjJV72NjY3VcyRUWMlXbE67dUROcIViLWHLDREVdoaGhrCzs8PLly8BpK65ktVWAESaSklJwatXr2BhYQEjo9ylJ0xutIQtN0RUFMh3rJYnOETaZGBggFKlSuU6aWZyoyVsuSGiokAikUAqlcLJyQmJiYn6DocKGRMTExgY5H7EDJMbLWHLDREVJYaGhrkeF0GkKxxQrCUSfGxC4yJ+RERE+sPkRkskEolilWJ2SxEREekPkxstMvjvdn5I/KDnSIiIiIouJjdaJB8E9SGZyQ0REZG+MLnRInm3VEoKx9wQERHpC2dL5ZIsSgZZtEypLDYpFkGyIACA1EoKqbVUH6EREREVSXptuZk1axZq1KgBa2trODk5oUOHDrhz5062x506dQp+fn4wMzODl5cXli1blgfRZm751eXwW+EHvxV++JCU2h0VHh2uKFt+dbneYiMiIiqK9JrcnDp1CkOGDMGFCxdw5MgRJCUloVmzZoiJicnymNDQULRq1QqffvopgoODMX78eAwfPhw7d+7Mw8g/CvALwNWBV3F14FVYmVgBAIpZFFOUBfgF6CUuIiKiokoi8tH2rq9evYKTkxNOnTqF+vXrZ1pn7Nix2Lt3L27duqUoGzRoEK5du4bz589ne43IyEjY2toiIiICNjY2WosdABznOOLth7dwtnKG7HtZ9gcQERGRWjT5/s5XA4ojIiIAAA4ODlnWOX/+PJo1a6ZU1rx5c1y5ciXTpcDj4+MRGRmp9NAV+YDifJQvEhERFTn5JrkRQmDkyJH45JNPUKlSpSzrhYeHo0SJEkplJUqUQFJSEl6/fp2h/qxZs2Bra6t4uLm5aT12OWNDY52dm4iIiNSTb5KboUOH4p9//sGWLVuyrZt+t1B5S0lmu4iOGzcOERERisfTp0+1E3AmTAxNdHZuIiIiUk++mAo+bNgw7N27F6dPn4arq6vKus7OzggPD1cqe/nyJYyMjODo6JihvqmpKUxNTbUab1bkm2dy+wUiIiL90WvLjRACQ4cOxa5du3D8+HF4enpme0ydOnVw5MgRpbLDhw/D398fxsb67RYylPyX3HBXcCIiIr3Ra3IzZMgQbNy4EZs3b4a1tTXCw8MRHh6OuLg4RZ1x48ahT58+iueDBg3C48ePMXLkSNy6dQurV6/GqlWrMGrUKH28BSVsuSEiItI/vSY3S5cuRUREBBo2bAipVKp4BAYGKurIZDI8efJE8dzT0xMHDhzAyZMn4evri59++gmLFi1C586d9fEWlLDlhoiISP/0OuZGnSnTa9euzVDWoEEDBAUF6SCi3GHLDRERkf7lm9lShQFbboiIiPSPyY0WKXYFF9wVnIiISF+Y3GiRYVJqiw27pYiIiPSHyY22LFgAw6AQxdOUP1bqLxYiIqIijMmNNoSFASNHwjBNb1TyN4NSy4mIiChPMbnRhnv3ACFgmGbyV7JIAe7f119MRERERRSTG20oWxaQSJRbbowMgDJl9BcTERFREcXkRhtcXYGePZVbbhYtTC0nIiKiPMXkRlsaNFBuuenVU3+xEBERFWFMbrTFyCjdmBtOByciItIHJjfaYmQEg7TJDVcpJiIi0gsmN9piZKTULcVViomIiPSDyY22sFuKiIgoX9A4uQkKCsL169cVz//880906NAB48ePR0JCglaDK1DStdywW4qIiEg/NE5uAgICcPfuXQDAw4cP0b17d1hYWGD79u0YM2aM1gMsMNhyQ0RElC9onNzcvXsXvr6+AIDt27ejfv362Lx5M9auXYudO3dqO76Cgy03RERE+YLGyY0QAikpqd/iR48eRatWrQAAbm5ueP36tXajK0jYckNERJQvaJzc+Pv7Y/r06diwYQNOnTqF1q1bAwBCQ0NRokQJrQdYYBgasuWGiIgoH9A4uVmwYAGCgoIwdOhQTJgwAWX+2z9px44dqFu3rtYDLDDYckNERJQvGGl6QJUqVZRmS8n9/PPPMDQ01EpQBRLH3BAREeULGrfcPH36FGFhYYrnly5dwogRI7B+/XoYGxtrNbgCJd0KxS9jXuovFiIioiJM4+Tmiy++wIkTJwAA4eHhaNq0KS5duoTx48dj2rRpWg+wwEjXLfUi5oX+YiEiIirCNE5ubty4gZo1awIAtm3bhkqVKuHcuXOK6eBFVvrtF1K4/QIREZE+aDzmJjExEaampgBSp4K3a9cOAODt7Q2ZTKbd6AoIWZQMsqi7eG3xsezWm1sIkgUBAKRWUkitpXqKjoiIqGjRuOXGx8cHy5Ytw99//40jR46gRYsWAIDnz5/D0dFR6wEWBMuvLoffkS7YWPVj2dyzc+G3wg9+K/yw/Opy/QVHRERUxGjccjNnzhx07NgRP//8M/r27YuqVVO/0ffu3avoripqAvwC0M7KD0umtcMqv9Sy72p9h15VewFIbbkhIiKivKFxctOwYUO8fv0akZGRsLe3V5QPHDgQFhYWKo4svKTWUkhL+KJU5McyD3sPVJdW119QRERERZTGyQ0AGBoaIikpCWfOnIFEIkG5cuXg4eGh5dAKGCMjGKdZ2iYpOUl/sRARERVhGo+5iYmJwVdffQWpVIr69evj008/hYuLC/r374/Y2FhdxFgwGBnBOM0EKWtTa/3FQkREVIRpnNyMHDkSp06dwr59+/D+/Xu8f/8ef/75J06dOoXvv/9eFzEWDEZGMGJyQ0REpHcad0vt3LkTO3bsQMOGDRVlrVq1grm5Obp27YqlS5dqM76CI123VGJyov5iISIiKsI0brmJjY3NdPdvJycndkulablJTGFyQ0REpA8aJzd16tTB5MmT8eHDB0VZXFwcpk6dijp16mg1uAKFLTdERET5gsbdUgsXLkSLFi3g6uqKqlWrQiKRICQkBKampjh8+LAuYiwYDA3ZckNERJQPaJzcVKpUCffu3cPGjRtx+/ZtCCHQvXt39OzZE+bm5rqIsWAwMIBxigRA6u6ZbLkhIiLSjxytc2Nubo4BAwYolT148AADBgzA8ePHtRJYQWQEAwCpfVNJKVznhoiISB80HnOTlejoaJw6dUpbpyt4Vq2CceLHQTeJl87rMRgiIqKiS2vJTZEWFgYMHKg8oHjfntRyIiIiylNMbrTh3j0gJUV5QLFEAPfv6y8mIiKiIorJjTaULZs6oDhty42RBChTRn8xERERFVFqDyiuVq0aJBJJlq8X6QX8XF2BFStgNPVrRVFisyap5URERJSn1E5uOnTooMMwCoH+/WG8aCyANwCApIre+o2HiIioiFI7uZk8ebIu4ygUjI1NFf/mOjdERET6wTE3WmQs+ZgrcoViIiIi/WByo0XGBkxuiIiI9I3JjRYZGxgr/s1uKSIiIv1gcqNFRmy5ISIi0jsmN1qUtlsqOiFaj5EQEREVXWrNllq0aJHaJxw+fHiOgyno0nZLRcdH6TESIiKiokut5Gb+/PlKz1+9eoXY2FjY2dkBAN6/fw8LCws4OTkxuflPUjJ3BSciItIHtZKb0NBQxb83b96MJUuWYNWqVShfvjwA4M6dOxgwYAACAgJ0E2U+J4uSQRYtQ5zNB0XZuw/vECQLAgBIraSQWkv1FR4REVGRIhFCCE0OKF26NHbs2IFq1aoplV+9ehVdunRRSoTyo8jISNja2iIiIgI2NjZaOeeUk1Mw9dTULF+f3GAypjScopVrERERFUWafH+rvUKxnEwmQ2JixplAycnJePHihaanKxQC/ALQrnw7iGFD4V/5PADA09odO3rsApDackNERER5Q+PkpnHjxhgwYABWrVoFPz8/SCQSXLlyBQEBAWjSpIkuYsz3pNb/dTslFINBCpBiAJgZmqC6tLq+QyMiIipyNJ4Kvnr1apQsWRI1a9aEmZkZTE1NUatWLUilUvzxxx+6iLHgMDKCUXLqPxNTOKCYiIhIHzRuuSlevDgOHDiAu3fv4tatWwCAChUqoFy5cloPrsAxMoJxCpAAANBoKBMRERFpicbJjVy5cuVQtmxZAIBEItFaQAWaoSFMk4EYANBsnDYRERFpSY5WKF6/fj0qV64Mc3NzmJubo0qVKtiwYYO2Yyt4/mu5AdgtRUREpC8at9z8+uuvmDRpEoYOHYp69epBCIGzZ89i0KBBeP36Nb777jtdxFkwGBnB6L/kJonJDRERkV5onNwsXrwYS5cuRZ8+fRRl7du3h4+PD6ZMmVLkkxtjxYBibpxJRESkDxp3S8lkMtStWzdDed26dSGTybQSVIHFbikiIiK90zi5KVOmDLZt25ahPDAwUDHAuMhK23KTzJYbIiIifdC4W2rq1Kno1q0bTp8+jXr16kEikeDMmTM4duxYpklPkXL7NozLpP4zMSEOWLUK6N9fvzEREREVMRq33HTu3BkXL15EsWLFsGfPHuzatQvFihXDpUuX0LFjR13EWDCEhQHHjikGFCcaAAgISC0nIiKiPJOjdW78/PywceNGbcdSsN27Bwih6JZKMQBSUpJhcP8+4Oqq39iIiIiKkBwlN8nJydizZw9u3boFiUSCihUrol27djA0NNR2fAVH2bKARALjlI+L9yUZGcCkTBk9BkVERFT0aJzc3L9/H61bt0ZYWBjKly8PIQTu3r0LNzc3/PXXXyhdurQu4sz/XF2BTp1gnLxTUZT4+2KYsNWGiIgoT2k85mb48OHw8vLC06dPERQUhODgYDx58gSenp4YPny4LmIsOOrVU0wFB4DE3l/oLxYiIqIiSuOWm1OnTuHChQtwcHBQlDk6OmL27NmoV6+eVoMrcExNFWNuACAsIgx2ZnZ6C4eIiKgo0rjlxtTUFFFRURnKo6OjYWJiotG5Tp8+jbZt28LFxQUSiQR79uxRWf/kyZOQSCQZHrdv39boujpjaqqYLQUAz6Ke6S8WIiKiIkrj5KZNmzYYOHAgLl68CCEEhBC4cOECBg0ahHbt2ml0rpiYGFStWhW//fabRsfduXMHMplM8cg3iweamCh1SyUJrlJMRESU1zTullq0aBH69u2LOnXqwNjYGACQlJSEdu3aYeHChRqdq2XLlmjZsqWmIcDJyQl2dnYaH6dLsigZZCnPEJWm8epa+DVIraQAAKmVFFJrqZ6iIyIiKjo0Tm7s7Ozw559/4t69e7h9+zaEEKhYsSLK5OGU52rVquHDhw+oWLEiJk6ciEaNGmVZNz4+HvHx8YrnkZGROolp+dXlmPpoKpCmEWnC8QmYcHwCAGByg8mY0nCKTq5NREREH+VonRsAKFu2bJ53B0mlUqxYsQJ+fn6Ij4/Hhg0b0LhxY5w8eRL169fP9JhZs2Zh6tSpOo8twC8A7V47Yta24djhk1o2sf5EdPROXbVZ3oJDREREuqVxcpOcnIy1a9fi2LFjePnyJVJSUpReP378uNaCS698+fIoX7684nmdOnXw9OlTzJs3L8vkZty4cRg5cqTieWRkJNzc3LQem9RaCqmDD9zSNAyVtiuN6tLqWr8WERERZU3j5Obbb7/F2rVr0bp1a1SqVAkSiUQXcamtdu3aKreCMDU1hampad4EY2oK0zRjiOOT47OuS0RERDqhcXKzdetWbNu2Da1atdJFPBoLDg6GVJpPunxMTGCWJrmxMrHSXyxERERFlMbJjYmJidYGD0dHR+P+/fuK56GhoQgJCYGDgwNKlSqFcePG4dmzZ1i/fj0AYMGCBfDw8ICPjw8SEhKwceNG7Ny5Ezt37szqEnnL1BSmaRbxszSx1F8sRERERZTGyc3333+PhQsX4rfffst1l9SVK1eUZjrJx8b07dsXa9euhUwmw5MnTxSvJyQkYNSoUXj27BnMzc3h4+ODv/76K9+0IsHUVKnl5kPSB/3FQkREVERJhBAiu0qdOnVSen78+HE4ODjAx8dHsdaN3K5du7QboZZFRkbC1tYWERERsLGx0e7JQ0OxrKsXBrdJfbqm/Rr08+2n3WsQEREVQZp8f6vVcmNra6v0vGPHjjmPrjBLP6A4iQOKiYiI8ppayc2aNWt0HUfhsH27crfUmZOAf4DewiEiIiqKNN5birIQFgaMHKmc3OwKTC0nIiKiPKNWy0316tVx7Ngx2Nvbo1q1aioHEgcFBWktuALl3j0gJUVptlS8gQDu3wdcXfUXFxERURGjVnLTvn17xUJ4HTp00GU8BVfZsoCBAcySPq7Y/NoCQB7uuUVERERqzpYqTHQ6W2rVKpyd+jU+6Z/6tKd5LWwcc0G71yAiIiqCNPn+5pgbberfH2bGZoqnCR6l9BgMERFR0aRWt5S9vb3aC/a9ffs2VwEVVLIoGWTRMjws/vGWPo9+jiBZ6hgkqZUUUut8sk0EERFRIaZWcrNgwQIdh1HwLb+6HFNPTQVafiw7+/Qs/Fb4AQAmN5iMKQ2n6Cc4IiKiIoRjbrRE3nLz4vNWaNX0BQCgunN1rGy3EgBbboiIiHJD6ysUp/fgwQOsWbMGDx48wMKFC+Hk5ISDBw/Czc0NPj4+OQq6oJNapyYvr6KtAaQmNxbGFqgura7fwIiIiIoYjQcUnzp1CpUrV8bFixexa9cuREdHAwD++ecfTJ48WesBFjRmRmkGFKck6DESIiKioknj5OaHH37A9OnTceTIEZiYmCjKGzVqhPPnz2s1uILIzND045Mi1eFHRESUP2ic3Fy/fj3TjTOLFy+ON2/eaCWogszIxAyS/5IaIVJUVyYiIiKt0zi5sbOzg0wmy1AeHByMkiVLaiWogkxiaqbYX+pDYpx+gyEiIiqCNE5uvvjiC4wdOxbh4eGQSCRISUnB2bNnMWrUKPTp00cXMRYsJiYfk5sP0fqNhYiIqAjSOLmZMWMGSpUqhZIlSyI6OhoVK1ZE/fr1UbduXUycOFEXMRYsMhlM/0tu4sMeA6tW6TceIiKiIibH69w8fPgQQUFBSElJQbVq1VC2bFnExcXB3Nxc2zFqlU73lgoLA9zc4Pkt8MgecIoGXsw3BB494s7gREREuaDTvaWGDBkCAPDy8kKXLl3QtWtXlC1bFjExMWjZsmU2Rxdy9+4BAEyTU5++NwNk5snA/ft6DIqIiKho0Ti5OXz4cIbup5iYGLRo0QLJyclaC6xAKlsWkEgUY24SjACZrQFQpox+4yIiIipCcpTcrFmzBvPnzwcAREVFoWnTppBIJDh48KDWAyxQXF2Bpk0VyQ0AiPHj2SVFRESUhzTefsHT0xOHDh1Cw4YNYWBggK1bt8LU1BR//fUXLC0tdRFjgSGLkkFW2x0J4R/LLtVwgYQ7gxMREeWZHA8ovnDhApo0aYJatWph//79+X4gsZwuBxRPOTkldWfwLHBncCIiopzR+saZ1apVg0QiyVBuamqK58+fo169eoqyoKAgDcMtPAL8AtAuJA6jg+fiuFdq2Zwmc9DEqwmA1JYbIiIi0i21kpsOHTroOIzCQWothdSmPNwiP5aVdijNncGJiIjykFrJDXf71oCFBSzTbAYexy0YiIiI8pTGs6UoG5aWsEz8+NTCyEJ/sRARERVBarXcODg44O7duyhWrBjs7e0zHX8j9/btW60FVyBZWMAqTcuNqZGp/mIhIiIqgtRKbubPnw9ra2sAwIIFC3QZT8F37JhSt1TMoX1Audb6i4eIiKiIUSu56du3b6b/TismJgZXr17VTlQFVVgYMGcOrKp9LHq2ZTnQcSIX8iMiIsojWhtzc//+fTRq1EhbpyuY7t0DUlKUxtzILMG9pYiIiPIQBxRrU9mygIGB0pibOBNwbykiIqI8xORGi2S2hgia9z2eW30se/hpZQQZvkSQLAiyKJn+giMiIioimNxo0fKry+EX+TOGtPlYdiD+OvxW+MFvhR+WX12uv+CIiIiKCLU3zty7d6/K10NDQ3MdTEEX4BeAduXb4V6LmujeKRkA8InbJ1jYciEAbr9ARESUF9RObtTZgkHV+jdFgdQ6dddvu/cWAKIAAJYmltx+gYiIKA+pndykpKToMo5CxdL4Y3ITmxir32CIiIiKGI650QErI0vFv1M+fNBjJEREREVPrpIbGxsbPHz4UFuxFBrmMfGKfz+6fxmyFb/oMRoiIqKiJVfJjRBCW3EUHmFhMAh7ptiC4ZkNIJs2JnX1YiIiItI5dktp2717AKC0vxRSUrhKMRERUR5Re0BxZnr16gUbGxttxVIoyEraQCYFjNKMvw5ykQB2cYAsCFKr1BlVREREpBu5arlZunQpihUrpq1YCoXlz/fBLwB4nibnG9BWwO/PVlzIj4iIKA9o3HKzaNGiTMslEgnMzMxQpkwZ1K9fH4aGhrkOriAK8AtAu7/uY2DEJlwtmVr2e6vfUdu1NgAu5EdERKRrGic38+fPx6tXrxAbGwt7e3sIIfD+/XtYWFjAysoKL1++hJeXF06cOAE3NzddxJyvSa2lkNqUR6mnUCQ3XnZeXMiPiIgoj2jcLTVz5kzUqFED9+7dw5s3b/D27VvcvXsXtWrVwsKFC/HkyRM4Ozvju+++00W8BYOtLezjPj6NTIjUXyxERERFjMYtNxMnTsTOnTtRunRpRVmZMmUwb948dO7cGQ8fPsTcuXPRuXNnrQZaoNjawj7N2n1R8VH6i4WIiKiI0bjlRiaTISkpKUN5UlISwsPDAQAuLi6IiirCX+iXLyu33Jw+rL9YiIiIihiNk5tGjRohICAAwcHBirLg4GAMHjwYn332GQDg+vXr8PT01F6UBUlYGLB0qVLLTeSf27iIHxERUR7ROLlZtWoVHBwc4OfnB1NTU5iamsLf3x8ODg5YtWoVAMDKygq//FI0txyQ3biAoBIpeG/6sexWMSAo+ACCZEGQRcn0FxwREVERIBE53EPh9u3buHv3LoQQ8Pb2Rvny5bUdm05ERkbC1tYWEREROlmAcMrekZgaPD/L1yc3mIwpDado/bpERESFmSbf3zleoThtQiORSHJ6mkInoNFotLtviOvr5qFfp9Syz0y98XPfTQC4zg0REZGu5WiF4vXr16Ny5cowNzeHubk5qlSpgg0bNmg7tgJJai1F9eGzUOfZxzLh4IDq0uqoLq3OrReIiIh0TOPk5tdff8XgwYPRqlUrbNu2DYGBgWjRogUGDRqE+fOz7o4pUoyM4JBkrHj6Nugc8N94JCIiItItjcfceHp6YurUqejTp49S+bp16zBlyhSEhoZqNUBt0/WYGwBAWBiS3N1g/GPq0wovgZvLDYFHjwBXV91ck4iIqBDT5Ps7R+vc1K1bN0N53bp1IZNxJpAsSoagoL/wTwnANDG17K05EOSUjKDgA5wtRUREpGMaJzdlypTBtm3bMpQHBgaibNmyWgmqIFt+dTn8ggfBLwCI/69n6oU14BcA+AUFcFdwIiIiHdN4ttTUqVPRrVs3nD59GvXq1YNEIsGZM2dw7NixTJOeoibALwDtyrcDevVC94q3cK9YavkehyFw6/IVZ0sRERHpmMbJTefOnXHx4kXMnz8fe/bsgRACFStWxKVLl1CtWjVdxFigSK2lqTOiKjaGx/uPyQ2aNuXO4ERERHkgR+vc+Pn5YePGjUplL168wLRp0/Djjz9qJbAC7+lTFDf8+PT1od2Ad3v9xUNERFRE5Gidm8yEh4dj6tSp2jpdgSa7exVBl/6EecLHskvH1iEo5H/cgoGIiEjHtJbc0EfLzy6CXwCwyv9j2Qo/wO/PVvBb4cdBxURERDqU4+0XKGsB9Yaj3cQNuCQVGNw2tazOU+C3oQeAEiU4qJiIiEiHmNzogLScH6Tf/ATzBRMVZQaly6C6b0s9RkVERFQ0qJ3cjBw5UuXrr169ynUwhYUsSgaZRQSiLD6W3Uh8hkMPDqG4RXFIraTcY4qIiEhH1E5ugoODs61Tv379XAVTWCw/8TOmRs4HvvpYFiHi0GJjCwDA5AaTMaXhFP0ER0REVMipndycOHFCl3EUKgEmddFueeomop9/Djx0SC1fVW40fBt255gbIiIiHeJsKR2QVqoNaYwEAFAy8mP5deN3AABZtIzTwYmIiHREr8nN6dOn0bZtW7i4uEAikWDPnj3ZHnPq1Cn4+fnBzMwMXl5eWLZsme4D1ZSrK5ZPbgO/AOBvj4/FC/79A34r/DgdnIiISIf0mtzExMSgatWq+O2339SqHxoailatWuHTTz9FcHAwxo8fj+HDh2Pnzp06jlRzAW0m4+pyYOLJj2Wl3gMHm2/E1YFXEeAXoK/QiIiICjW9TgVv2bIlWrZUf3r0smXLUKpUKSxYsAAAUKFCBVy5cgXz5s1D586ddRRlDj0MBQBUfvmx6Ikd8PrxLRR3r6CfmIiIiIqAAjXm5vz582jWrJlSWfPmzXHlyhUkJibqKarMLU84B78AoFtX5fJet2ewW4qIiEiHcpTc/P333+jVqxfq1KmDZ8+eAQA2bNiAM2fOaDW49MLDw1GiRAmlshIlSiApKQmvX7/O9Jj4+HhERkYqPfJCgHdPHNwAbNwJuER8LK9kVx6LWy5GHdc6HFRMRESkAxonNzt37kTz5s1hbm6O4OBgxMfHAwCioqIwc+ZMrQeYnkQiUXouhMi0XG7WrFmwtbVVPNzc3HQeIwBIn0XivBvQqzPw3PZj+Y33dzDsf8PQYlMLtt4QERHpgMbJzfTp07Fs2TKsXLkSxsbGivK6desiKChIq8Gl5+zsjPDwcKWyly9fwsjICI6OjpkeM27cOERERCgeT58+1WmMCmXLosOd1JabL64pvzSm7hhs7LQRHcp3yJtYiIiIihCNBxTfuXMn05WIbWxs8P79e23ElKU6depg3759SmWHDx+Gv7+/UqKVlqmpKUxNTXUaV6ZcXbFnSBNMTTqa4aW55+YCAAZWH4gpDadwKwYiIiIt0rjlRiqV4v79+xnKz5w5Ay8vL43OFR0djZCQEISEhABIneodEhKCJ0+eAEhtdenTp4+i/qBBg/D48WOMHDkSt27dwurVq7Fq1SqMGjVK07eRJwL8AjDwStavrwhawa4pIiIiLdO45SYgIADffvstVq9eDYlEgufPn+P8+fMYNWoUfvzxR43OdeXKFTRq1EjxXL45Z9++fbF27VrIZDJFogMAnp6eOHDgAL777jv8/vvvcHFxwaJFi/LfNPD/SB3dMfgyUP8xsMoXOFH642t9qvRBszLN4FPMR2/xERERFUYSIR+Rq4EJEyZg/vz5+PDhA4DUrp9Ro0bhp59+0nqA2hYZGQlbW1tERETAxsZGtxc7cQJTpn2GqQ2zrsKuKSIiouxp8v2do+QGAGJjY3Hz5k2kpKSgYsWKsLKyylGweS1Pk5uwMMgqumFME2Bj1ayrjaw9Er80/0W3sRARERVgmnx/azzmZt26dYiJiYGFhQX8/f1Rs2bNApPY5DlXV0gDRsEiQXW16ITovImHiIioCNA4uRk1ahScnJzQvXt37N+/H0lJSbqIq/Do2hVW2SyefOPlDYTIQvIkHCIiosJO4+RGJpMhMDAQhoaG6N69O6RSKb755hucO3dOF/EVfNHRGHUOqP486yrnws6hx44eXLGYiIhICzROboyMjNCmTRts2rQJL1++xIIFC/D48WM0atQIpUuXzv4ERU3ZspDGSLDqT6Dl3ayr3X57G/POzcu7uIiIiAqpXG2caWFhgebNm6Nly5YoW7YsHj16pKWwChFXV2DcOPi+ACpkvv2VwoWwC+yeIiIiyqUcJTexsbHYtGkTWrVqBRcXF8yfPx8dOnTAjRs3tB1f4dCkCQBg1DmgcnjW1c6FnUPHwI5McIiIiHJB40X8evTogX379sHCwgKff/45Tp48ibp16+oitsKjbFkAgDQaWL8baNsdCLPPvOqjiEfos7sP/vnmnzwMkIiIqPDQuOVGIpEgMDAQz58/x++//87ERh2ursAXXwAAfF8ArTPuXqHk/rv72Hp9ax4ERkREVPhonNxs3rwZrVu3hpGRxo0+RVv79op/Tj6lunsqLikOA/YNwJEHR/IgMCIiosJFrRWKFy1ahIEDB8LMzAyLFi1SWXf48OFaC04X8nSF4rTCwgA3N8XTkBJAx27AI4esD5FaSnGg5wH4Sn11Hx8REVE+pvXtFzw9PXHlyhU4OjrC09Mz65NJJHj48KHmEechvSU3ABAQAKxYoXgaUgJo1ROQqQjDw9YDu7vtZoJDRERFWp7sLVVQ6TW5Sdd6AwBHPIFO3YFo06wPczRzxNE+R5ngEBFRkaXTvaWmTZuG2NjYDOVxcXGYNm2apqcrWlxdgfHjlYqahgIr/wTM47M+7M2HN/h8++dcwZiIiEgNGrfcGBoaQiaTwcnJSan8zZs3cHJyQnJyslYD1Da9ttzI1a4NXLyoVFRlEHDdWfVhMxvNxLj643QYGBERUf6k05YbIQQkEkmG8mvXrsHBQcXoWPpox44MRet3Ax5vVR826eQkzDo9iy04REREKqid3Njb28PBwQESiQTlypWDg4OD4mFra4umTZuia9euuoy18Eiz7o2c7wtgd6DqBCdZJGP8ifHov7c/ExwiIqIsqN0ttW7dOggh8NVXX2HBggWwtbVVvGZiYgIPDw/UqVNHZ4FqS77olgIyHVwMpM6g+qITcKuE6sO9Hb1xvO9xSK2lOgqQiIgo/9DpbKlTp06hbt26MDY2zlWQ+pJvkhsgw9RwuSOeQJteQIKh6sNblWmFP9r9wQSHiIgKPZ2OuWnQoIEisYmLi0NkZKTSgzQwaVKmxU1DgXU7AeOUjGOb0jpw/wDWBq/VQWBEREQFl8bJTWxsLIYOHQonJydYWVnB3t5e6UEacHUF5s7N9KXuN4H1OwRMoLr5ZuKJiWi8rjF3EiciIvqPxsnN6NGjcfz4cSxZsgSmpqb4448/MHXqVLi4uGD9+vW6iLFwGz0aGDYs05e63wT2r0uGhaF5loenIAXHHx1Hx8COTHCIiIiQgzE3pUqVwvr169GwYUPY2NggKCgIZcqUwYYNG7BlyxYcOHBAV7FqRb4ac5NWJmvfyG3t64/+ZW8iNinj4olp2ZvZ43if41zJmIiICh2djrl5+/atYn8pGxsbvH2bOnf5k08+wenTp3MQLgHIdO0bue7rrmBPXHuYGJioPMW7D+/QanMr7iZORERFmsbJjZeXFx49egQAqFixIrZt2wYA2LdvH+zs7LQZW9GSydYMaTWdsQXrjD6HsYHqWWqyaBk6BXZigkNEREWWxsnNl19+iWvXrgEAxo0bpxh7891332H06NFaD7BImTEDqFUry5e7j9+E9fXnw8rISuVpohOj0WFrB2y9vlXbERIREeV7ud4V/MmTJ7hy5QpKly6NqlWraisuncm3Y27ksljcT6F2bYTsWopWm1tBFq16lWIjiRHal2+Pxa0Wcy0cIiIq0HQ65ia9UqVKoVOnTgUisSkQVEwPBwBcuADfWauxrsM6SC1VJyxJIgk7b+/kWjhERFSkGGl6wKJFizItl0gkMDMzQ5kyZVC/fn0YGmazvC5lbfRoICIitZsqM4sXo6mNDQ4MOYDP1n2Gd/HvVJ5u0olJuCq7yhYcIiIqEjTulvL09MSrV68QGxsLe3t7CCHw/v17WFhYwMrKCi9fvoSXlxdOnDgBN1XdK3qS77ul0urUCdi9O+vXJ0xAyJAuGHJgCM6Fncv2dG7WbljVfhWalm6qxSCJiIh0T6fdUjNnzkSNGjVw7949vHnzBm/fvsXdu3dRq1YtLFy4EE+ePIGzszO+++67HL8B+k8WrWQKM2bA9/cd2NF1Bzp5d4KhRHVr2dOop+i8rTMG7RvEXcWJiKjQ0rjlpnTp0ti5cyd8fX2VyoODg9G5c2c8fPgQ586dQ+fOnSGT5b8v0ALVcgMAP/8MjBmTfZ1RozDr9CyMP5H1dPK0fmv5G4bUHKKFAImIiHRPpy03MpkMSUlJGcqTkpIQHh4OAHBxcUFUVJSmp6bMjB4NTJiQfZ2wMPSr1g+dvDvBSJL9UKrvD32PLoFd2IJDRESFjsbJTaNGjRAQEIDg4GBFWXBwMAYPHozPPvsMAHD9+nXFKsakBdOnZ5/gtGgBqbUUO7vtxIaOG2BhZKGyenxKPHbe3olaK2txwT8iIipUNE5uVq1aBQcHB/j5+cHU1BSmpqbw9/eHg4MDVq1aBQCwsrLCL7/8ovVgi7TsEpx//wXq1QMAdK/cHXu674GTuVO2p30a9RRtt7RlgkNERIVGjhfxu337Nu7evQshBLy9vVG+fHltx6YTBW7MTXrDhwOLF2f9et26wNmzAIAQWQg6BnbEo4hH2Z7WztQObrZuWN9hPTfeJCKifEeT7+8cJzcJCQkIDQ1F6dKlYWSk8XI5elPgkxsAaNwYOH4869fTJTifb/sc99/fV+vUrtauaF22NSY3nMw1cYiIKN/Q6YDi2NhY9O/fHxYWFvDx8cGTJ08AAMOHD8fs2bNzFjFp5tix1AQmK+fOpSZAAHylvjj91Wl08u4Ec0PzbE8dFhWG5UHL0Xxjc4TIQrQUMBERUd7ROLkZN24crl27hpMnT8LMzExR3qRJEwQGBmo1OFLh7FnVCc7x46ldWIBioPGfPf5UaxwOAFx/eR3ttrRjgkNERAWOxsnNnj178Ntvv+GTTz6BRCJRlFesWBEPHjzQanCUjewSnMWLgYkTFU+blm6KQ70PwdXaVa3TP416iiYbmqDK0ipMcoiIqMDQOLl59eoVnJwy/t9/TEyMUrJDeeTsWcDHJ+vXZ8xQtOAAqd1U+3rsQxm7Mmqd/k3cG1x/eR1tt7TlysZERFQgaJzc1KhRA3/99ZfiuTyhWblyJerUqaO9yEh9Bw+qfn3xYqBNG8VT+TicgdUHqt1NJR+L89m6z9iKQ0RE+ZrGs6XOnTuHFi1aoGfPnli7di0CAgLw77//4vz58zh16hT8/Px0FatWFIrZUplRZ5uGYcMy7Fd15MERdNvRDe8+qN5ZPC2ppRQu1i74o90fnDZORER5QqezperWrYuzZ88iNjYWpUuXxuHDh1GiRAmcP38+3yc2hZo62zQsXqzURQWkjsM53uc4KhevDEdzR7UuJYuR4Wr4VTRc1xC9d/VmVxUREeUrOV7npqAqtC03chMnpo6zUeWzz1Knk6ej6Zo4ct6O3mjg3oBr4xARkc7otOWG8jl19qE6fhzw989QnHZNHE3cfnNbMR6Hg46JiEjf1G65MTAwyHY2lEQiyXTH8Pyk0LfcyKnTglO6NHA/YyuNLEqGKSen4MDdAwiLDtP40t6O3tjSeQvH4xARkdboZPuFP//8M8vXzp07h8WLF0MIgbi4OM2izWNFJrkB1EtwypVL7aJyzbj2TYgsBP339ocsSgZZjGatMa7WrihhWYKDjomISCvyZG8pIHXzzHHjxmHfvn3o2bMnfvrpJ5QqVSqnp8sTRSq5AYB581IHG2dn7tws6+V0LA4AWJtYw8POgxtyEhFRruh8zM3z588xYMAAVKlSBUlJSQgJCcG6devyfWJTJI0aBTx9mtpCo8qYMUqrGaflK/XF9q7bUd25utqrG8tFJUTh+svr6BjYEVWXVOWYHCIi0jmNWm4iIiIwc+ZMLF68GL6+vpgzZw4+/fRTXcandUWu5SatMmWA7LbIyGQtnLTkXVVhkWF4GfsyR2E4WThhY6eNaFq6aY6OJyKiokcn3VJz587FnDlz4OzsjJkzZ6J9+/ZaCTavFenkBkidJXX1quo61aoBe/dmOg5H7siDI+i5qydexb7KURhSSynsTO1gYWLBcTlERJQtnSQ3BgYGMDc3R5MmTWBoaJhlvV27dmkWbR4r8skNADRunDodPDvjx6sckCyfVXX68WncfnM7x+G4WrsiOSUZ6zquY2sOERFlSifJTb9+/dTaGHPNmjXqRaknTG7+89VXgDo/qywW/EtLnuQcfnAYjyIe5TgkqaUUxS2Ko45bHS4ISERESvJstlRBxOQmjTZtgDSboGbJxyd1c04V3VTAx/E4b+Pe5irJAQB7M3u42rhylhUREQFgcqMSk5t01FkLRy6bbio5xfo40TLIonM3M8rD1gPmhuYcm0NEVMQxuVGByU0mwsKAdu2A4ODs69atC5w9q9Zpjzw4gj67+8DIwAhhUZqvdJyeraktpFZS7mNFRFQEMblRgcmNCsOHp+4cnh1HR+B//wNq1FDrtPKWnMgPkTlaCDAzHrYeiE+K5yBkIqIigsmNCkxusqHuisYAUKsWsGNHtmNx5GRRMow5Mgb77+3H+w/vcx5jGvIp5caGxhyITERUiDG5UYHJjRo06aYC1B6LIydvyYlLiENcclyuBx+nZW9mjxKWJdh1RURUyDC5UYHJjQY0GWxcpgywebPaXVVyIbIQ9NndB08jnuJ9wnvNY1QVkn0ZGEuMORiZiKgQYHKjApMbDYWFpXY/PX+uXn1vb2D9+hwlOdoel5OWq7UrrI2t2X1FRFRAMblRgclNDqm7qrGchuNx5OTjcg7dPwRTI1OtzLLKjIetB2xMbJjoEBEVEExuVGBykwuXLwNduwKPHql/zBdfAHPmaJzkAMpjc94nvM/1mjlZ8bD1QFR8FFxsXLhoIBFRPsXkRgUmN1qg7srGaWk46Dg9+Zo5ZkZmWh2AnJ6HrQeMJEZ49+Edkx0ionyEyY0KTG605PJloF8/4OZN9Y/x8ADGjAHats1RSw6QZrPOR6cRHh2u9UHI6clXSAbAgclERHrE5EYFJjdadvky0LMncO+eZscNHaregoEqpO22SkxJ1MlA5PTkA5MBJjtERHmJyY0KTG50RJNp43IODsD06blqyZFLOxDZ0sRSp11XaTHZISLKG0xuVGByo0NhYcC4ccDGjZof27gxMGuWxlPIMyPvujr/9Dyi4qPwKPJRrs+pLltTWxS3KM71dYiItKxAJTdLlizBzz//DJlMBh8fHyxYsACffvpppnVPnjyJRo0aZSi/desWvL291boek5s8EBYGfP45cOGC5seWLg18/71WWnMA5UQnKTkJUQlRCIvWzfTyzLhauyIuMQ725vawMLLg1HMiohwqMMlNYGAgevfujSVLlqBevXpYvnw5/vjjD9y8eROlSpXKUF+e3Ny5c0fpjRUvXhyGhoZqXZPJTR7KyaDjtDp0SB2Xo4UkR04f43TSszezh6O5I1t3iIg0UGCSm1q1aqF69epYunSpoqxChQro0KEDZs2alaG+PLl59+4d7OzscnRNJjd6cPkyMGECcORIzo6vVy910LKWWnPk0s68ehX7CubG5jpbNFAVtu4QEWWvQCQ3CQkJsLCwwPbt29GxY0dF+bfffouQkBCcOnUqwzHy5MbDwwMfPnxAxYoVMXHixEy7quTi4+MRHx+veB4ZGQk3NzcmN/oQFga0bw8EBeX8HPXqAfPna2VsTnppt4B49+Gd3pIdgK07RETpaZLcGOVRTBm8fv0aycnJKFGihFJ5iRIlEB4enukxUqkUK1asgJ+fH+Lj47FhwwY0btwYJ0+eRP369TM9ZtasWZg6darW46cccHUFrl7NXUvO2bNAzZpAyZKpWzx8+WXqooJa4Cv1xdWAq4rnSruXJ8Xl6cDkdx/e4d2Hd4rnbbe0VbTuMOEhIlJNby03z58/R8mSJXHu3DnUqVNHUT5jxgxs2LABt2/fVus8bdu2hUQiwd69ezN9nS03+VhuZlel5eCQOj7H31/rXVdy6Qcm53Wyk5m03VlMeIiosCu03VKZmTFjBjZu3Ihbt26pVZ9jbvKhsDBg/35g82bg779zfz5/f+Crr3SW6AAZkx0AOt3/Sl1pp6IDXHuHiAqPApHcAKkDiv38/LBkyRJFWcWKFdG+fftMBxRnpkuXLnj79i2Oq7ljNZObfC4sDPj2W2DXLu2cz9//44BkHYzTSUu+/5WViRWMJcaIS47Ls8UEVWHCQ0SFQYFJbuRTwZctW4Y6depgxYoVWLlyJf7991+4u7tj3LhxePbsGdavXw8AWLBgATw8PODj44OEhARs3LgRs2fPxs6dO9GpUye1rsnkpoDQdmsOALi7py4WqMPuq7RCZCHos7sPnkc9h725PSCgl6nnmUm7sjLAhIeI8r8Ck9wAqYv4zZ07FzKZDJUqVcL8+fMVg4P79euHR48e4eTJkwCAuXPnYsWKFXj27BnMzc3h4+ODcePGoVWrVmpfj8lNARQWBmzYAPz+O/DsmfbOm4etOoDyFhG2Zrb5qnUHYMJDRPlbgUpu8hqTmwLu8mVgyxZgxw7g6VPtndfJCahdG/D0zLNkB8jfrTvAx4QnMSUR7z68g4uNC9Z3WM+Eh4jyHJMbFZjcFCLyROfsWeDSJe2eW0/JTn5v3QEAD1sPmBuaKxIee3N72JraspWHiHSKyY0KTG4KKXnX1ZYtwPXr2j9/yZJA2bKpe18FBORZsgNkbN3JjwkPkHFqurGhMVdbJiKtYXKjApObIkAXg5HTkyc7zs5A/fp5MkA5rcwSnvwwFT0zaVdbBsCkh4hyhMmNCkxuihh5onP1KnDsGBAaqrtrVamSuqCgHlp3gIxT0YH8sfZOVtJ3b3E8DxGpwuRGBSY3RZx8nE54OHDhgm6THXf31IQnKirfJDz5YWVlVTieh4iywuRGBSY3pOTyZWDFCuDBA+D+fe3OwMqMvDvLxibPBysDma+snN8THiDjeB6A3VtERQ2TGxWY3JBKeZ3sAEx4ckne2gMAiSmJiIyPRLMyzTC36VwmPUSFCJMbFZjckEbk3VgPH6Z2Y714kTfXlU9FNzcHLC3zbFVlufQJj7ybyNzYHGFRYXkSQ26VsS8DCCi6t7i5KFHBxuRGBSY3lCtpk53bt4E7d/L2+lWqABUqpP5bD0lPiCwE/ff2R1xCnNK4mKSUpHw3NT0r6bu4ElMSEZMYg3Ud16Fp6ab6Do+IssDkRgUmN6RVaWdjvXqVt607afn7A1IpkJgIfPop0KeP3qemJ6Yk5qvVlrMjtZTCztQOADigmSgfYnKjApMb0rm0M7JiY/Wf8ERF6WU8T2arLQNAVEIUwqILRteWXGYDmgHuv0WUl5jcqMDkhvQibXdWZGTeDVbOTNrxPEDqQoR5vJ+WvGsLQIEcz5NW2g1H5e+luGVxNHBvwJlcRFrE5EYFJjeUb+SnhAdInbX1ySep/46NBYyNgS+/BNq0yZPLF4bxPOl52HogKj6KLT5EWsDkRgUmN5SvpZ2KbmMDyGTa3xRUUw4OQL16H7u3zM3ztLUns/E8QMHs3kor/Y7rnNFFpBqTGxWY3FCBI98U9MyZ1NYUCwvdr66srrRdXLGxeTqgOX33FpDaLfQm7g3exb/T6bV1LbMxPpzVRUUdkxsVmNxQoZF24DIA3LoFhIToNSQl6Qc051GLT9o1euIS45RaRfLjbuqaks/qSt/iA7C7iwo3JjcqMLmhQi3t1PSYmNTWlPwwnie99C0+ebT/VlZT1l/Hvsb7hPc6u25esjW1RXGL4or3Jk+ALIwsuF0FFWhMblRgckNFVvrxPBYWwJEjwOvX+o5MWdoNR+UtPjpesDCrGVwFfUBzZjIb5MxtK6ggYHKjApMbonT27wfWrUsdL2NhkVp25kz+aulJy98/tZVH3uJjYwMUK6az5CerAc0Faf8tTWS2bYUcu71In5jcqMDkhkhN6RcjjIxMTST++Sd/DGbOSrlygLe3zru8MttwVN7iY21qXahae9LKapYXwJ3aSbeY3KjA5IZICzLr4spvA5ozk1mXlw7W9JEnPqcfncar2FdFosUnrfQ7tadNgpgAUU4xuVGByQ2RDmU1oLkgtPgAqWv6NG2q3OWl5ZWcs9pxvTDN6FIHEyDSFJMbFZjcEOlRZi0++tx/Kyfc3VNneqVPgLQ06DmrMT6FbVaXOpgAUVpMblRgckOUTxXUMT6ZqVIFqFBB6wlQVltUyBOg9wnvIYuWafvd5Gv2ZvZwNHfMMPWd44AKHyY3KjC5ISqA0i9YCHxMgN69y/9jfTKTVQIE5LgL7MiDI+izuw+sTKwyfNlDAPff39fBGykY5K1AmSVAXP25YGByowKTG6JCSD7W5/Tp1ASooHZ5pZd+M9NctAKpGuRcWLatyC1Vqz9zx3f9Y3KjApMboiIoqy4vC4v8vaaPujJb+0fDmWCqtq0AisYsL01kth4Qt8TQLSY3KjC5IaIM0nd7pU2A8sPO7NqgaiaYGpueZjfLCyj4O7XrSlZbYqRvFXKxccH6DuuZCGWByY0KTG6ISGPpp7gDhTMBkqtSJTUZSp8AZbMidFY7taf9MmcCpFpWY4PYKsTkRiUmN0SkE2FhwIYNqd1cxsYfx/wU1gRITj4wGsi6VShN1xgTIO1St1XI3twetqa2BToZYnKjApMbItKb7BKg/LqZqbY4OAD16mWeAKUpC3ExRH/Xq4gzNlAcmllLRlGe/ZVTHrYeMJIYZdsqlB+7ypjcqMDkhojyvcw2My3oa//khIrZYjKjOIyxv4pDVi9ga2AOYztHJJoYcRyQDmSWEAEZE055ma52mGdyowKTGyIqFFSt/VOYZoJpomRJoGxZpVahkBQZ+rtdRZwRAIkEiSIZ7wwTYC/MYGxuBdhYI1GCIrf6c164OvAqqkura+18mnx/G2ntqkRElHdq1FBvkT9VM8HkrUIFYdNTdTx7lvpIwxfA1Uwrx/z3eAFUqYIQt9LoX/Ee4gxSPiZAySYwlhgCgKLM2sAcj4yjdfo2KPfYckNERMozwl69ypgAFfQVobVEZgVMaQCc9zJCnLmxUhKUWVL03iABMtMEPUetH242bqhdsjYaejRExwodc91FxW4pFZjcEBHlUmYDo4HMW4WKWtdYOkc8gT4dAasEwDgZSDQA3lkC9jDP0CpkLowRZvZBzxHrxuQGkzGl4ZRcnYPJjQpMboiI8piqFaIB5bKzZwvvbLFshJQA+rcD4oz+S4LMAfsPaZIi+XMDQ8DcPN+3CnnZesHPxY8tN3mByQ0RUT6X3WyxtGUFee+wXMq0VShNQgQolyUZAI8c8i4+DigmIiKSa9Mm272wlMhbhh4+VN0qJF9X6PZt4M4d3cSeh5qGArJf1a8vbxmKNFHRKpQuKTJPBMLscxjgixeAnvYWZcsNEREVPaoGULOVSCGrhAjIOimKNAOaPQDmHgWkC/4A+vfXSizsllKByQ0REeWYqvFDqsYT3b9fNAdWGxgAjx9nuhmrptgtRUREpAvqri+UmcuXgRUrgAcPVHedpS8ryPuSpaSkJnZaSG40weSGiIgoL+QmMcpuZ/rsWo70tT6RgQFQpkyeX5bJDRERUX7n6goMGpS7c8gTpNOnU7vV1EmK0tfRNElasSLPW20AjrnRdzhEREQFS1ZJEpCaBCUmpm542ru3VhMbDihWgckNERFRwaPJ97dBHsVERERElCeY3BAREVGhwuSGiIiIChUmN0RERFSoMLkhIiKiQoXJDRERERUqTG6IiIioUGFyQ0RERIUKkxsiIiIqVJjcEBERUaHC5IaIiIgKlSK3K7h8K63IyEg9R0JERETqkn9vq7MlZpFLbqKiogAAbm5ueo6EiIiINBUVFQVbW1uVdYrcruApKSl4/vw5rK2tIZFItHruyMhIuLm54enTp9xxXId4n/MG73Pe4H3OO7zXeUNX91kIgaioKLi4uMDAQPWomiLXcmNgYABXV1edXsPGxoa/OHmA9zlv8D7nDd7nvMN7nTd0cZ+za7GR44BiIiIiKlSY3BAREVGhwuRGi0xNTTF58mSYmprqO5RCjfc5b/A+5w3e57zDe5038sN9LnIDiomIiKhwY8sNERERFSpMboiIiKhQYXJDREREhQqTGyIiIipUmNxoyZIlS+Dp6QkzMzP4+fnh77//1ndIBcqsWbNQo0YNWFtbw8nJCR06dMCdO3eU6gghMGXKFLi4uMDc3BwNGzbEv//+q1QnPj4ew4YNQ7FixWBpaYl27dohLCwsL99KgTJr1ixIJBKMGDFCUcb7rB3Pnj1Dr1694OjoCAsLC/j6+uLq1auK13mfcy8pKQkTJ06Ep6cnzM3N4eXlhWnTpiElJUVRh/c5Z06fPo22bdvCxcUFEokEe/bsUXpdW/f13bt36N27N2xtbWFra4vevXvj/fv3uX8DgnJt69atwtjYWKxcuVLcvHlTfPvtt8LS0lI8fvxY36EVGM2bNxdr1qwRN27cECEhIaJ169aiVKlSIjo6WlFn9uzZwtraWuzcuVNcv35ddOvWTUilUhEZGamoM2jQIFGyZElx5MgRERQUJBo1aiSqVq0qkpKS9PG28rVLly4JDw8PUaVKFfHtt98qynmfc+/t27fC3d1d9OvXT1y8eFGEhoaKo0ePivv37yvq8D7n3vTp04Wjo6PYv3+/CA0NFdu3bxdWVlZiwYIFijq8zzlz4MABMWHCBLFz504BQOzevVvpdW3d1xYtWohKlSqJc+fOiXPnzolKlSqJNm3a5Dp+JjdaULNmTTFo0CClMm9vb/HDDz/oKaKC7+XLlwKAOHXqlBBCiJSUFOHs7Cxmz56tqPPhwwdha2srli1bJoQQ4v3798LY2Fhs3bpVUefZs2fCwMBAHDx4MG/fQD4XFRUlypYtK44cOSIaNGigSG54n7Vj7Nix4pNPPsnydd5n7WjdurX46quvlMo6deokevXqJYTgfdaW9MmNtu7rzZs3BQBx4cIFRZ3z588LAOL27du5ipndUrmUkJCAq1evolmzZkrlzZo1w7lz5/QUVcEXEREBAHBwcAAAhIaGIjw8XOk+m5qaokGDBor7fPXqVSQmJirVcXFxQaVKlfizSGfIkCFo3bo1mjRpolTO+6wde/fuhb+/Pz7//HM4OTmhWrVqWLlypeJ13mft+OSTT3Ds2DHcvXsXAHDt2jWcOXMGrVq1AsD7rCvauq/nz5+Hra0tatWqpahTu3Zt2Nra5vreF7mNM7Xt9evXSE5ORokSJZTKS5QogfDwcD1FVbAJITBy5Eh88sknqFSpEgAo7mVm9/nx48eKOiYmJrC3t89Qhz+Lj7Zu3YqgoCBcvnw5w2u8z9rx8OFDLF26FCNHjsT48eNx6dIlDB8+HKampujTpw/vs5aMHTsWERER8Pb2hqGhIZKTkzFjxgz06NEDAD/PuqKt+xoeHg4nJ6cM53dycsr1vWdyoyUSiUTpuRAiQxmpZ+jQofjnn39w5syZDK/l5D7zZ/HR06dP8e233+Lw4cMwMzPLsh7vc+6kpKTA398fM2fOBABUq1YN//77L5YuXYo+ffoo6vE+505gYCA2btyIzZs3w8fHByEhIRgxYgRcXFzQt29fRT3eZ93Qxn3NrL427j27pXKpWLFiMDQ0zJBlvnz5MkNWS9kbNmwY9u7dixMnTsDV1VVR7uzsDAAq77OzszMSEhLw7t27LOsUdVevXsXLly/h5+cHIyMjGBkZ4dSpU1i0aBGMjIwU94n3OXekUikqVqyoVFahQgU8efIEAD/P2jJ69Gj88MMP6N69OypXrozevXvju+++w6xZswDwPuuKtu6rs7MzXrx4keH8r169yvW9Z3KTSyYmJvDz88ORI0eUyo8cOYK6devqKaqCRwiBoUOHYteuXTh+/Dg8PT2VXvf09ISzs7PSfU5ISMCpU6cU99nPzw/GxsZKdWQyGW7cuMGfxX8aN26M69evIyQkRPHw9/dHz549ERISAi8vL95nLahXr16GpQzu3r0Ld3d3APw8a0tsbCwMDJS/xgwNDRVTwXmfdUNb97VOnTqIiIjApUuXFHUuXryIiIiI3N/7XA1HJiHEx6ngq1atEjdv3hQjRowQlpaW4tGjR/oOrcAYPHiwsLW1FSdPnhQymUzxiI2NVdSZPXu2sLW1Fbt27RLXr18XPXr0yHTqoaurqzh69KgICgoSn332WZGf0pmdtLOlhOB91oZLly4JIyMjMWPGDHHv3j2xadMmYWFhITZu3Kiow/uce3379hUlS5ZUTAXftWuXKFasmBgzZoyiDu9zzkRFRYng4GARHBwsAIhff/1VBAcHK5Y40dZ9bdGihahSpYo4f/68OH/+vKhcuTKngucnv//+u3B3dxcmJiaievXqiinMpB4AmT7WrFmjqJOSkiImT54snJ2dhampqahfv764fv260nni4uLE0KFDhYODgzA3Nxdt2rQRT548yeN3U7CkT254n7Vj3759olKlSsLU1FR4e3uLFStWKL3O+5x7kZGR4ttvvxWlSpUSZmZmwsvLS0yYMEHEx8cr6vA+58yJEycy/Zvct29fIYT27uubN29Ez549hbW1tbC2thY9e/YU7969y3X8EiGEyF3bDxEREVH+wTE3REREVKgwuSEiIqJChckNERERFSpMboiIiKhQYXJDREREhQqTGyIiIipUmNwQERFRocLkhog0JpFIsGfPHo2Pu3PnDpydnREVFaX9oP6zdu1a2NnZaXRMw4YNMWLECJ3Ek19cv34drq6uiImJ0XcoRDrH5IaoAOnXrx8kEkmGR4sWLfQdmlomTJiAIUOGwNraOsv3kvaRE926dcPdu3c1OmbXrl346aefcnQ9TTx8+BA9evSAi4sLzMzM4Orqivbt2yviffToESQSCUJCQrR+7cqVK6NmzZqYP3++1s9NlN8wuSEqYFq0aAGZTKb02LJli77DylZYWBj27t2LL7/8EgCwcOFCpfcAAGvWrMlQJpeQkKDWdczNzeHk5KRRbA4ODrC2ttboGE0lJCSgadOmiIyMxK5du3Dnzh0EBgaiUqVKiIiI0Om15b788kssXboUycnJeXI9In1hckNUwJiamsLZ2VnpYW9vr3hdIpFg6dKlaNmyJczNzeHp6Ynt27crneP69ev47LPPYG5uDkdHRwwcOBDR0dFKdVavXg0fHx+YmppCKpVi6NChSq+/fv0aHTt2hIWFBcqWLYu9e/eqjHvbtm2oWrUqXF1dAQC2trZK7wEA7OzsFM+7d++OoUOHYuTIkShWrBiaNm0KAPj1119RuXJlWFpaws3NDd98841S7Om7paZMmQJfX19s2LABHh4esLW1Rffu3ZW6xtJ3S3l4eGDmzJn46quvYG1tjVKlSmHFihVK7+fcuXPw9fWFmZkZ/P39sWfPHpWtLjdv3sTDhw+xZMkS1K5dG+7u7qhXrx5mzJiBGjVqAEjdbRkAqlWrBolEgoYNGyqOX7NmDSpUqAAzMzN4e3tjyZIlitfkLT5bt25F3bp1YWZmBh8fH5w8eVIphubNm+PNmzc4deqUip8UUcHH5IaoEJo0aRI6d+6Ma9euoVevXujRowdu3boFAIiNjUWLFi1gb2+Py5cvY/v27Th69KhS8rJ06VIMGTIEAwcOxPXr17F3716UKVNG6RpTp05F165d8c8//6BVq1bo2bMn3r59m2VMp0+fhr+/v0bvY926dTAyMsLZs2exfPlyAICBgQEWLVqEGzduYN26dTh+/DjGjBmj8jwPHjzAnj17sH//fuzfvx+nTp3C7NmzVR7zyy+/wN/fH8HBwfjmm28wePBg3L59GwAQFRWFtm3bonLlyggKCsJPP/2EsWPHqjxf8eLFYWBggB07dmTZcnLp0iUAwNGjRyGTybBr1y4AwMqVKzFhwgTMmDEDt27dwsyZMzFp0iSsW7dO6fjRo0fj+++/R3BwMOrWrYt27drhzZs3itdNTExQtWpV/P333ypjJSrwcr31JhHlmb59+wpDQ0NhaWmp9Jg2bZqiDgAxaNAgpeNq1aolBg8eLIQQYsWKFcLe3l5ER0crXv/rr7+EgYGBCA8PF0II4eLiIiZMmJBlHADExIkTFc+jo6OFRCIR//vf/7I8pmrVqkpxZnbO3bt3K543aNBA+Pr6Zllfbtu2bcLR0VHxfM2aNcLW1lbxfPLkycLCwkJERkYqykaPHi1q1aqldK20O6O7u7uLXr16KZ6npKQIJycnsXTpUiGEEEuXLhWOjo4iLi5OUWflypUCgAgODs4y1t9++01YWFgIa2tr0ahRIzFt2jTx4MEDxeuhoaGZnsPNzU1s3rxZqeynn34SderUUTpu9uzZitcTExOFq6urmDNnjtJxHTt2FP369csyRqLCwEifiRURaa5Ro0ZYunSpUpmDg4PS8zp16mR4Lu8uuXXrFqpWrQpLS0vF6/Xq1UNKSgru3LkDiUSC58+fo3HjxirjqFKliuLflpaWsLa2xsuXL7OsHxcXBzMzM5XnTC+zlp4TJ05g5syZuHnzJiIjI5GUlIQPHz4gJiZG6T2l5eHhoTSmRiqVqowVUH5/EokEzs7OimPu3LmDKlWqKL2fmjVrZvt+hgwZgj59+uDEiRO4ePEitm/fjpkzZ2Lv3r2Kbrf0Xr16hadPn6J///4YMGCAojwpKQm2trZKddP+3I2MjODv769osZMzNzdHbGxstrESFWRMbogKGEtLywxdROqQzz4SQmQ5E0kikcDc3Fyt8xkbG2c4NiUlJcv6xYoVw7t379SMNlX6ZOXx48do1aoVBg0ahJ9++gkODg44c+YM+vfvj8TERK3Fmt0xmd1DIUS27wcArK2t0a5dO7Rr1w7Tp09H8+bNMX369CyTG/k1V65ciVq1aim9ZmhomO310sf59u1blC5dWq1YiQoqjrkhKoQuXLiQ4bm3tzcAoGLFiggJCVFa7+Ts2bMwMDBAuXLlYG1tDQ8PDxw7dkyrMVWrVg03b97M1TmuXLmCpKQk/PLLL6hduzbKlSuH58+faylC9Xl7e+Off/5BfHy8Umyakkgk8Pb2VvwsTExMAEBpTE6JEiVQsmRJPHz4EGXKlFF6yAcgy6X9uSclJeHq1auKn7vcjRs3UK1aNY1jJSpImNwQFTDx8fEIDw9Xerx+/Vqpzvbt27F69WrcvXsXkydPxqVLlxQDhnv27AkzMzP07dsXN27cwIkTJzBs2DD07t0bJUqUAJA6w+iXX37BokWLcO/ePQQFBWHx4sW5irt58+Y4f/58rqYhly5dGklJSVi8eDEePnyIDRs2YNmyZbmKKye++OILpKSkYODAgbh16xYOHTqEefPmAcjYUiIXEhKC9u3bY8eOHbh58ybu37+PVatWYfXq1Wjfvj0AwMnJCebm5jh48CBevHihmCI+ZcoUzJo1CwsXLsTdu3dx/fp1rFmzBr/++qvSNX7//Xfs3r0bt2/fxpAhQ/Du3Tt89dVXitcfPXqEZ8+eoUmTJrq4LUT5BpMbogLm4MGDkEqlSo9PPvlEqc7UqVOxdetWVKlSBevWrcOmTZtQsWJFAICFhQUOHTqEt2/fokaNGujSpQsaN26M3377TXF83759sWDBAixZsgQ+Pj5o06YN7t27l6u4W7VqBWNjYxw9ejTH5/D19cWvv/6KOXPmoFKlSti0aRNmzZqVq7hywsbGBvv27UNISAh8fX0xYcIE/PjjjwCQ5bgiV1dXeHh4YOrUqahVqxaqV6+OhQsXYurUqZgwYQKA1HEyixYtwvLly+Hi4qJIer7++mv88ccfWLt2LSpXrowGDRpg7dq1GVpuZs+ejTlz5ihmRP35558oVqyY4vUtW7agWbNmcHd318VtIco3JELdjmIiKhAkEgl2796NDh066DuUDJYsWYI///wThw4d0ncoWrdp0yZ8+eWXiIiIUHvckrY8evQInp6eCA4Ohq+vb6Z14uPjUbZsWWzZsgX16tXL0/iI8hoHFBNRnhk4cCDevXuHqKgona8IrGvr16+Hl5cXSpYsiWvXrmHs2LHo2rVrnic26nr8+DEmTJjAxIaKBCY3RJRnjIyMFF0wBV14eDh+/PFHhIeHQyqV4vPPP8eMGTP0HVaWypUrh3Llyuk7DKI8wW4pIiIiKlQ4oJiIiIgKFSY3REREVKgwuSEiIqJChckNERERFSpMboiIiKhQYXJDREREhQqTGyIiIipUmNwQERFRocLkhoiIiAqV/wMTmoEQPGq/fQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def loss_plot(losses_train, losses_val):\n",
    "    plt.title('How does loss function change over training steps?', fontsize=12)\n",
    "\n",
    "    plt.plot(losses_train, color='red', marker='.', linewidth=2.0, label='Training Loss')\n",
    "    plt.plot(losses_val, color='green', marker='+', linewidth=2.0,  label='Validation Loss')\n",
    "\n",
    "    plt.xlabel('Epoch (Training Step)', fontsize=10)\n",
    "    plt.ylabel('Negative Log-Likelihood Loss', fontsize=10)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(fname='output/Loss_Function_over_Training_Steps.png', dpi=600)\n",
    "    plt.show()\n",
    "#end-def\n",
    "\n",
    "loss_plot(losses_train, losses_val)\n",
    "\n",
    "# def loss_plot(losses):\n",
    "#     plt.title('How does loss function change over training steps?', fontsize=12)\n",
    "\n",
    "#     plt.plot(losses, color='red', marker='.', linewidth=2.0)\n",
    "#     plt.xlabel('Epoch (Training Step)', fontsize=10)\n",
    "#     plt.ylabel('MSE Loss', fontsize=10)\n",
    "\n",
    "#     plt.savefig(fname='output/Loss_Function_over_Training_Steps.png', dpi=600)\n",
    "#     plt.show()\n",
    "# #end-def\n",
    "\n",
    "# loss_plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe434f5f",
   "metadata": {},
   "source": [
    "###### Test Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b57b6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 100.00%.\n"
     ]
    }
   ],
   "source": [
    "def accuracy(X, Y):\n",
    "    Yp = [model(xs) for xs in X]\n",
    "    # return Yp\n",
    "\n",
    "    ysp = []\n",
    "    for yp in Yp:\n",
    "        ps = softmax(yp)\n",
    "        p1, p2, p3 = ps[0].data, ps[1].data, ps[2].data\n",
    "        v = [p1, p2, p3]\n",
    "        ysp.append(int(np.argmax(v)))\n",
    "    #end-for\n",
    "\n",
    "    ys = []\n",
    "    for i in Y:\n",
    "        v = np.array(i)\n",
    "        ys.append(int(np.argmax(v)))\n",
    "    #end-for\n",
    "\n",
    "    counter = 0\n",
    "    for y, yp in zip(ys, ysp):\n",
    "        if y == yp:\n",
    "            counter += 1\n",
    "        #end-def\n",
    "    #end-for\n",
    "    \n",
    "    assert len(ys) == len(ysp)\n",
    "\n",
    "    return counter / (len(ys))\n",
    "\n",
    "\n",
    "    # Ysoftmax = softmax(Yp)\n",
    "\n",
    "    # return Ysoftmax\n",
    "\n",
    "    # categorical_cross_entrorpy(Y, Yp)\n",
    "    # ys = []\n",
    "    # for i in Y:\n",
    "    #     v = np.array(i)\n",
    "    #     ys_test.append(int(np.argmax(v)))\n",
    "    # #end-for\n",
    "\n",
    "    # ys_test = np.array(ys)\n",
    "    # ys_test\n",
    "    # return ys\n",
    "\n",
    "\n",
    "value = accuracy(X_test, Y_test)\n",
    "print(f'Test accuracy is {(value*100):0.2f}%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1896c3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2bff00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c570ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246cb009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
